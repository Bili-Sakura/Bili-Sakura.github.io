
@inproceedings{Peebles_2023_ICCV,
	title = {Scalable {Diffusion} {Models} with {Transformers}},
	url = {https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops---through increased transformer depth/width or increased number of input tokens---consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
	language = {en},
	urldate = {2024-08-12},
	booktitle = {Proceedings of the {IEEE}/{CVF} international conference on computer vision ({ICCV})},
	author = {Peebles, William and Xie, Saining},
	publisher = {ICCV},
	year = {2023},
	keywords = {DiT},
	pages = {4195--4205},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\H4G4MHR9\\Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf:application/pdf;Peebles_Scalable_Diffusion_Models_ICCV_2023_supplemental:D\:\\Zotero\\storage(sakura)\\storage\\2JD3JY3P\\Peebles_Scalable_Diffusion_Models_ICCV_2023_supplemental.pdf:application/pdf},
}

@inproceedings{chenPixArtaFastTraining2023,
	title = {{PixArt}-α: {Fast} {Training} of {Diffusion} {Transformer} for {Photorealistic} {Text}-to-{Image} {Synthesis}},
	shorttitle = {{PixArt}-\${\textbackslash}alpha\$},
	url = {https://openreview.net/forum?id=eAKmQPe3m1},
	abstract = {The most advanced text-to-image (T2I) models require significant training costs (e.g., millions of GPU hours), seriously hindering the fundamental innovation for the AIGC community while increasing CO2 emissions. This paper introduces PixArt-\${\textbackslash}alpha\$, a Transformer-based T2I diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, SDXL, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost, as shown in Figure 1 and 2. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient T2I Transformer: We incorporate cross-attention modules into Diffusion Transformer (DiT) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, PixArt-\${\textbackslash}alpha\$'s training speed markedly surpasses existing large-scale T2I models, e.g., PixArt-\${\textbackslash}alpha\$ only takes 10.8\% of Stable Diffusion v1.5's training time ({\textasciitilde}675 vs. {\textasciitilde}6,250 A100 GPU days), saving nearly {\textbackslash}{\textbackslash}\$300,000 ({\textbackslash}{\textbackslash}\$26,000 vs. {\textbackslash}{\textbackslash}\$320,000) and reducing 90\% CO2 emissions. Moreover, compared with a larger SOTA model, RAPHAEL, our training cost is merely 1\%. Extensive experiments demonstrate that PixArt-\${\textbackslash}alpha\$ excels in image quality, artistry, and semantic control. We hope PixArt-\${\textbackslash}alpha\$ will provide new insights to the AIGC community and startups to accelerate building their own high-quality yet low-cost generative models from scratch.},
	language = {en},
	urldate = {2025-01-12},
	publisher = {ICLR},
	author = {Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao, Lewei and Xie, Enze and Wang, Zhongdao and Kwok, James and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
	month = oct,
	year = {2024},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\JPEFMGRP\\Chen et al. - 2023 - PixArt-\$alpha\$ Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis.pdf:application/pdf},
}

@inproceedings{esserScalingRectifiedFlow2024a,
	title = {Scaling {Rectified} {Flow} {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {https://openreview.net/forum?id=FPnUhsQJ5B},
	abstract = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models. Stability AI is considering making experimental data, code, and model weights publicly available.},
	language = {en},
	urldate = {2024-07-07},
	author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Müller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
	month = jun,
	year = {2024},
	publisher = {ICML},
	keywords = {Stable Diffusion},
	file = {2403.03206v1:D\:\\Zotero\\storage(sakura)\\storage\\DDRG7XNT\\2403.03206v1.pdf:application/pdf;Esser et al_2024_Scaling Rectified Flow Transformers for High-Resolution Image Synthesis.pdf:D\:\\Zotero\\storage(sakura)\\storage\\6PDBK7QS\\Esser et al_2024_Scaling Rectified Flow Transformers for High-Resolution Image Synthesis.pdf:application/pdf},
}

@inproceedings{xieSANAEfficientHighResolution2024,
	title = {{SANA}: {Efficient} {High}-{Resolution} {Text}-to-{Image} {Synthesis} with {Linear} {Diffusion} {Transformers}},
	shorttitle = {{SANA}},
	url = {https://openreview.net/forum?id=N8Oj1XhtYZ},
	abstract = {We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096\${\textbackslash}times\$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8\${\textbackslash}times\$, we trained an AE that can compress images 32\${\textbackslash}times\$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024\${\textbackslash}times\$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.},
	language = {en},
	urldate = {2025-05-12},
	author = {Xie, Enze and Chen, Junsong and Chen, Junyu and Cai, Han and Tang, Haotian and Lin, Yujun and Zhang, Zhekai and Li, Muyang and Zhu, Ligeng and Lu, Yao and Han, Song},
	month = oct,
	publisher = {ICLR},
	year = {2025},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\QZ7LWGNI\\Xie et al. - 2024 - SANA Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers.pdf:application/pdf},
}

@misc{chenPIXARTdFastControllable2024,
	title = {{PIXART}-δ: {Fast} and {Controllable} {Image} {Generation} with {Latent} {Consistency} {Models}},
	shorttitle = {{PIXART}-δ},
	url = {http://arxiv.org/abs/2401.05252},
	doi = {10.48550/arXiv.2401.05252},
	abstract = {This technical report introduces PIXART-\{{\textbackslash}delta\}, a text-to-image synthesis framework that integrates the Latent Consistency Model (LCM) and ControlNet into the advanced PIXART-\{{\textbackslash}alpha\} model. PIXART-\{{\textbackslash}alpha\} is recognized for its ability to generate high-quality images of 1024px resolution through a remarkably efficient training process. The integration of LCM in PIXART-\{{\textbackslash}delta\} significantly accelerates the inference speed, enabling the production of high-quality images in just 2-4 steps. Notably, PIXART-\{{\textbackslash}delta\} achieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images, marking a 7x improvement over the PIXART-\{{\textbackslash}alpha\}. Additionally, PIXART-\{{\textbackslash}delta\} is designed to be efficiently trainable on 32GB V100 GPUs within a single day. With its 8-bit inference capability (von Platen et al., 2023), PIXART-\{{\textbackslash}delta\} can synthesize 1024px images within 8GB GPU memory constraints, greatly enhancing its usability and accessibility. Furthermore, incorporating a ControlNet-like module enables fine-grained control over text-to-image diffusion models. We introduce a novel ControlNet-Transformer architecture, specifically tailored for Transformers, achieving explicit controllability alongside high-quality image generation. As a state-of-the-art, open-source image generation model, PIXART-\{{\textbackslash}delta\} offers a promising alternative to the Stable Diffusion family of models, contributing significantly to text-to-image synthesis.},
	urldate = {2024-11-20},
	publisher = {arXiv},
	author = {Chen, Junsong and Wu, Yue and Luo, Simian and Xie, Enze and Paul, Sayak and Luo, Ping and Zhao, Hang and Li, Zhenguo},
	month = jan,
	year = {2024},
	note = {arXiv:2401.05252},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Zotero\\storage(sakura)\\storage\\FT6UMWN9\\Chen et al. - 2024 - PIXART-δ Fast and Controllable Image Generation with Latent Consistency Models.pdf:application/pdf;Snapshot:D\:\\Zotero\\storage(sakura)\\storage\\EU98VP36\\2401.html:text/html},
}

@misc{qinLuminaImage20Unified2025,
	title = {Lumina-{Image} 2.0: {A} {Unified} and {Efficient} {Image} {Generative} {Framework}},
	shorttitle = {Lumina-{Image} 2.0},
	url = {http://arxiv.org/abs/2503.21758},
	doi = {10.48550/arXiv.2503.21758},
	abstract = {We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.},
	urldate = {2025-05-12},
	publisher = {arXiv},
	author = {Qin, Qi and Zhuo, Le and Xin, Yi and Du, Ruoyi and Li, Zhen and Fu, Bin and Lu, Yiting and Yuan, Jiakang and Li, Xinyue and Liu, Dongyang and Zhu, Xiangyang and Zhang, Manyuan and Beddow, Will and Millon, Erwann and Perez, Victor and Wang, Wenhai and He, Conghui and Zhang, Bo and Liu, Xiaohong and Li, Hongsheng and Qiao, Yu and Xu, Chang and Gao, Peng},
	month = mar,
	year = {2025},
	note = {arXiv:2503.21758 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech Report, 21 pages, 12 figures},
	file = {Full Text PDF:D\:\\Zotero\\storage(sakura)\\storage\\CSJK9NKS\\Qin et al. - 2025 - Lumina-Image 2.0 A Unified and Efficient Image Generative Framework.pdf:application/pdf;Snapshot:D\:\\Zotero\\storage(sakura)\\storage\\GRZM783M\\2503.html:text/html},
}
