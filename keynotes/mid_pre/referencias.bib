@online{chenRSCC2025,
  title = {{{RSCC}}: {{A Large-Scale Remote Sensing Change Caption Dataset}} for {{Disaster Events}}},
  author = {Chen, Zhenyuan and Wang, Chenxi and Zhang, Ningyu and Zhang, Feng},
  date = {2025},
  year ={2025},
  pubstate = {prepublished},
  file = {D:\ZoteroLib\storage\CU53RM42\280_RSCC_0516v2_single_blind.pdf}
}
@online{zhangInContextEditEnabling2025,
  title = {In-{{Context Edit}}: {{Enabling Instructional Image Editing}} with {{In-Context Generation}} in {{Large Scale Diffusion Transformer}}},
  shorttitle = {In-{{Context Edit}}},
  author = {Zhang, Zechuan and Xie, Ji and Lu, Yu and Yang, Zongxin and Yang, Yi},
  date = {2025-04-29},
  eprint = {2504.20690},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.20690},
  url = {http://arxiv.org/abs/2504.20690},
  urldate = {2025-05-02},
  year ={2025},
  abstract = {Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5\% training data and 1\% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\XFQ2TXS9\\Zhang et al. - 2025 - In-Context Edit Enabling Instructional Image Editing with In-Context Generation in Large Scale Diff.pdf;D\:\\ZoteroLib\\storage\\BJF945V9\\2504.html}
}
@inproceedings{Brooks_2023_CVPR,
  title = {{{InstructPix2Pix}}: {{Learning}} to Follow Image Editing Instructions},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Brooks, Tim and Holynski, Aleksander and Efros, Alexei A.},
  date = {2023-06},
  year ={2023},
  pages = {18392--18402},
  url = {http://openaccess.thecvf.com//content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf}
}

@inproceedings{zhaoUltraEditInstructionbasedFineGrained2024b,
  title = {{{UltraEdit}}: {{Instruction-based Fine-Grained Image Editing}} at {{Scale}}},
  shorttitle = {{{UltraEdit}}},
  author = {Zhao, Haozhe and Ma, Xiaojian and Chen, Liang and Si, Shuzheng and Wu, Rujie and An, Kaikai and Yu, Peiyu and Zhang, Minjia and Li, Qing and Chang, Baobao},
  date = {2024-11-13},
  year ={2024},
  url = {https://openreview.net/forum?id=9ZDdlgH6O8#discussion},
  urldate = {2025-05-06},
  abstract = {This paper presents UltraEdit, a large-scale (\textasciitilde{} 4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a *systematic* approach to producing massive and high-quality image editing samples: 1) UltraEdit includes more diverse editing instructions by combining LLM creativity and in-context editing examples by human raters; 2) UltraEdit is anchored on real images (photographs or artworks), which offers more diversity and less biases than those purely synthesized by text-to-image models; 3) UltraEdit supports region-based editing with high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on challenging MagicBrush and Emu-Edit benchmarks, respectively. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models will be made public.},
  eventtitle = {The {{Thirty-eight Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\3HRIS4DE\\attachment.pdf;D\:\\ZoteroLib\\storage\\6X784FLV\\Zhao et al. - 2024 - UltraEdit Instruction-based Fine-Grained Image Editing at Scale.pdf}
}
@online{liuStep1XEditPracticalFramework2025,
  title = {{{Step1X-Edit}}: {{A Practical Framework}} for {{General Image Editing}}},
  shorttitle = {{{Step1X-Edit}}},
  author = {Liu, Shiyu and Han, Yucheng and Xing, Peng and Yin, Fukun and Wang, Rui and Cheng, Wei and Liao, Jiaqi and Wang, Yingming and Fu, Honghao and Han, Chunrui and Li, Guopeng and Peng, Yuang and Sun, Quan and Wu, Jingwei and Cai, Yan and Ge, Zheng and Ming, Ranchen and Xia, Lei and Zeng, Xianfang and Zhu, Yibo and Jiao, Binxing and Zhang, Xiangyu and Yu, Gang and Jiang, Daxin},
  date = {2025-05-06},
  year ={2025},
  eprint = {2504.17761},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.17761},
  url = {http://arxiv.org/abs/2504.17761},
  urldate = {2025-05-22},
  abstract = {In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\S7SNA8MB\\Liu et al. - 2025 - Step1X-Edit A Practical Framework for General Image Editing.pdf;D\:\\ZoteroLib\\storage\\BXWU9KI4\\2504.html}
}
@inproceedings{ghiasiSimpleCopyPasteStrong2021,
  title = {Simple {{Copy-Paste Is}} a {{Strong Data Augmentation Method}} for {{Instance Segmentation}}},
  author = {Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D. and Le, Quoc V. and Zoph, Barret},
  date = {2021},
  year ={2021},
  pages = {2918--2928},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.html?ref=https://githubhelp.com},
  urldate = {2025-06-06},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\H6WWNUZX\\Ghiasi_Simple_Copy-Paste_Is_CVPR_2021_supplemental.pdf;D\:\\ZoteroLib\\storage\\QBC4YHPQ\\Ghiasi et al. - 2021 - Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation.pdf}
}
@online{devriesImprovedRegularizationConvolutional2017,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  date = {2017-08-15},
  year={2017},
  url = {https://arxiv.org/abs/1708.04552v2},
  urldate = {2025-06-09},
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  langid = {english},
  pubstate = {prepublished},
  file = {D:\ZoteroLib\storage\3RBNHX6L\DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Networks with Cutout.pdf}
}


@inproceedings{heSyntheticDataGenerative2022,
  title = {Is Synthetic Data from Generative Models Ready for Image  Recognition?},
  author = {He, Ruifei and Sun, Shuyang and Yu, Xin and Xue, Chuhui and Zhang, Wenqing and Torr, Philip and Bai, Song and Qi, Xiaojuan},
  date = {2022-09-29},
  year ={2023},
  url = {https://openreview.net/forum?id=nUmCcZ5RKF},
  urldate = {2024-09-01},
  abstract = {Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData.},
  eventtitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  file = {D\:\\ZoteroLib\\storage\\AKJ2342C\\He et al. - 2022 - IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION.pdf;D\:\\ZoteroLib\\storage\\QXK48PZH\\id220_supp_final.pdf}
}

@inproceedings{sastryGeoSynthContextuallyAwareHighResolution2024,
  title = {{{GeoSynth}}: {{Contextually-Aware High-Resolution Satellite Image Synthesis}}},
  shorttitle = {{{GeoSynth}}},
  author = {Sastry, Srikumar and Khanal, Subash and Dhakal, Aayush and Jacobs, Nathan},
  date = {2024},
  year={2024},
  pages = {460--470},
  url = {https://openaccess.thecvf.com/content/CVPR2024W/EarthVision/html/Sastry_GeoSynth_Contextually-Aware_High-Resolution_Satellite_Image_Synthesis_CVPRW_2024_paper.html},
  urldate = {2024-09-18},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\3IZN5Z66\\Sastry_GeoSynth_Contextually-Aware_High-Resolution_CVPRW_2024_supplemental.pdf;D\:\\ZoteroLib\\storage\\YHGA565B\\Sastry et al. - 2024 - GeoSynth Contextually-Aware High-Resolution Satellite Image Synthesis.pdf}
}

@article{steinerHowTrainYour2022,
  title = {How to Train Your {{ViT}}? {{Data}}, {{Augmentation}}, and {{Regularization}} in {{Vision Transformers}}},
  shorttitle = {How to Train Your {{ViT}}?},
  author = {Steiner, Andreas Peter and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  date = {2022-04-02},
  year={2022},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=4nPswr1KcP&nesting=2&sort=date-desc},
  urldate = {2025-06-10},
  abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
  langid = {english},
  file = {D:\ZoteroLib\storage\M9MEK48J\Steiner et al. - 2022 - How to train your ViT Data, Augmentation, and Regularization in Vision Transformers.pdf}
}

@inproceedings{Toker_2024_CVPR,
  title = {{{SatSynth}}: {{Augmenting}} Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation},
  shorttitle = {{{SatSynth}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition ({{CVPR}})},
  author = {Toker, Aysim and Eisenberger, Marvin and Cremers, Daniel and Leal-Taixé, Laura},
  date = {2024-06},
  pages = {27695--27705},
  url = {http://openaccess.thecvf.com//content/CVPR2024/papers/Toker_SatSynth_Augmenting_Image-Mask_Pairs_through_Diffusion_Models_for_Aerial_Semantic_CVPR_2024_paper.pdf},
  urldate = {2024-11-21},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {D\:\\ZoteroLib\\storage\\LTNUQYNE\\Toker_SatSynth_Augmenting_Image-Mask_CVPR_2024_supplemental.pdf;D\:\\ZoteroLib\\storage\\Z83J4BG2\\Toker et al. - 2024 - SatSynth Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation.pdf}
}

@inproceedings{yunCutMixRegularizationStrategy2019,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers With Localizable Features}}},
  shorttitle = {{{CutMix}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  date = {2019},
  year={2019},
  pages = {6023--6032},
  url = {https://openaccess.thecvf.com/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_Strong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html},
  urldate = {2025-06-06},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  file = {D\:\\ZoteroLib\\storage\\HG75M439\\Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Classifiers With Localizable Features.pdf;D\:\\ZoteroLib\\storage\\NA5HP9L4\\Yun et al. - CutMix Regularization Strategy to Train Strong Classiﬁers with Localizable Features – Supplementary.pdf}
}
