@inproceedings{heMaskedAutoencodersAre2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  publisher={CVPR},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2022},
  pages = {16000--16009},
  urldate = {2024-08-20},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\87YIQASS\\He_Masked_Autoencoders_Are_CVPR_2022_supplemental.pdf;D\:\\ZoteroLib\\storage\\SHI3PRMZ\\He et al. - 2022 - Masked Autoencoders Are Scalable Vision Learners.pdf}
}


@misc{imagenet2010challenge,
  author = {Berg, A. and Deng, J. and Fei-Fei, L.},
  title = {Large Scale Visual Recognition Challenge 2010},
  year = {2010},
  howpublished = {\url{http://www.imagenet.org/challenges}},
  note = {Accessed: 2024-06-01}
}

@inproceedings{chertiReproducibleScalingLaws2023,
  title = {Reproducible {{Scaling Laws}} for {{Contrastive Language-Image Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  year = {2023},
  publisher = {CVPR},
  pages = {2818--2829},
  urldate = {2024-07-19},
  langid = {english},
  annotation = {CCF: A},
  file = {D\:\\ZoteroLib\\storage\\4YNXQK3Z\\Cherti_Reproducible_Scaling_Laws_CVPR_2023_supplemental.pdf;D\:\\ZoteroLib\\storage\\M7VBVLZE\\Cherti et al_2023_Reproducible Scaling Laws for Contrastive Language-Image Learning.pdf}
}

@inproceedings{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  publisher = {ICLR},
  month = oct,
  urldate = {2024-04-28},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  keywords = {DNN,Google,ViT},
  file = {D:\ZoteroLib\storage\7IWEULRX\Dosovitskiy ç­‰ - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@inproceedings{fangDataFilteringNetworks2023,
  title = {Data {{Filtering Networks}}},
  booktitle = {The {{Twelfth International Conference}} on {{Learning Representations}}},
  author = {Fang, Alex and Jose, Albin Madappally and Jain, Amit and Schmidt, Ludwig and Toshev, Alexander T. and Shankar, Vaishaal},
  year = {2024},
publisher = {ICLR},
  month = oct,
  urldate = {2024-07-21},
  abstract = {Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a *data filtering network* (DFN) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on ImageNet can yield worse training sets than a model with low ImageNet accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset DFN-5B enables us to train state-of-the-art models for their compute budgets: among other improvements on a variety of tasks, a ViT-H trained on our dataset achieves 83.0\% zero-shot transfer accuracy on ImageNet, out-performing larger models trained on other datasets such as LAION-2B, DataComp-1B, or OpenAI's WIT. In order to facilitate further research in dataset design, we also release a new 2 billion example dataset DFN-2B and show that high performance data filtering networks can be trained from scratch using only publicly available data.},
  langid = {english},
  file = {D:\ZoteroLib\storage\AKYPDQSX\Fang et al_2023_Data Filtering Networks.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  publisher = {CVPR},
  pages = {770--778},
  urldate = {2024-06-20},
  file = {D:\ZoteroLib\storage\G6388CZG\He et al_2016_Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},

  year = {2012},
  volume = {25},
  publisher = {NeurIPS},
  urldate = {2024-04-29},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {D:\ZoteroLib\storage\HGIP47U9\Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@inproceedings{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  booktitle = {International {{Conference}} on {{Computer Vision}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  year = {2021},
  publisher = {ICCV},
  month = oct,
  pages = {9992--10002},
  issn = {2380-7504},
  doi = {10.1109/ICCV48922.2021.00986},
  urldate = {2024-04-15},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.},
  keywords = {Computational modeling,Computer architecture,Computer vision,Detection and localization in 2D and 3D,DNN,grouping and shape,ICCV 2024,Image segmentation,Object detection,Recognition and classification,Representation learning,Segmentation,Semantics,Visualization},
  file = {D:\ZoteroLib\storage\R39QXNUD\Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@inproceedings{liuSwinTransformerV22022,
  title = {Swin {{Transformer V2}}: {{Scaling Up Capacity}} and {{Resolution}}},
  shorttitle = {Swin {{Transformer V2}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  year = {2022},
  publisher = {CVPR},
  pages = {12009--12019},
  urldate = {2025-06-11},
  file = {D:\ZoteroLib\storage\GTRXNJYE\Liu et al. - 2022 - Swin Transformer V2 Scaling Up Capacity and Resolution.pdf}
}

@inproceedings{radfordLearningTransferableVisual2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8748--8763},
  publisher = {ICML},
  issn = {2640-3498},
  urldate = {2024-07-07},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english},
  annotation = {CCF: A},
  file = {D\:\\ZoteroLib\\storage\\HKNJNPGP\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;D\:\\ZoteroLib\\storage\\MRSLVU8U\\Radford et al_2021_Learning Transferable Visual Models From Natural Language Supervision.pdf}
}

@article{yuCoCaContrastiveCaptioners2022,
  title = {{{CoCa}}: {{Contrastive Captioners}} Are {{Image-Text Foundation Models}}},
  shorttitle = {{{CoCa}}},
  author = {Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  year = {2022},
  month = jul,
  journal = {TMLR},
  issn = {2835-8856},
  urldate = {2025-06-10},
  abstract = {Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3\% zero-shot top-1 accuracy, 90.6\% with a frozen encoder and learned classification head, and 91.0\% with a finetuned encoder.},
  langid = {english},
  file = {D:\ZoteroLib\storage\MG4867AG\Yu et al. - 2022 - CoCa Contrastive Captioners are Image-Text Foundation Models.pdf}
}

@misc{sunEVACLIPImprovedTraining2023,
  title = {{{EVA-CLIP}}: {{Improved Training Techniques}} for {{CLIP}} at {{Scale}}},
  shorttitle = {{{EVA-CLIP}}},
  author = {Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  year = {2023},
  month = mar,
  number = {arXiv:2303.15389},
  eprint = {2303.15389},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.15389},
  urldate = {2024-07-27},
  abstract = {Contrastive language-image pre-training, CLIP for short, has gained increasing attention for its potential in various scenarios. In this paper, we propose EVA-CLIP, a series of models that significantly improve the efficiency and effectiveness of CLIP training. Our approach incorporates new techniques for representation learning, optimization, and augmentation, enabling EVA-CLIP to achieve superior performance compared to previous CLIP models with the same number of parameters but significantly smaller training costs. Notably, our largest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples achieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller EVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples achieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open access and open research, we release the complete suite of EVA-CLIP to the community at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D\:\\ZoteroLib\\storage\\U3TPSRE5\\Sun et al_2023_EVA-CLIP.pdf;D\:\\ZoteroLib\\storage\\QKP3AFNZ\\2303.html}
}

@misc{tschannenSigLIP2Multilingual2025,
  title = {{{SigLIP}} 2: {{Multilingual Vision-Language Encoders}} with {{Improved Semantic Understanding}}, {{Localization}}, and {{Dense Features}}},
  shorttitle = {{{SigLIP}} 2},
  author = {Tschannen, Michael and Gritsenko, Alexey and Wang, Xiao and Naeem, Muhammad Ferjad and Alabdulmohsin, Ibrahim and Parthasarathy, Nikhil and Evans, Talfan and Beyer, Lucas and Xia, Ye and Mustafa, Basil and H{\'e}naff, Olivier and Harmsen, Jeremiah and Steiner, Andreas and Zhai, Xiaohua},
  year = {2025},
  month = feb,
  number = {arXiv:2502.14786},
  eprint = {2502.14786},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.14786},
  urldate = {2025-05-24},
  abstract = {We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\ZoteroLib\storage\D8EBW94F\Tschannen et al. - 2025 - SigLIP 2 Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization,.pdf}
}

@inproceedings{xieDataScalingMasked2023,
  title = {On {{Data Scaling}} in {{Masked Image Modeling}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Wei, Yixuan and Dai, Qi and Hu, Han},
  publisher = {CVPR},
  year = {2023},
  pages = {10365--10374},
  urldate = {2025-06-11},
  file = {D\:\\ZoteroLib\\storage\\BZD7H9GY\\Xie_On_Data_Scaling_CVPR_2023_supplemental.pdf;D\:\\ZoteroLib\\storage\\PDIQQLAD\\Xie et al. - 2023 - On Data Scaling in Masked Image Modeling.pdf}
}

@inproceedings{zhaiSigmoidLossLanguage2023,
  title = {Sigmoid {{Loss}} for {{Language Image Pre-Training}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  year = {2023},
  publisher = {ICCV},
  pages = {11975--11986},
  urldate = {2024-08-26},
  langid = {english},
  file = {D\:\\ZoteroLib\\storage\\823S78LK\\Zhai_Sigmoid_Loss_for_ICCV_2023_supplemental.pdf;D\:\\ZoteroLib\\storage\\FUUCK7ZN\\Zhai et al. - 2023 - Sigmoid Loss for Language Image Pre-Training.pdf}
}

@misc{zhuInternVL3ExploringAdvanced2025,
  title = {{{InternVL3}}: {{Exploring Advanced Training}} and {{Test-Time Recipes}} for {{Open-Source Multimodal Models}}},
  shorttitle = {{{InternVL3}}},
  author = {Zhu, Jinguo and Wang, Weiyun and Chen, Zhe and Liu, Zhaoyang and Ye, Shenglong and Gu, Lixin and Duan, Yuchen and Tian, Hao and Su, Weijie and Shao, Jie and Gao, Zhangwei and Cui, Erfei and Cao, Yue and Liu, Yangzhou and Xu, Weiye and Li, Hao and Wang, Jiahao and Lv, Han and Chen, Dengnian and Li, Songze and He, Yinan and Jiang, Tan and Luo, Jiapeng and Wang, Yi and He, Conghui and Shi, Botian and Zhang, Xingcheng and Shao, Wenqi and He, Junjun and Xiong, Yingtong and Qu, Wenwen and Sun, Peng and Jiao, Penglong and Wu, Lijun and Zhang, Kaipeng and Deng, Huipeng and Ge, Jiaye and Chen, Kai and Wang, Limin and Dou, Min and Lu, Lewei and Zhu, Xizhou and Lu, Tong and Lin, Dahua and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  year = {2025},
  month = apr,
  number = {arXiv:2504.10479},
  eprint = {2504.10479},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.10479},
  urldate = {2025-04-15},
  abstract = {We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {D:\ZoteroLib\storage\NILC74FB\Zhu et al. - 2025 - InternVL3 Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models.pdf}
}
