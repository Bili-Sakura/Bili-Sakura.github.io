
        <paper>
            <title>Towards Attack-tolerant Federated Learning via Critical Parameter Analysis</title>
            <abstract>Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_Towards_Attack-tolerant_Federated_Learning_via_Critical_Parameter_Analysis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_Towards_Attack-tolerant_Federated_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sungwon Han</author><author>Sungwon Park</author><author>Fangzhao Wu</author><author>Sundong Kim</author><author>Bin Zhu</author><author>Xing Xie</author><author>Meeyoung Cha</author>
            </authors>
        </paper>
        

        <paper>
            <title>Stochastic Segmentation with Conditional Categorical Diffusion Models</title>
            <abstract>Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the common objective of generating a single segmentation output that accurately matches the image&apos;s content may not be suitable for safety-critical domains such as medical diagnostics and autonomous driving. Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annotation maps. In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distributions, high-dimensional output spaces, and limited annotation data. To address these challenges, we propose a conditional categorical diffusion model (CCDM) for semantic segmentation based on Denoising Diffusion Probabilistic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from divergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, and outperforms established baselines on the classical segmentation dataset Cityscapes.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zbinden_Stochastic_Segmentation_with_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lukas Zbinden</author><author>Lars Doorenbos</author><author>Theodoros Pissas</author><author>Adrian Thomas Huber</author><author>Raphael Sznitman</author><author>Pablo M rquez-Neila</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain&apos;s Recognition Mechanism</title>
            <abstract>There are two main approaches to object detection: CNN-based and Transformer-based. The former views object detection as a dense local matching problem, while the latter sees it as a sparse global retrieval problem. Research in neuroscience has shown that the recognition decision in the brain is based on two processes, namely familiarity and recollection. Based on this biological support, we propose an efficient and effective dual-processing object detection framework. It integrates CNN- and Transformer-based detectors into a comprehensive object detection system consisting of a shared backbone, an efficient dual-stream encoder, and a dynamic dual-decoder. To better integrate local and global features, we design a search space for the CNN-Transformer dual-stream encoder to find the optimal fusion solution. To enable better coordination between the CNN- and Transformer-based decoders, we provide the dual-decoder with a selective mask. This mask dynamically chooses the more advantageous decoder for each position in the image based on high-level representation. As demonstrated by extensive experiments, our approach shows flexibility and effectiveness in prompting the mAP of the various source detectors by 3.0 3.7 without increasing FLOPs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_A_Dynamic_Dual-Processing_Object_Detection_Framework_Inspired_by_the_Brains_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Minying Zhang</author><author>Tianpeng Bu</author><author>Lulu Hu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient</title>
            <abstract>Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Hard_No-Box_Adversarial_Attack_on_Skeleton-Based_Human_Action_Recognition_with_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lu_Hard_No-Box_Adversarial_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Zhengzhi Lu</author><author>He Wang</author><author>Ziyi Chang</author><author>Guoan Yang</author><author>Hubert P. H. Shum</author>
            </authors>
        </paper>
        

        <paper>
            <title>GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving</title>
            <abstract>Autonomous vehicles operating in complex real-world environments require accurate predictions of interactive behaviors between traffic participants. This paper tackles the interaction prediction problem by formulating it with hierarchical game theory and proposing the GameFormer model for its implementation. The model incorporates a Transformer encoder, which effectively models the relationships between scene elements, alongside a novel hierarchical Transformer decoder structure. At each decoding level, the decoder utilizes the prediction outcomes from the previous level, in addition to the shared environmental context, to iteratively refine the interaction process. Moreover, we propose a learning process that regulates an agent&apos;s behavior at the current level to respond to other agents&apos; behaviors from the preceding level. Through comprehensive experiments on large-scale real-world driving datasets, we demonstrate the state-of-the-art accuracy of our model on the Waymo interaction prediction task. Additionally, we validate the model&apos;s capacity to jointly reason about the motion plan of the ego agent and the behaviors of multiple agents in both open-loop and closed-loop planning tests, outperforming various baseline methods. Furthermore, we evaluate the efficacy of our model on the nuPlan planning benchmark, where it achieves leading performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_GameFormer_Game-theoretic_Modeling_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhiyu Huang</author><author>Haochen Liu</author><author>Chen Lv</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels</title>
            <abstract>Conventional multi-label classification (MLC) methods assume that all samples are fully labeled and identically distributed. Unfortunately, this assumption is unrealistic in large-scale MLC data that has long-tailed (LT) distribution and partial labels (PL). To address the problem, we introduce a novel task, Partial labeling and Long-Tailed Multi-Label Classification (PLT-MLC), to jointly consider the above two imperfect learning environments. Not surprisingly, we find that most LT-MLC and PL-MLC approaches fail to solve the PLT-MLC, resulting in significant performance degradation on the two proposed PLT-MLC benchmarks. Therefore, we propose an end-to-end learning framework: COrrection -&gt; ModificatIon -&gt; balanCe, abbreviated as COMC. Our bootstrapping philosophy is to simultaneously correct the missing labels (Correction) with convinced prediction confidence over a class-aware threshold and to learn from these recall labels during training. We next propose a novel multi-focal modifier loss that simultaneously addresses head-tail imbalance and positive-negative imbalance to adaptively modify the attention to different samples (Modification) under the LT class distribution. We also develop a balanced training strategy by distilling the model&apos;s learning effect from head and tail samples, and thus design the balanced classifier (Balance) conditioned on the head and tail learning effect to maintain a stable performance. Our experimental study shows that the proposed method significantly outperforms the general MLC, LT-MLC and ML-MLC methods in terms of effectiveness and robustness on our newly created PLT-MLC datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Learning_in_Imperfect_Environment_Multi-Label_Classification_with_Long-Tailed_Distribution_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Wenqiao Zhang</author><author>Changshuo Liu</author><author>Lingze Zeng</author><author>Bengchin Ooi</author><author>Siliang Tang</author><author>Yueting Zhuang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance</title>
            <abstract>In real-world scenarios, typical visual recognition systems could fail under two major causes, i.e., the misclassification between known classes and the excusable misbehavior on unknown-class images. To tackle these deficiencies, flexible visual recognition should dynamically predict multiple classes when they are unconfident between choices and reject making predictions when the input is entirely out of the training distribution. Two challenges emerge along with this novel task. First, prediction uncertainty should be separately quantified as confusion depicting inter-class uncertainties and ignorance identifying out-of-distribution samples. Second, both confusion and ignorance should be comparable between samples to enable effective decision-making. In this paper, we propose to model these two sources of uncertainty explicitly with the theory of Subjective Logic. Regarding recognition as an evidence-collecting process, confusion is then defined as conflicting evidence, while ignorance is the absence of evidence. By predicting Dirichlet concentration parameters for singletons, comprehensive subjective opinions, including confusion and ignorance, could be achieved via further evidence combinations. Through a series of experiments on synthetic data analysis, visual recognition, and open-set detection, we demonstrate the effectiveness of our methods in quantifying two sources of uncertainties and dealing with flexible recognition.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fan_Flexible_Visual_Recognition_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lei Fan</author><author>Bo Liu</author><author>Haoxiang Li</author><author>Ying Wu</author><author>Gang Hua</author>
            </authors>
        </paper>
        

        <paper>
            <title>Texture Generation on 3D Meshes with Point-UV Diffusion</title>
            <abstract>In this work, we focus on synthesizing high-quality textures on 3D meshes. We present Point-UV diffusion, a coarse-to-fine pipeline that marries the denoising diffusion model with UV mapping to generate 3D consistent and high-quality texture images in UV space. We start with introducing a point diffusion model to synthesize low-frequency texture components with our tailored style guidance to tackle the biased color distribution. The derived coarse texture offers global consistency and serves as a condition for the subsequent UV diffusion stage, aiding in regularizing the model to generate a 3D consistent UV texture image. Then, a UV diffusion model with hybrid conditions is developed to enhance the texture fidelity in the 2D UV space. Our method can process meshes of any genus, generating diversified, geometry-compatible, and high-fidelity textures.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_Texture_Generation_on_3D_Meshes_with_Point-UV_Diffusion_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xin Yu</author><author>Peng Dai</author><author>Wenbo Li</author><author>Lan Ma</author><author>Zhengzhe Liu</author><author>Xiaojuan Qi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Enhanced Soft Label for Semi-Supervised Semantic Segmentation</title>
            <abstract>As a mainstream framework in the field of semi-supervised learning (SSL), self-training via pseudo labeling and its variants have witnessed impressive progress in semi-supervised semantic segmentation with the recent advance of deep neural networks. However, modern self-training based SSL algorithms use a pre-defined constant threshold to select unlabeled pixel samples that contribute to the training, thus failing to be compatible with different learning difficulties of variant categories and different learning status of the model. To address these issues, we propose Enhanced Soft Label (ESL), a curriculum learning approach to fully leverage the high-value supervisory signals implicit in the untrustworthy pseudo label. ESL believes that pixels with unconfident predictions can be pretty sure about their belonging to a subset of dominant classes though being arduous to determine the exact one. It thus contains a Dynamic Soft Label (DSL) module to dynamically maintain the high probability classes, keeping the label &quot;soft&quot; so as to make full use of the high entropy prediction. However, the DSL itself will inevitably introduce ambiguity between dominant classes, thus blurring the classification boundary. Therefore, we further propose a pixel-to-part contrastive learning method cooperated with an unsupervised object part grouping mechanism to improve its ability to distinguish between different classes. Extensive experimental results on Pascal VOC 2012 and Cityscapes show that our approach achieves remarkable improvements over existing state-of-the-art approaches.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ma_Enhanced_Soft_Label_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jie Ma</author><author>Chuan Wang</author><author>Yang Liu</author><author>Liang Lin</author><author>Guanbin Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer</title>
            <abstract>Vehicle-to-Vehicle technologies have enabled autonomous vehicles to share information to see through occlusions, greatly enhancing perception performance. Nevertheless, existing works all focused on homogeneous traffic where vehicles are equipped with the same type of sensors, which significantly hampers the scale of collaboration and benefit of cross-modality interactions. In this paper, we investigate the multi-agent hetero-modal cooperative perception problem where agents may have distinct sensor modalities. We present HM-ViT, the first unified multi-agent hetero-modal cooperative perception framework that can collaboratively predict 3D objects for highly dynamic Vehicle-to-Vehicle (V2V) collaborations with varying numbers and types of agents. To effectively fuse features from multi-view images and LiDAR point clouds, we design a novel heterogeneous 3D graph transformer to jointly reason inter-agent and intra-agent interactions. The extensive experiments on the V2V perception dataset OPV2V demonstrate that the HM-ViT outperforms SOTA cooperative perception methods for V2V hetero-modal cooperative perception. Our code will be released at https://github.com/XHwind/HM-ViT.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Hao Xiang</author><author>Runsheng Xu</author><author>Jiaqi Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces</title>
            <abstract>In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose. Existing state-of-the-art face reenactment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics. We propose to address these limitations by leveraging the photorealistic generation ability and the disentangled properties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hypernetwork to perform: (i) refinement of the source identity characteristics and (ii) facial pose re-targeting, eliminating this way the dependence on external editing methods that typically produce artifacts. Our method operates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning. We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and VoxCeleb2, demonstrating the superiority of our approach in producing artifact-free images, exhibiting remarkable robustness even under extreme head pose changes. We make the code and the pretrained models publicly available at: https://github.com/StelaBou/HyperReenact</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bounareli_HyperReenact_One-Shot_Reenactment_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Stella Bounareli</author><author>Christos Tzelepis</author><author>Vasileios Argyriou</author><author>Ioannis Patras</author><author>Georgios Tzimiropoulos</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unified Visual Relationship Detection with Vision and Language Models</title>
            <abstract>This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model. Our code will be made publicly available on GitHub.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Unified_Visual_Relationship_Detection_with_Vision_and_Language_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhao_Unified_Visual_Relationship_Detection_with_Vision_and_Language_Models_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Long Zhao</author><author>Liangzhe Yuan</author><author>Boqing Gong</author><author>Yin Cui</author><author>Florian Schroff</author><author>Ming-Hsuan Yang</author><author>Hartwig Adam</author><author>Ting Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis</title>
            <abstract>While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nudity or violence, and help to make image generation safer.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Struppek_Rickrolling_the_Artist_Injecting_Backdoors_into_Text_Encoders_for_Text-to-Image_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Struppek_Rickrolling_the_Artist_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lukas Struppek</author><author>Dominik Hintersdorf</author><author>Kristian Kersting</author>
            </authors>
        </paper>
        

        <paper>
            <title>LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation</title>
            <abstract>Large-scale pre-training tasks like image classification, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, recent generative foundation models built using text-based latent diffusion techniques may learn semantic boundaries. This is because they have to synthesize intricate details about all objects in an image based on a text description. Therefore, we present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20% improvement compared to state-of-the-art techniques. The project is available at https://koutilya-pnvr.github.io/LD-ZNet/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/PNVR_LD-ZNet_A_Latent_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Koutilya PNVR</author><author>Bharat Singh</author><author>Pallabi Ghosh</author><author>Behjat Siddiquie</author><author>David Jacobs</author>
            </authors>
        </paper>
        

        <paper>
            <title>Downstream-agnostic Adversarial Examples</title>
            <abstract>Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of &quot;big model&quot;. Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use. In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_Downstream-agnostic_Adversarial_Examples_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ziqi Zhou</author><author>Shengshan Hu</author><author>Ruizhi Zhao</author><author>Qian Wang</author><author>Leo Yu Zhang</author><author>Junhui Hou</author><author>Hai Jin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Studying How to Efficiently and Effectively Guide Models with Explanations</title>
            <abstract>Despite being highly performant, deep neural networks might base their decisions on features that spuriously correlate with the provided labels, thus hurting generalization. To mitigate this, &apos;model guidance&apos; has recently gained popularity, i.e. the idea of regularizing the models&apos; explanations to ensure that they are &quot;right for the right reasons&quot;. While various techniques to achieve such model guidance have been proposed, experimental validation of these approaches has thus far been limited to relatively simple and / or synthetic datasets. To better understand the effectiveness of the various design choices that have been explored in the context of model guidance, in this work we conduct an in-depth evaluation across various loss functions, attribution methods, models, and &apos;guidance depths&apos; on the PASCAL VOC 2007 and MS COCO 2014 datasets. As annotation costs for model guidance can limit its applicability, we also place a particular focus on efficiency. Specifically, we guide the models via bounding box annotations, which are much cheaper to obtain than the commonly used segmentation masks, and evaluate the robustness of model guidance under limited (e.g. with only 1% of annotated images) or overly coarse annotations. Further, we propose using the EPG score as an additional evaluation metric and loss function (&apos;Energy loss&apos;). We show that optimizing for the Energy loss leads to models that exhibit a distinct focus on object-specific features, despite only using bounding box annotations that also include background regions. Lastly, we show that such model guidance can improve generalization under distribution shifts. Code available at: https://github.com/sukrutrao/Model-Guidance</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Rao_Studying_How_to_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sukrut Rao</author><author>Moritz B hle</author><author>Amin Parchami-Araghi</author><author>Bernt Schiele</author>
            </authors>
        </paper>
        

        <paper>
            <title>SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training</title>
            <abstract>Skeleton sequence representation learning has shown great advantages for action recognition due to its promising ability to model human joints and topology. However, the current methods usually require sufficient labeled data for training computationally expensive models. Moreover, these methods ignore how to utilize the fine-grained dependencies among different skeleton joints to pre-train an efficient skeleton sequence learning model that can generalize well across different datasets. In this paper, we propose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehensively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmetric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into graph convolutional network and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and outperforms the state-of-the-art self-supervised skeleton-based methods on FineGym, Diving48, NTU 60 and NTU 120 datasets. Moreover, we obtain comparable performance to some fully supervised methods. The code is avaliable at https://github.com/HongYan1123/SkeletonMAE.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yan_SkeletonMAE_Graph-based_Masked_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hong Yan</author><author>Yang Liu</author><author>Yushen Wei</author><author>Zhen Li</author><author>Guanbin Li</author><author>Liang Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Pose-Free Neural Radiance Fields via Implicit Pose Regularization</title>
            <abstract>Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed multi-view images and it has achieved very impressive success in recent years. Most existing works share the pipeline of training a coarse pose estimator with rendered images at first, followed by a joint optimization of estimated poses and neural radiance field. However, as the pose estimator is trained with only rendered images, the pose estimation is usually biased or inaccurate for real images due to the domain gap between real images and rendered images, leading to poor robustness for the pose estimation of real images and further local min- ima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF that introduces implicit pose regularization to refine pose estimator with unposed real images and improve the robustness of the pose estimation for real images. With a collection of 2D images of a specific scene, IR-NeRF constructs a scene codebook that stores scene features and captures the scene-specific pose distribution implicitly as priors. Thus, the robustness of pose estimation can be promoted with the scene priors according to the rationale that a 2D real image can be well reconstructed from the scene codebook only when its estimated pose lies within the pose distribution. Extensive experiments show that IR-NeRF achieves superior novel view synthesis and outperforms the state-of-the-art consistently across multiple synthetic and real datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Pose-Free_Neural_Radiance_Fields_via_Implicit_Pose_Regularization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jiahui Zhang</author><author>Fangneng Zhan</author><author>Yingchen Yu</author><author>Kunhao Liu</author><author>Rongliang Wu</author><author>Xiaoqin Zhang</author><author>Ling Shao</author><author>Shijian Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories</title>
            <abstract>We propose Encyclopedic-VQA, a large scale visual question answering (VQA) dataset featuring visual questions about detailed properties of fine-grained categories and instances. It contains 221k unique question+answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples. Moreover, our dataset comes with a controlled knowledge base derived from Wikipedia, marking the evidence to support each answer. Empirically, we show that our dataset poses a hard challenge for large vision+language models as they perform poorly on our dataset: PaLI [9] is state-of-the-art on OK-VQA [29], yet it only achieves 13.0% accuracy on our dataset. Moreover, we experimentally show that progress on answering our encyclopedic questions can be achieved by augmenting large models with a mechanism that retrieves relevant information for the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and an automatic retrieval- augmented prototype yields 48.8%. We believe that our dataset enables future research on retrieval-augmented vision+language models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mensink_Encyclopedic_VQA_Visual_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Thomas Mensink</author><author>Jasper Uijlings</author><author>Lluis Castrejon</author><author>Arushi Goel</author><author>Felipe Cadar</author><author>Howard Zhou</author><author>Fei Sha</author><author>Andr Araujo</author><author>Vittorio Ferrari</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View</title>
            <abstract>This paper aims to explain the generalization of deepfake detectors from the novel perspective of multi-order interactions among visual concepts. Specifically, we propose three hypotheses: 1. Deepfake detectors encode multi-order interactions among visual concepts, in which the low-order interactions usually have substantially negative contributions to deepfake detection. 2. Deepfake detectors with better generalization abilities tend to encode low-order interactions with fewer negative contributions. 3. Generalized deepfake detectors usually weaken the negative contributions of low-order interactions by suppressing their strength. Accordingly, we design several mathematical metrics to evaluate the effect of low-order interaction for deepfake detectors. Extensive comparative experiments are conducted, which verify the soundness of our hypotheses. Based on the analyses, we further propose a generic method, which directly reduces the toxic effects of low-order interactions to improve the generalization of deepfake detectors to some extent. The code will be released when the paper is accepted.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yao_Towards_Understanding_the_Generalization_of_Deepfake_Detectors_from_a_Game-Theoretical_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yao_Towards_Understanding_the_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kelu Yao</author><author>Jin Wang</author><author>Boyu Diao</author><author>Chao Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>3DPPE: 3D Point Positional Encoding for Transformer-based Multi-Camera 3D Object Detection</title>
            <abstract>Transformer-based methods have swept the benchmarks on 2D and 3D detection on images. Because tokenization before the attention mechanism drops the spatial information, positional encoding becomes critical for those methods. Recent works found that encodings based on samples of the 3D viewing rays can significantly improve the quality of multi-camera 3D object detection. We hypothesize that 3D point locations can provide more information than rays. Therefore, we introduce 3D point positional encoding, 3DPPE, to the 3D detection Transformer decoder. Although 3D measurements are not available at the inference time of monocular 3D object detection, 3DPPE uses predicted depth to approximate the real point positions. Our hybrid-depth module combines direct and categorical depth to estimate the refined depth of each pixel. Despite the approximation, 3DPPE achieves 46.0 mAP and 51.4 NDS on the competitive nuScenes dataset, significantly outperforming encodings based on ray samples. The codes are available at https://github.com/drilistbox/3DPPE.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shu_3DPPE_3D_Point_Positional_Encoding_for_Transformer-based_Multi-Camera_3D_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shu_3DPPE_3D_Point_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Changyong Shu</author><author>Jiajun Deng</author><author>Fisher Yu</author><author>Yifan Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>VertexSerum: Poisoning Graph Neural Networks for Link Inference</title>
            <abstract>Graph neural networks (GNNs) have brought superb performance to various applications utilizing graph structural data, such as social analysis and fraud detection. The graph links, e.g., social relationships and transaction history, are sensitive and valuable information, which raises privacy concerns when using GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novel graph poisoning attack that increases the effectiveness of graph link stealing by amplifying the link connectivity leakage. To infer node adjacency more accurately, we propose an attention mechanism that can be embedded into the link detection network. Our experiments demonstrate that VertexSerum significantly outperforms the SOTA link inference attack, improving the AUC scores by an average of 9.8% across four real-world datasets and three different GNN structures. Furthermore, our experiments reveal the effectiveness of VertexSerum in both black-box and online learning settings, further validating its applicability in real-world scenarios.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ding_VertexSerum_Poisoning_Graph_Neural_Networks_for_Link_Inference_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ruyi Ding</author><author>Shijin Duan</author><author>Xiaolin Xu</author><author>Yunsi Fei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Deep Geometrized Cartoon Line Inbetweening</title>
            <abstract>We aim to address a significant but understudied problem in the anime industry, namely the inbetweening of cartoon line drawings. Inbetweening involves generating intermediate frames between two black-and-white line drawings and is a time-consuming and expensive process that can benefit from automation. However, existing frame interpolation methods that rely on matching and warping whole raster images are unsuitable for line inbetweening and often produce blurring artifacts that damage the intricate line structures. To preserve the precision and detail of the line drawings, we propose a new approach, called AnimeInbet, which geometrizes raster line drawings into graphs of endpoints and reframes the inbetweening task as a graph fusion problem with vertex repositioning. Our method can effectively capture the sparsity and unique structure of line drawings while preserving the details during inbetweening. This is made possible through our novel modules, i.e., vertex encoding, a vertex correspondence Transformer, an effective mechanism for vertex repositioning and a visibility predictor. To train our method, we introduce MixamoLine240, a new dataset of line drawings with ground truth vectorization and matching labels. Our experiments demonstrate that AnimeInbet synthesizes high-quality, clean, and complete intermediate line drawings, outperforming existing methods quantitatively and qualitatively, especially in cases with large motions.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Siyao_Deep_Geometrized_Cartoon_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Li Siyao</author><author>Tianpei Gu</author><author>Weiye Xiao</author><author>Henghui Ding</author><author>Ziwei Liu</author><author>Chen Change Loy</author>
            </authors>
        </paper>
        

        <paper>
            <title>MatrixCity: A Large-scale City Dataset for City-scale Neural Rendering and Beyond</title>
            <abstract>Neural radiance fields (NeRF) and its subsequent variants have led to remarkable progress in neural rendering. While most of recent neural rendering works focus on objects and small-scale scenes, developing neural rendering methods for city-scale scenes is of great potential in many real-world applications. However, this line of research is impeded by the absence of a comprehensive and high-quality dataset, yet collecting such a dataset over real city-scale scenes is costly, sensitive, and technically infeasible. To this end, we build a large-scale, comprehensive, and high-quality synthetic dataset for city-scale neural rendering researches. Leveraging the Unreal Engine 5 City Sample project, we developed a pipeline to easily collect aerial and street city views, accompanied by ground-truth camera poses and a range of additional data modalities. Flexible controls on environmental factors like light, weather, human and car crowd are also available in our pipeline, supporting the need of various tasks covering city-scale neural rendering and beyond. The resulting pilot dataset, MatrixCity, contains 67k aerial images and 452k street images from two city maps of total size 28km^2. On top of MatrixCity, a thorough benchmark is also conducted, which not only reveals unique challenges of the task of city-scale neural rendering, but also highlights potential improvements for future works. The dataset and code will be publicly available at the project page: https://city-super.github.io/matrixcity/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_MatrixCity_A_Large-scale_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yixuan Li</author><author>Lihan Jiang</author><author>Linning Xu</author><author>Yuanbo Xiangli</author><author>Zhenzhi Wang</author><author>Dahua Lin</author><author>Bo Dai</author>
            </authors>
        </paper>
        

        <paper>
            <title>LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis</title>
            <abstract>This work presents an easy-to-use regularizer for GAN training, which helps explicitly link some axes of the latent space to a set of pixels in the synthesized image. Establishing such a connection facilitates a more convenient local control of GAN generation, where users can alter the image content only within a spatial area simply by partially resampling the latent code. Experimental results confirm four appealing properties of our regularizer, which we call LinkGAN. (1) The latent-pixel linkage is applicable to either a fixed region (i.e., same for all instances) or a particular semantic category (i.e., varying across instances), like the sky. (2) Two or multiple regions can be independently linked to different latent axes, which further supports joint control. (3) Our regularizer can improve the spatial controllability of both 2D and 3D-aware GAN models, barely sacrificing the synthesis performance. (4) The models trained with our regularizer are compatible with GAN inversion techniques and maintain editability on real images.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_LinkGAN_Linking_GAN_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiapeng Zhu</author><author>Ceyuan Yang</author><author>Yujun Shen</author><author>Zifan Shi</author><author>Bo Dai</author><author>Deli Zhao</author><author>Qifeng Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>SVDiff: Compact Parameter Space for Diffusion Fine-Tuning</title>
            <abstract>Recently, diffusion models have achieved remarkable success in text-to-image generation, enabling the creation of high-quality images from text prompts and various conditions. However, existing methods for customizing these models are limited by handling multiple personalized subjects and the risk of overfitting. Moreover, the large parameter space is inefficient for model storage. In this paper, we propose a novel approach to address the limitations in existing text-to-image diffusion models for personalization and customization. Our method involves fine-tuning the singular values of the weight matrices, leading to a compact and efficient parameter space that reduces the risk of overfitting and language-drifting. Our approach also includes a Cut-Mix-Unmix data-augmentation technique to enhance the quality of multi-subject image generation and a simple text-based image editing framework. Our proposed SVDiff method has a significantly smaller model size (1.7MB for StableDiffusion) compared to existing methods, making it more practical for real-world applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_SVDiff_Compact_Parameter_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ligong Han</author><author>Yinxiao Li</author><author>Han Zhang</author><author>Peyman Milanfar</author><author>Dimitris Metaxas</author><author>Feng Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</title>
            <abstract>Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a small- or mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student&apos;s OOD generalization: (1) by better imitating teacher&apos;s visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enriching the teacher&apos;s language representations with informative and finegrained semantic attributes to effectively distinguish between different labels. We propose several metrics and conduct extensive experiments to investigate their techniques. The results demonstrate significant improvements in zero-shot and few-shot student performance on open-vocabulary out-of-distribution classification, highlighting the effectiveness of our proposed approaches.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Distilling_Large_Vision-Language_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xuanlin Li</author><author>Yunhao Fang</author><author>Minghua Liu</author><author>Zhan Ling</author><author>Zhuowen Tu</author><author>Hao Su</author>
            </authors>
        </paper>
        

        <paper>
            <title>What do neural networks learn in image classification? A frequency shortcut perspective</title>
            <abstract>Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transferability of frequency shortcuts on out-of-distribution (OOD) test sets. Our results suggest that frequency shortcuts can be transferred across datasets and cannot be fully avoided by larger model capacity and data augmentation. We recommend that future research should focus on effective training schemes mitigating frequency shortcut learning. Codes and data are available at https://github.com/nis-research/nn-frequency-shortcuts.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_What_do_neural_networks_learn_in_image_classification_A_frequency_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_What_do_neural_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shunxin Wang</author><author>Raymond Veldhuis</author><author>Christoph Brune</author><author>Nicola Strisciuglio</author>
            </authors>
        </paper>
        

        <paper>
            <title>PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3</title>
            <abstract>Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PromptCap is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate PromptCap&apos;s effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. PromptCap outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PromptCap generalizes well to unseen domains.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hu_PromptCap_Prompt-Guided_Image_Captioning_for_VQA_with_GPT-3_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hu_PromptCap_Prompt-Guided_Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yushi Hu</author><author>Hang Hua</author><author>Zhengyuan Yang</author><author>Weijia Shi</author><author>Noah A. Smith</author><author>Jiebo Luo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Periodically Exchange Teacher-Student for Source-Free Object Detection</title>
            <abstract>Source-free object detection (SFOD) aims to adapt the source detector to unlabeled target domain data in the absence of source domain data. Most SFOD methods follow the same self-training paradigm using mean-teacher (MT) framework where the student model is guided by only one single teacher model. However, such paradigm can easily fall into a training instability problem that when the teacher model collapses uncontrollably due to the domain shift, the student model also suffers drastic performance degradation. To address this issue, we propose the Periodically Exchange Teacher-Student (PETS) method, a simple yet novel approach that introduces a multiple-teacher framework consisting of a static teacher, a dynamic teacher, and a student model. During the training phase, we periodically exchange the weights between the static teacher and the student model. Then, we update the dynamic teacher using the moving average of the student model that has already been exchanged by the static teacher. In this way, the dynamic teacher can integrate knowledge from past periods, effectively reducing error accumulation and enabling a more stable training process within the MT-based framework. Further, we develop a consensus mechanism to merge the predictions of two teacher models to provide higher-quality pseudo labels for student model. Extensive experiments on multiple SFOD benchmarks show that the proposed method achieves state-of-the-art performance compared with other related methods, demonstrating the effectiveness and superiority of our method on SFOD task.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Periodically_Exchange_Teacher-Student_for_Source-Free_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Qipeng Liu</author><author>Luojun Lin</author><author>Zhifeng Shen</author><author>Zhifeng Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning to Transform for Generalizable Instance-wise Invariance</title>
            <abstract>Computer vision research has long aimed to build systems that are robust to transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time. We treat invariance as a prediction problem. Given any image, we predict a distribution over transformations. We use variational inference to learn this distribution end-to-end. Combined with a graphical model approach, this distribution forms a flexible, generalizable, and adaptive form of invariance. Our experiments show that it can be used to align datasets and discover prototypes, adapt to out-of-distribution poses, and generalize invariances across classes. When used for data augmentation, our method shows consistent gains in accuracy and robustness on CIFAR 10, CIFAR10-LT, and TinyImageNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Utkarsh Singhal</author><author>Carlos Esteves</author><author>Ameesh Makadia</author><author>Stella X. Yu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multiple Instance Learning Framework with Masked Hard Instance Mining for Whole Slide Image Classification</title>
            <abstract>The whole slide image (WSI) classification is often formulated as a multiple instance learning (MIL) problem. Since the positive tissue is only a small fraction of the gigapixel WSI, existing MIL methods intuitively focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting hard-to-classify instances. Some literature has revealed that hard examples are beneficial for modeling a discriminative boundary accurately. By applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which uses a Siamese structure (Teacher-Student) with a consistency constraint to explore the potential hard instances. With several instance masking strategies based on attention scores, MHIM-MIL employs a momentum teacher to implicitly mine hard instances for training the student model, which can be any attention-based MIL model. This counter-intuitive strategy essentially enables the student to learn a better discriminating boundary. Moreover, the student is used to update the teacher with an exponential moving average (EMA), which in turn identifies new hard instances for subsequent training iterations and stabilizes the optimization. Experimental results on the CAMELYON-16 and TCGA Lung Cancer datasets demonstrate that MHIM-MIL outperforms other latest methods in terms of performance and training cost. The code is available at: https://github.com/DearCaat/MHIM-MIL.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tang_Multiple_Instance_Learning_Framework_with_Masked_Hard_Instance_Mining_for_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tang_Multiple_Instance_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenhao Tang</author><author>Sheng Huang</author><author>Xiaoxian Zhang</author><author>Fengtao Zhou</author><author>Yi Zhang</author><author>Bo Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</title>
            <abstract>Text-to-image generative models have enabled high-resolution image synthesis across different domains, but require users to specify the content they wish to generate. In this paper, we consider the inverse problem - given a collection of different images, can we discover the generative concepts that represent each image? We present an unsupervised approach to discover generative concepts from a collection of images, disentangling different art styles in paintings, objects, and lighting from kitchen scenes, and discovering image classes given ImageNet images. We show how such generative concepts can accurately represent the content of images, be recombined and composed to generate new artistic and hybrid images, and be further used as a representation for downstream classification tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Unsupervised_Compositional_Concepts_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nan Liu</author><author>Yilun Du</author><author>Shuang Li</author><author>Joshua B. Tenenbaum</author><author>Antonio Torralba</author>
            </authors>
        </paper>
        

        <paper>
            <title>Partition-And-Debias: Agnostic Biases Mitigation via a Mixture of Biases-Specific Experts</title>
            <abstract>Bias mitigation in image classification has been widely researched, and existing methods have yielded notable results. However, most of these methods implicitly assume that a given image contains only one type of known or unknown bias, failing to consider the complexities of real-world biases. We introduce a more challenging scenario, agnostic biases mitigation, aiming at bias removal regardless of whether the type of bias or the number of types is unknown in the datasets. To address this difficult task, we present the Partition-and-Debias (PnD) method that uses a mixture of biases-specific experts to implicitly divide the bias space into multiple subspaces and a gating module to find a consensus among experts to achieve debiased classification. Experiments on both public and constructed benchmarks demonstrated the efficacy of the PnD. Code is available at: https://github.com/Jiaxuan-Li/PnD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Partition-And-Debias_Agnostic_Biases_Mitigation_via_a_Mixture_of_Biases-Specific_Experts_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Partition-And-Debias_Agnostic_Biases_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiaxuan Li</author><author>Duc Minh Vo</author><author>Hideki Nakayama</author>
            </authors>
        </paper>
        

        <paper>
            <title>Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes</title>
            <abstract>Object detection via inaccurate bounding box supervision has boosted a broad interest due to the expensive high-quality annotation data or the occasional inevitability of low annotation quality (e.g. tiny objects). The previous works usually utilize multiple instance learning (MIL), which highly depends on category information, to select and refine a low-quality box. Those methods suffer from part domination, object drift and group prediction problems without exploring spatial information. In this paper, we heuristically propose a Spatial Self-Distillation based Object Detector (SSD-Det) to mine spatial information to refine the inaccurate box in a self-distillation fashion. SSD-Det utilizes a Spatial Position Self-Distillation SPSD) module to exploit spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation (SISD) module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation verify our method&apos;s effectiveness and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Spatial_Self-Distillation_for_Object_Detection_with_Inaccurate_Bounding_Boxes_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Spatial_Self-Distillation_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Di Wu</author><author>Pengfei Chen</author><author>Xuehui Yu</author><author>Guorong Li</author><author>Zhenjun Han</author><author>Jianbin Jiao</author>
            </authors>
        </paper>
        

        <paper>
            <title>CC3D: Layout-Conditioned Generation of Compositional 3D Scenes</title>
            <abstract>In this work, we introduce CC3D, a conditional generative model that synthesizes complex 3D scenes conditioned on 2D semantic scene layouts, trained using single-view images. Different from most existing 3D GANs that limit their applicability to aligned single objects, we focus on generating complex scenes with multiple objects, by modeling the compositional nature of 3D scenes. By devising a 2D layout-based approach for 3D synthesis and implementing a new 3D field representation with a stronger geometric inductive bias, we have created a 3D GAN that is both efficient and of high quality, while allowing for a more controllable generation process. Our evaluations on synthetic 3D-FRONT and real-world KITTI-360 datasets demonstrate that our model generates scenes of improved visual and geometric quality in comparison to previous works.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bahmani_CC3D_Layout-Conditioned_Generation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sherwin Bahmani</author><author>Jeong Joon Park</author><author>Despoina Paschalidou</author><author>Xingguang Yan</author><author>Gordon Wetzstein</author><author>Leonidas Guibas</author><author>Andrea Tagliasacchi</author>
            </authors>
        </paper>
        

        <paper>
            <title>TextPSG: Panoptic Scene Graph Generation from Textual Descriptions</title>
            <abstract>Panoptic Scene Graph has recently been proposed for comprehensive scene understanding. However, previous works adopt a fully-supervised learning manner, requiring large amounts of pixel-wise densely-annotated data, which is always tedious and expensive to obtain. To address this limitation, we study a new problem of Panoptic Scene Graph Generation from Purely Textual Descriptions (Caption-to-PSG). The key idea is to leverage the large collection of free image-caption data on the Web alone to generate panoptic scene graphs. The problem is very challenging for three constraints: 1) no location priors; 2) no explicit links between visual regions and textual entities; and 3) no pre-defined concept sets. To tackle this problem, we propose a new framework TextPSG consisting of four modules, i.e., a region grouper, an entity grounder, a segment merger, and a label generator, with several novel techniques. The region grouper first groups image pixels into different segments and the entity grounder then aligns visual segments with language entities based on the textual description of the segment being referred to. The grounding results can thus serve as pseudo labels enabling the segment merger to learn the segment similarity as well as guiding the label generator to learn object semantics and relation predicates, resulting in a fine-grained structured scene understanding. Our framework is effective, significantly outperforming the baselines and achieving strong out-of-distribution robustness. We perform comprehensive ablation studies to corroborate the effectiveness of our design choices and provide an in-depth analysis to highlight future directions. Our code, data, and results are available on our project page: https://vis-www.cs.umass.edu/TextPSG.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhao_TextPSG_Panoptic_Scene_Graph_Generation_from_Textual_Descriptions_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chengyang Zhao</author><author>Yikang Shen</author><author>Zhenfang Chen</author><author>Mingyu Ding</author><author>Chuang Gan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Cross-modal Latent Space Alignment for Image to Avatar Translation</title>
            <abstract>We present a novel method for automatic vectorized avatar generation from a single portrait image. Most existing approaches that create avatars rely on image-to-image translation methods, which present some limitations when applied to 3D rendering, animation, or video. Instead, we leverage modality-specific autoencoders trained on large-scale unpaired portraits and parametric avatars, and then learn a mapping between both modalities via an alignment module trained on a significantly smaller amount of data. The resulting cross-modal latent space preserves facial identity, producing more visually appealing and higher fidelity avatars than previous methods, as supported by our quantitative and qualitative evaluations. Moreover, our method&apos;s virtue of being resolution-independent makes it highly versatile and applicable in a wide range of settings.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/de_Guevara_Cross-modal_Latent_Space_Alignment_for_Image_to_Avatar_Translation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/de_Guevara_Cross-modal_Latent_Space_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Manuel Ladron de Guevara</author><author>Jose Echevarria</author><author>Yijun Li</author><author>Yannick Hold-Geoffroy</author><author>Cameron Smith</author><author>Daichi Ito</author>
            </authors>
        </paper>
        

        <paper>
            <title>Inspecting the Geographical Representativeness of Images from Text-to-Image Models</title>
            <abstract>Recent progress in generative models has resulted in models that produce both realistic as well as relevant images for most textual inputs. These models are being used to generate millions of images everyday, and hold the potential to drastically impact areas such as generative art, digital marketing and data augmentation. Given their outsized impact, it is important to ensure that the generated content reflects the artifacts and surroundings across the globe, rather than over-representing certain parts of the world. In this paper, we measure the geographical representativeness of common nouns (e.g., a house) generated through DALL.E 2 and Stable Diffusion models using a crowdsourced study comprising 540 participants across 27 countries. For deliberately underspecified inputs without country names, the generated images most reflect the surroundings of the United States followed by India, and the top generations rarely reflect surroundings from all other countries (average score less than 3 out of 5). Specifying the country names in the input increases the representativeness by 1.44 points on average on a 5-point Likert scale for DALL.E 2 and 0.75 for Stable Diffusion, however, the overall scores for many countries still remain low, highlighting the need for future models to be more geographically inclusive. Lastly, we examine the feasibility of quantifying the geographical representativeness of generated images without conducting user studies.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Basu_Inspecting_the_Geographical_Representativeness_of_Images_from_Text-to-Image_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Basu_Inspecting_the_Geographical_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Abhipsa Basu</author><author>R. Venkatesh Babu</author><author>Danish Pruthi</author>
            </authors>
        </paper>
        

        <paper>
            <title>HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models</title>
            <abstract>Despite the proven significance of hyperspectral images (HSIs) in performing various computer vision tasks, its potential is adversely affected by the low-resolution (LR) property in the spatial domain, resulting from multiple physical factors. Inspired by recent advancements in deep generative models, we propose an HSI Super-resolution (SR) approach with Conditional Diffusion Models (HSR-Diff) that merges a high-resolution (HR) multispectral image (MSI) with the corresponding LR-HSI. HSR-Diff generates an HR-HSI via repeated refinement, in which the HR-HSI is initialized with pure Gaussian noise and iteratively refined. At each iteration, the noise is removed with a Conditional Denoising Transformer (CDFormer) that is trained on denoising at different noise levels, conditioned on the hierarchical feature maps of HR-MSI and LR-HSI. In addition, a progressive learning strategy is employed to exploit the global information of full-resolution images. Systematic experiments have been conducted on four public datasets, demonstrating that HSR-Diff outperforms state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_HSR-Diff_Hyperspectral_Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chanyue Wu</author><author>Dong Wang</author><author>Yunpeng Bai</author><author>Hanyu Mao</author><author>Ying Li</author><author>Qiang Shen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Advancing Example Exploitation Can Alleviate Critical Challenges in Adversarial Training</title>
            <abstract>Deep neural networks have achieved remarkable results across various tasks. However, they are susceptible to adversarial examples, which are generated by adding adversarial perturbations to original data. Adversarial training (AT) is the most effective defense mechanism against adversarial examples and has received significant attention. Recent studies highlight the importance of example exploitation, where the model&apos;s learning intensity is altered for specific examples to extend classic AT approaches. However, the analysis methodologies employed by these studies are varied and contradictory, which may lead to confusion in future research. To address this issue, we provide a comprehensive summary of representative strategies focusing on exploiting examples within a unified framework. Furthermore, we investigate the role of examples in AT and find that examples which contribute primarily to accuracy or robustness are distinct. Based on this finding, we propose a novel example-exploitation idea that can further improve the performance of advanced AT methods. This new idea suggests that critical challenges in AT, such as the accuracy-robustness trade-off, robust overfitting, and catastrophic overfitting, can be alleviated simultaneously from an example-exploitation perspective. The code can be found in https://github.com/geyao1995/advancing-example-exploitation-in-adversarial-training.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ge_Advancing_Example_Exploitation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yao Ge</author><author>Yun Li</author><author>Keji Han</author><author>Junyi Zhu</author><author>Xianzhong Long</author>
            </authors>
        </paper>
        

        <paper>
            <title>ShiftNAS: Improving One-shot NAS via Probability Shift</title>
            <abstract>One-shot Neural architecture search (One-shot NAS) has been proposed as a time-efficient approach to obtain optimal subnet architectures and weights under different complexity cases by training only once. However, the subnet performance obtained by weight sharing is often inferior to the performance achieved by retraining. In this paper, we investigate the performance gap and attribute it to the use of uniform sampling, which is a common approach in supernet training. Uniform sampling concentrates training resources on subnets with intermediate computational resources, which are sampled with high probability. However, subnets with different complexity regions require different optimal training strategies for optimal performance. To address the problem of uniform sampling, we propose ShiftNAS, a method that can adjust the sampling probability based on the complexity of subnets. We achieve this by evaluating the performance variation of subnets with different complexity and designing an architecture generator that can accurately and efficiently provide subnets with the desired complexity. Both the sampling probability and the architecture generator can be trained end-to-end in a gradient-based manner. With ShiftNAS, we can directly obtain the optimal model architecture and parameters for a given computational complexity. We evaluate our approach on multiple visual network models, including convolutional neural networks (CNNs) and vision transformers (ViTs), and demonstrate that ShiftNAS is model-agnostic. Experimental results on ImageNet show that ShiftNAS can improve the performance of one-shot NAS without additional computational consumption. Source codes are available at GitHub.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_ShiftNAS_Improving_One-shot_NAS_via_Probability_Shift_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_ShiftNAS_Improving_One-shot_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mingyang Zhang</author><author>Xinyi Yu</author><author>Haodong Zhao</author><author>Linlin Ou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Adaptive Testing of Computer Vision Models</title>
            <abstract>Vision models often fail systematically on groups of data that share common semantic characteristics (e.g., rare objects or unusual scenes), but identifying these failure modes is a challenge. We introduce AdaVision, an interactive process for testing vision models which helps users identify and fix coherent failure modes. Given a natural language description of a coherent group, AdaVision retrieves relevant images from LAION-5B with CLIP. The user then labels a small amount of data for model correctness, which is used in successive retrieval rounds to hill-climb towards high-error regions, refining the group definition. Once a group is saturated, AdaVision uses GPT-3 to suggest new group descriptions for the user to explore. We demonstrate the usefulness and generality of AdaVision in user studies, where users find major bugs in state-of-the-art classification, object detection, and image captioning models. These user-discovered groups have failure rates 2-3x higher than those surfaced by automatic error clustering methods. Finally, finetuning on examples found with AdaVision fixes the discovered bugs when evaluated on unseen examples, without degrading in-distribution accuracy, and while also improving performance on out-of-distribution datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gao_Adaptive_Testing_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Irena Gao</author><author>Gabriel Ilharco</author><author>Scott Lundberg</author><author>Marco Tulio Ribeiro</author>
            </authors>
        </paper>
        

        <paper>
            <title>Feature Proliferation -- the &quot;Cancer&quot; in StyleGAN and its Treatments</title>
            <abstract>Despite the success of StyleGAN in image synthesis, the images it synthesizes are not always perfect and the well-known truncation trick has become a standard post-processing technique for StyleGAN to synthesize high-quality images. Although effective, it has long been noted that the truncation trick tends to reduce the diversity of synthesized images and unnecessarily sacrifices many distinct image features. To address this issue, in this paper, we first delve into the StyleGAN image synthesis mechanism and discover an important phenomenon, namely Feature Proliferation, which demonstrates how specific features reproduce with forward propagation. Then, we show how the occurrence of Feature Proliferation results in StyleGAN image artifacts. As an analogy, we refer to it as the &quot;cancer&quot; in StyleGAN from its proliferating and malignant nature. Finally, we propose a novel feature rescaling method that identifies and modulates risky features to mitigate feature proliferation. Thanks to our discovery of Feature Proliferation, the proposed feature rescaling method is less destructive and retains more useful image features than the truncation trick, as it is more fine-grained and works in a lower-level feature space rather than a high-level latent space. Experimental results justify the validity of our claims and the effectiveness of the proposed feature rescaling method. Our code is available at https://github.com/songc42/Feature-proliferation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Song_Feature_Proliferation_--_the_Cancer_in_StyleGAN_and_its_Treatments_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Song_Feature_Proliferation_--_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shuang Song</author><author>Yuanbang Liang</author><author>Jing Wu</author><author>Yu-Kun Lai</author><author>Yipeng Qin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multi-Label Self-Supervised Learning with Scene Images</title>
            <abstract>Self-supervised learning (SSL) methods targeting scene images have seen a rapid growth recently, and they mostly rely on either a dedicated dense matching mechanism or a costly unsupervised object discovery module. This paper shows that instead of hinging on these strenuous operations, quality image representations can be learned by treating scene/multi-label image SSL simply as a multi-label classification problem, which greatly simplifies the learning framework. Specifically, multiple binary pseudo-labels are assigned for each input image by comparing its embeddings with those in two dictionaries, and the network is optimized using the binary cross entropy loss. The proposed method is named Multi-Label Self-supervised learning (MLS). Visualizations qualitatively show that clearly the pseudo-labels by MLS can automatically find semantically similar pseudo-positive pairs across different images to facilitate contrastive learning. MLS learns high quality representations on MS-COCO and achieves state-of-the-art results on classification, detection and segmentation benchmarks. At the same time, MLS is much simpler than existing methods, making it easier to deploy and for further exploration.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Multi-Label_Self-Supervised_Learning_with_Scene_Images_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Multi-Label_Self-Supervised_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ke Zhu</author><author>Minghao Fu</author><author>Jianxin Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization</title>
            <abstract>Backdoor defense, which aims to detect or mitigate the effect of malicious triggers introduced by attackers, is becoming increasingly critical for machine learning security and integrity. Fine-tuning based on benign data is a natural defense to erase the backdoor effect in a backdoored model. However, recent studies show that, given limited benign data, vanilla fine-tuning has poor defense performance. In this work, we firstly investigate the vanilla fine-tuning process for backdoor mitigation from the neuron weight perspective, and find that backdoor-related neurons are only slightly perturbed in the vanilla fine-tuning process, which explains its poor backdoor defense performance. To enhance the fine-tuning based defense, inspired by the observation that the backdoor-related neurons often have larger weight norms, we propose FT-SAM, a novel backdoor defense paradigm that aims to shrink the norms of backdoorrelated neurons by incorporating sharpness-aware minimization with fine-tuning. We demonstrate the effectiveness of our method on several benchmark datasets and network architectures, where it achieves state-of-the-art defense performance, and provide extensive analysis to reveal the FTSAM&apos;s mechanism. Overall, our work provides a promising avenue for improving the robustness of machine learning models against backdoor attacks. Codes are available at https://github.com/SCLBD/BackdoorBench.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Enhancing_Fine-Tuning_Based_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mingli Zhu</author><author>Shaokui Wei</author><author>Li Shen</author><author>Yanbo Fan</author><author>Baoyuan Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Deep Geometry-Aware Camera Self-Calibration from Video</title>
            <abstract>Accurate intrinsic calibration is essential for camera-based 3D perception, yet, it typically requires targets of well-known geometry. Here, we propose a camera self-calibration approach that infers camera intrinsics during application, from monocular videos in the wild. We propose to explicitly model projection functions and multi-view geometry, while leveraging the capabilities of deep neural networks for feature extraction and matching. To achieve this, we build upon recent research on integrating bundle adjustment into deep learning models, and introduce a self-calibrating bundle adjustment layer. The self-calibrating bundle adjustment layer optimizes camera intrinsics through classical Gauss-Newton steps and can be adapted to different camera models without re-training. As a specific realization, we implemented this layer within the deep visual SLAM system DROID-SLAM, and show that the resulting model, DroidCalib, yields state-of-the-art calibration accuracy across multiple public datasets. Our results suggest that the model generalizes to unseen environments and different camera models, including significant lens distortion. Thereby, the approach enables performing 3D perception tasks without prior knowledge about the camera. Code is available at https://github.com/boschresearch/droidcalib.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hagemann_Deep_Geometry-Aware_Camera_Self-Calibration_from_Video_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hagemann_Deep_Geometry-Aware_Camera_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Annika Hagemann</author><author>Moritz Knorr</author><author>Christoph Stiller</author>
            </authors>
        </paper>
        

        <paper>
            <title>Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection</title>
            <abstract>In this paper, we propose a long-sequence modeling framework, named StreamPETR, for multi-view 3D object detection. Built upon the sparse query design in the PETR series, we systematically develop an object-centric temporal mechanism. The model is performed in an online manner and the long-term historical information is propagated through object queries frame by frame. Besides, we introduce a motion-aware layer normalization to model the movement of the objects. StreamPETR achieves significant performance improvements only with negligible computation cost, compared to the single-frame baseline. On the standard nuScenes benchmark, it is the first online multi-view method that achieves comparable performance (67.6% NDS &amp; 65.3% AMOTA) with lidar-based methods. The lightweight version realizes 45.0% mAP and 31.7 FPS, outperforming the state-of-the-art method (SOLOFusion) by 2.3% mAP and 1.8x faster FPS. Code has been available at https://github.com/exiawsh/StreamPETR.git.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Shihao Wang</author><author>Yingfei Liu</author><author>Tiancai Wang</author><author>Ying Li</author><author>Xiangyu Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes</title>
            <abstract>We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding. ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yeshwanth_ScanNet_A_High-Fidelity_Dataset_of_3D_Indoor_Scenes_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yeshwanth_ScanNet_A_High-Fidelity_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Chandan Yeshwanth</author><author>Yueh-Cheng Liu</author><author>Matthias Nie ner</author><author>Angela Dai</author>
            </authors>
        </paper>
        

        <paper>
            <title>Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations</title>
            <abstract>Training deep generative models usually requires a large amount of data. To alleviate the data collection cost, the task of zero-shot GAN adaptation aims to reuse well-trained generators to synthesize images of an unseen target domain without any further training samples. Due to the data absence, the textual description of the target domain and the vision-language models, e.g., CLIP, are utilized to effectively guide the generator. However, with only a single representative text feature instead of real images, the synthesized images gradually lose diversity as the model is optimized, which is also known as mode collapse. To tackle the problem, we propose a novel method to find semantic variations of the target text in the CLIP space. Specifically, we explore diverse semantic variations based on the informative text feature of the target domain while regularizing the uncontrolled deviation of the semantic information. With the obtained variations, we design a novel directional moment loss that matches the first and second moments of image and text direction distributions. Moreover, we introduce elastic weight consolidation and a relation consistency loss to effectively preserve valuable content information from the source domain, e.g., appearances. Through extensive experiments, we demonstrate the efficacy of the proposed methods in ensuring sample diversity in various scenarios of zero-shot GAN adaptation. We also conduct ablation studies to validate the effect of each proposed component. Notably, our model achieves a new state-of-the-art on zero-shot GAN adaptation in terms of both diversity and quality.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jeon_Improving_Diversity_in_Zero-Shot_GAN_Adaptation_with_Semantic_Variations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jeon_Improving_Diversity_in_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Seogkyu Jeon</author><author>Bei Liu</author><author>Pilhyeon Lee</author><author>Kibeom Hong</author><author>Jianlong Fu</author><author>Hyeran Byun</author>
            </authors>
        </paper>
        

        <paper>
            <title>Vox-E: Text-Guided Voxel Editing of 3D Objects</title>
            <abstract>Large scale text-guided diffusion models have garnered significant attention due to their ability to synthesize diverse images that convey complex visual concepts. This generative power has more recently been leveraged to perform text-to-3D synthesis. In this work, we present a technique that harnesses the power of latent diffusion models for editing existing 3D objects. Our method takes oriented 2D images of a 3D object as input and learns a grid-based volumetric representation of it. To guide the volumetric representation to conform to a target text prompt, we follow unconditional text-to-3D methods and optimize a Score Distillation Sampling (SDS) loss. However, we observe that combining this diffusion-guided loss with an image-based regularization loss that encourages the representation not to deviate too strongly from the input object is challenging, as it requires achieving two conflicting goals while viewing only structure-and-appearance coupled 2D projections. Thus, we introduce a novel volumetric regularization loss that operates directly in 3D space, utilizing the explicit nature of our 3D representation to enforce correlation between the global structure of the original and edited object. Furthermore, we present a technique that optimizes cross-attention volumetric grids to refine the spatial extent of the edits. Extensive experiments and comparisons demonstrate the effectiveness of our approach in creating a myriad of edits which cannot be achieved by prior works. Our code and data will be made publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sella_Vox-E_Text-Guided_Voxel_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Etai Sella</author><author>Gal Fiebelman</author><author>Peter Hedman</author><author>Hadar Averbuch-Elor</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection</title>
            <abstract>Anomaly detection (AD), aiming to find samples that deviate from the training distribution, is essential in safety-critical applications. Though recent self-supervised learning based attempts achieve promising results by creating virtual outliers, their training objectives are less faithful to AD which requires a concentrated inlier distribution as well as a dispersive outlier distribution. In this paper, we propose Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation (UniCon-HA), taking into account both the requirements above. Specifically, we explicitly encourage the concentration of inliers and the dispersion of virtual outliers via supervised and unsupervised contrastive losses, respectively. Considering that standard contrastive data augmentation for generating positive views may induce outliers, we additionally introduce a soft mechanism to re-weight each augmented inlier according to its deviation from the inlier distribution, to ensure a purified concentration. Moreover, to prompt a higher concentration, inspired by curriculum learning, we adopt an easy-to-hard hierarchical augmentation strategy and perform contrastive aggregation at different depths of the network based on the strengths of data augmentation. Our method is evaluated under three AD settings including unlabeled one-class, unlabeled multi-class, and labeled multi-class, demonstrating its consistent superiority over other competitors.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Unilaterally_Aggregated_Contrastive_Learning_with_Hierarchical_Augmentation_for_Anomaly_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Unilaterally_Aggregated_Contrastive_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guodong Wang</author><author>Yunhong Wang</author><author>Jie Qin</author><author>Dongming Zhang</author><author>Xiuguo Bao</author><author>Di Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration</title>
            <abstract>Recent work of discrete generative priors, in the form of codebooks, has shown exciting performance for image reconstruction and restoration, since the discrete prior space spanned by the codebooks increases the robustness against diverse image degradations. Nevertheless, these methods require separate training of codebooks for different image categories, which limits their use to specific image categories only (e.g. face, architecture, etc.), and fail to handle arbitrary natural images. In this paper, we propose AdaCode for learning image-adaptive codebooks for class-agnostic image restoration. Instead of learning a single codebook for all categories of images, we learn a set of basis codebooks. For a given input image, AdaCode learns a weight map with which we compute a weighted combination of these basis codebooks for adaptive image restoration. Intuitively, AdaCode is a more flexible and expressive discrete generative prior than previous work. Experimental results show that AdaCode achieves state-of-the-art performance on image reconstruction and restoration tasks, including image super-resolution and inpainting.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Learning_Image-Adaptive_Codebooks_for_Class-Agnostic_Image_Restoration_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Learning_Image-Adaptive_Codebooks_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kechun Liu</author><author>Yitong Jiang</author><author>Inchang Choi</author><author>Jinwei Gu</author>
            </authors>
        </paper>
        

        <paper>
            <title>3D Segmentation of Humans in Point Clouds with Synthetic Data</title>
            <abstract>Segmenting humans in 3D indoor scenes has become increasingly important with the rise of human-centered robotics and AR/VR applications. To this end, we propose the task of joint 3D human semantic segmentation, instance segmentation and multi-human body-part segmentation. Few works have attempted to directly segment humans in cluttered 3D scenes, which is largely due to the lack of annotated training data of humans interacting with 3D scenes. We address this challenge and propose a framework for generating training data of synthetic humans interacting with real 3D scenes. Furthermore, we propose a novel transformer-based model, Human3D, which is the first end-to-end model for segmenting multiple human instances and their body-parts in a unified manner. The key advantage of our synthetic data generation framework is its ability to generate diverse and realistic human-scene interactions, with highly accurate ground truth. Our experiments show that pre-training on synthetic data improves performance on a wide variety of 3D human segmentation tasks. Finally, we demonstrate that Human3D outperforms even task-specific state-of-the-art 3D segmentation methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Takmaz_3D_Segmentation_of_Humans_in_Point_Clouds_with_Synthetic_Data_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Takmaz_3D_Segmentation_of_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Ay a Takmaz</author><author>Jonas Schult</author><author>Irem Kaftan</author><author>Mertcan Ak ay</author><author>Bastian Leibe</author><author>Robert Sumner</author><author>Francis Engelmann</author><author>Siyu Tang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Mastering Spatial Graph Prediction of Road Networks</title>
            <abstract>Accurately predicting road networks from satellite images requires a global understanding of the network topology. We propose to capture such high-level information by introducing a graph-based framework that given a partially generated graph, sequentially adds new edges. To deal with misalignment between the model predictions and the intended purpose, and to optimize over complex, non-continuous metrics of interest, we adopt a reinforcement learning (RL) approach that nominates modifications that maximize a cumulative reward. As opposed to standard supervised techniques that tend to be more restricted to commonly used surrogate losses, our framework yields more power and flexibility to encode problem-dependent knowledge. Empirical results on several benchmark datasets demonstrate enhanced performance and increased high-level reasoning about the graph topology when using a tree-based search. We further demonstrate the superiority of our approach in handling examples with substantial occlusion and additionally provide evidence that our predictions better match the statistical properties of the ground dataset.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sotiris_Mastering_Spatial_Graph_Prediction_of_Road_Networks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sotiris_Mastering_Spatial_Graph_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Anagnostidis Sotiris</author><author>Aurelien Lucchi</author><author>Thomas Hofmann</author>
            </authors>
        </paper>
        

        <paper>
            <title>Domain Generalization via Rationale Invariance</title>
            <abstract>This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer. Specifically, we propose treating the element-wise contributions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the rationale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale invariance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at https://github.com/liangchen527/RIDG.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Domain_Generalization_via_Rationale_Invariance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Domain_Generalization_via_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Liang Chen</author><author>Yong Zhang</author><author>Yibing Song</author><author>Anton van den Hengel</author><author>Lingqiao Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models</title>
            <abstract>Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model selection as two real-world downstream tasks for VLMs and show that the estimated uncertainty aids both tasks. Lastly, we present a novel technique for visualizing the embedding distributions using a large-scale pre-trained latent diffusion model.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Upadhyay_ProbVLM_Probabilistic_Adapter_for_Frozen_Vison-Language_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Upadhyay_ProbVLM_Probabilistic_Adapter_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Uddeshya Upadhyay</author><author>Shyamgopal Karthik</author><author>Massimiliano Mancini</author><author>Zeynep Akata</author>
            </authors>
        </paper>
        

        <paper>
            <title>Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition</title>
            <abstract>Most research on facial expression recognition (FER) is conducted in highly controlled environments, but its performance is often unacceptable when applied to real-world situations. This is because when unexpected objects occlude the face, the FER network faces difficulties extracting facial features and accurately predicting facial expressions. Therefore, occluded FER (OFER) is a challenging problem. Previous studies on occlusion-aware FER have typically required fully annotated facial images for training. However, collecting facial images with various occlusions and expression annotations is time-consuming and expensive. Latent-OFER, the proposed method, can detect occlusions, restore occluded parts of the face as if they were unoccluded, and recognize them, improving FER accuracy. This approach involves three steps: First, the vision transformer (ViT)-based occlusion patch detector masks the occluded position by training only latent vectors from the unoccluded patches using the support vector data description algorithm. Second, the hybrid reconstruction network generates the masking position as a complete image using the ViT and convolutional neural network (CNN). Last, the expression-relevant latent vector extractor retrieves and uses expression-related information from all latent vectors by applying a CNN-based class activation map. This mechanism has a significant advantage in preventing performance degradation from occlusion by unseen objects. The experimental results on several databases demonstrate the superiority of the proposed method over state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Latent-OFER_Detect_Mask_and_Reconstruct_with_Latent_Vectors_for_Occluded_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lee_Latent-OFER_Detect_Mask_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Isack Lee</author><author>Eungi Lee</author><author>Seok Bong Yoo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Self-supervised Cross-view Representation Reconstruction for Change Captioning</title>
            <abstract>Change captioning aims to describe the difference between a pair of similar images. Its key challenge is how to learn a stable difference representation under pseudo changes caused by viewpoint change. In this paper, we address this by proposing a self-supervised cross-view representation reconstruction (SCORER) network. Concretely, we first design a multi-head token-wise matching to model relationships between cross-view features from similar/dissimilar images. Then, by maximizing cross-view contrastive alignment of two similar images, SCORER learns two view-invariant image representations in a self-supervised way. Based on these, we reconstruct the representations of unchanged objects by cross-attention, thus learning a stable difference representation for caption generation. Further, we devise a cross-modal backward reasoning to improve the quality of caption. This module reversely models a &quot;hallucination&quot; representation with the caption and &quot;before&quot; representation. By pushing it closer to the &quot;after&quot; representation, we enforce the caption to be informative about the difference in a self-supervised manner. Extensive experiments show our method achieves the state-of-the-art results on four datasets. The code is available at https://github.com/tuyunbin/SCORER.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tu_Self-supervised_Cross-view_Representation_Reconstruction_for_Change_Captioning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tu_Self-supervised_Cross-view_Representation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yunbin Tu</author><author>Liang Li</author><author>Li Su</author><author>Zheng-Jun Zha</author><author>Chenggang Yan</author><author>Qingming Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation</title>
            <abstract>Automatic radiology report generation has attracted enormous research interest due to its practical value in reducing the workload of radiologists. However, simultaneously establishing global correspondences between the image (e.g., Chest X-ray) and its related report and local alignments between image patches and keywords remains challenging. To this end, we propose an Unify, Align and then Refine (UAR) approach to learn multi-level cross-modal alignments and introduce three novel modules: Latent Space Unifier (LSU), Cross-modal Representation Aligner (CRA) and Text-to-Image Refiner (TIR). Specifically, LSU unifies multimodal data into discrete tokens, making it flexible to learn common knowledge among modalities with a shared network. The modality-agnostic CRA learns discriminative features via a set of orthonormal basis and a dual-gate mechanism first and then globally aligns visual and textual representations under a triplet contrastive loss. TIR boosts token-level local alignment via calibrating text-to-image attention with a learnable mask. Additionally, we design a two-stage training procedure to make UAR gradually grasp cross-modal alignments at different levels, which imitates radiologists&apos; workflow: writing sentence by sentence first and then checking word by word. Extensive experiments and analyses on IU-Xray and MIMIC-CXR benchmark datasets demonstrate the superiority of our UAR against varied state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yaowei Li</author><author>Bang Yang</author><author>Xuxin Cheng</author><author>Zhihong Zhu</author><author>Hongxiang Li</author><author>Yuexian Zou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Scene-Aware Feature Matching</title>
            <abstract>Current feature matching methods focus on point-level matching, pursuing better representation learning of individual features, but lacking further understanding of the scene. This results in significant performance degradation when handling challenging scenes such as scenes with large viewpoint and illumination changes. To tackle this problem, we propose a novel model named SAM, which applies attentional grouping to guide Scene-Aware feature Matching. SAM handles multi-level features, i.e., image tokens and group tokens, with attention layers, and groups the image tokens with the proposed token grouping module. Our model can be trained by ground-truth matches only and produce reasonable grouping results. With the sense-aware grouping guidance, SAM is not only more accurate and robust but also more interpretable than conventional feature matching models. Sufficient experiments on various applications, including homography estimation, pose estimation, and image matching, demonstrate that our model achieves state-of-the-art performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Scene-Aware_Feature_Matching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xiaoyong Lu</author><author>Yaping Yan</author><author>Tong Wei</author><author>Songlin Du</author>
            </authors>
        </paper>
        

        <paper>
            <title>FDViT: Improve the Hierarchical Architecture of Vision Transformer</title>
            <abstract>Despite the fact that transformer-based models have yielded great success in computer vision tasks, they suffer from the challenge of high computational costs that limits their use on resource-constrained devices. One major reason is that vision transformers have redundant calculations since the self-attention operation generates patches with high similarity at a later stage in the network. Hierarchical architectures have been proposed for vision transformers to alleviate this challenge. However, by shrinking the spatial dimensions to half of the originals with downsampling layers, the challenge is actually overcompensated, as too much information is lost. In this paper, we propose FDViT to improve the hierarchical architecture of the vision transformer by using a flexible downsampling layer that is not limited to integer stride to smoothly reduce the sizes of the middle feature maps. Furthermore, a masked auto-encoder architecture is used to facilitate the training of the proposed flexible downsampling layer and produces informative outputs. Experimental results on benchmark datasets demonstrate that the proposed method can reduce computational costs while increasing classification performance and achieving state-of-the-art results. For example, the proposed FDViT-S model achieves a top-1 accuracy of 81.5%, which is 1.7 percent points higher than the ViT-S model and reduces 39% FLOPs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_FDViT_Improve_the_Hierarchical_Architecture_of_Vision_Transformer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yixing Xu</author><author>Chao Li</author><author>Dong Li</author><author>Xiao Sheng</author><author>Fan Jiang</author><author>Lu Tian</author><author>Ashish Sirasao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Robust Model Watermark via Reducing Parametric Vulnerability</title>
            <abstract>Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is &quot;stolen&quot; from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parametric space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a minimax formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermarking against parametric changes and numerous watermark-removal attacks. The codes for reproducing our main experiments are available at https://github.com/GuanhaoGan/robust-model-watermarking.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gan_Towards_Robust_Model_Watermark_via_Reducing_Parametric_Vulnerability_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gan_Towards_Robust_Model_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guanhao Gan</author><author>Yiming Li</author><author>Dongxian Wu</author><author>Shu-Tao Xia</author>
            </authors>
        </paper>
        

        <paper>
            <title>LEA2: A Lightweight Ensemble Adversarial Attack via Non-overlapping Vulnerable Frequency Regions</title>
            <abstract>Recent work shows that well-designed adversarial examples can fool deep neural networks (DNNs). Due to their transferability, adversarial examples can also attack target models without extra information, called black-box attacks. However, most existing ensemble attacks depend on numerous substitute models to cover the vulnerable subspace of a target model. In this work, we find three types of models with non-overlapping vulnerable frequency regions, which can cover a large enough vulnerable subspace. Based on this finding, we propose a lightweight ensemble adversarial attack named LEA2, integrated by standard, weakly robust, and robust models. Moreover, we analyze Gaussian noise from the perspective of frequency and find that Gaussian noise is located in the vulnerable frequency regions of standard models. Therefore, we substitute standard models with Gaussian noise to ensure the use of high-frequency vulnerable regions while reducing attack time consumption. Experiments on several image datasets indicate that LEA^2 achieves better transferability under different defended models compared with extensive baselines and state-of-the-art attacks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qian_LEA2_A_Lightweight_Ensemble_Adversarial_Attack_via_Non-overlapping_Vulnerable_Frequency_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Qian_LEA2_A_Lightweight_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yaguan Qian</author><author>Shuke He</author><author>Chenyu Zhao</author><author>Jiaqiang Sha</author><author>Wei Wang</author><author>Bin Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Domain Adaptive Detection with Network Stability Analysis</title>
            <abstract>Domain adaptive detection aims to improve the generality of a detector, learned from the labeled source domain, on the unlabeled target domain. In this work, drawing inspiration from the concept of stability from the control theory that a robust system requires to remain consistent both externally and internally regardless of disturbances, we propose a novel framework that achieves unsupervised domain adaptive detection through stability analysis. In specific, we treat discrepancies between images and regions from different domains as disturbances, and introduce a novel simple but effective Network Stability Analysis (NSA) framework that considers various disturbances for domain adaptation. Particularly, we explore three types of perturbations including heavy and light image-level disturbances and instance-level disturbance. For each type, NSA performs external consistency analysis on the outputs from raw and perturbed images and/or internal consistency analysis on their features, using teacher-student models. By integrating NSA into Faster R-CNN, we immediately achieve state-of-the-art results. In particular, we set a new record of 52.7% mAP on Cityscapes-to-FoggyCityscapes, showing the potential of NSA for domain adaptive detection. It is worth noticing, our NSA is designed for general purpose, and thus applicable to one-stage detection model (e.g., FCOS) besides the adopted one, as shown by experiments. Code is released at https://github.com/tiankongzhang/NSA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_Unsupervised_Domain_Adaptive_Detection_with_Network_Stability_Analysis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhou_Unsupervised_Domain_Adaptive_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenzhang Zhou</author><author>Heng Fan</author><author>Tiejian Luo</author><author>Libo Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions</title>
            <abstract>This paper strives for motion expressions guided video segmentation, which focuses on segmenting objects in video content based on a sentence describing the motion of the objects. Existing referring video object datasets typically focus on salient objects and use language expressions that contain excessive static attributes that could potentially enable the target object to be identified in a single frame. These datasets downplay the importance of motion in video content for language-guided video object segmentation. To investigate the feasibility of using motion expressions to ground and segment objects in videos, we propose a large-scale dataset called MeViS, which contains numerous motion expressions to indicate target objects in complex environments. We benchmarked 5 existing referring video object segmentation (RVOS) methods and conducted a comprehensive comparison on the MeViS dataset. The results show that current RVOS methods cannot effectively address motion expression-guided video segmentation. We further analyze the challenges and propose a baseline approach for the proposed MeViS dataset. The goal of our benchmark is to provide a platform that enables the development of effective language-guided video segmentation algorithms that leverage motion expressions as a primary cue for object segmentation in complex video scenes. The proposed MeViS dataset has been released at https://henghuiding.github.io/MeViS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Henghui Ding</author><author>Chang Liu</author><author>Shuting He</author><author>Xudong Jiang</author><author>Chen Change Loy</author>
            </authors>
        </paper>
        

        <paper>
            <title>OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions</title>
            <abstract>The pretrain-finetune paradigm in modern computer vision facilitates the success of self-supervised learning, which tends to achieve better transferability than supervised learning. However, with the availability of massive labeled data, a natural question emerges: how to train a better model with both self and full supervision signals? In this paper, we propose Omni-suPErvised Representation leArning with hierarchical supervisions (OPERA) as a solution. We provide a unified perspective of supervisions from labeled and unlabeled data and propose a unified framework of fully supervised and self-supervised learning. We extract a set of hierarchical proxy representations for each image and impose self and full supervisions on the corresponding proxy representations. Extensive experiments on both convolutional neural networks and vision transformers demonstrate the superiority of OPERA in image classification, segmentation, and object detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_OPERA_Omni-Supervised_Representation_Learning_with_Hierarchical_Supervisions_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Chengkun Wang</author><author>Wenzhao Zheng</author><author>Zheng Zhu</author><author>Jie Zhou</author><author>Jiwen Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning</title>
            <abstract>Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_GPFL_Simultaneously_Learning_Global_and_Personalized_Feature_Information_for_Personalized_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_GPFL_Simultaneously_Learning_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Jianqing Zhang</author><author>Yang Hua</author><author>Hao Wang</author><author>Tao Song</author><author>Zhengui Xue</author><author>Ruhui Ma</author><author>Jian Cao</author><author>Haibing Guan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis</title>
            <abstract>This paper presents ER-NeRF, a novel conditional Neural Radiance Fields (NeRF) based architecture for talking portrait synthesis that can concurrently achieve fast convergence, real-time rendering, and state-of-the-art performance with small model size. Our idea is to explicitly exploit the unequal contribution of spatial regions to guide talking portrait modeling. Specifically, to improve the accuracy of dynamic head reconstruction, a compact and expressive NeRF-based Tri-Plane Hash Representation is introduced by pruning empty spatial regions with three planar hash encoders. For speech audio, we propose a Region Attention Module to generate region-aware condition feature via an attention mechanism. Different from existing methods that utilize an MLP-based encoder to learn the cross-modal relation implicitly, the attention mechanism builds an explicit connection between audio features and spatial regions to capture the priors of local motions. Moreover, a direct and fast Adaptive Pose Encoding is introduced to optimize the head-torso separation problem by mapping the complex transformation of the head pose into spatial coordinates. Extensive experiments demonstrate that our method renders better high-fidelity and audio-lips synchronized talking portrait videos, with realistic details and high efficiency compared to previous methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Efficient_Region-Aware_Neural_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Jiahe Li</author><author>Jiawei Zhang</author><author>Xiao Bai</author><author>Jun Zhou</author><author>Lin Gu</author>
            </authors>
        </paper>
        

        <paper>
            <title>End2End Multi-View Feature Matching with Differentiable Pose Optimization</title>
            <abstract>Erroneous feature matches have severe impact on subsequent camera pose estimation and often require additional, time-costly measures, like RANSAC, for outlier rejection. Our method tackles this challenge by addressing feature matching and pose optimization jointly. To this end, we propose a graph attention network to predict image correspondences along with confidence weights. The resulting matches serve as weighted constraints in a differentiable pose estimation. Training feature matching with gradients from pose optimization naturally learns to down-weight outliers and boosts pose estimation on image pairs compared to SuperGlue by 6.7% on ScanNet. At the same time, it reduces the pose estimation time by over 50% and renders RANSAC iterations unnecessary. Moreover, we integrate information from multiple views by spanning the graph across multiple frames to predict the matches all at once. Multi-view matching combined with end-to-end training improves the pose estimation metrics on Matterport3D by 18.5% compared to SuperGlue.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Roessle_End2End_Multi-View_Feature_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Barbara Roessle</author><author>Matthias Nie ner</author>
            </authors>
        </paper>
        

        <paper>
            <title>Exploring the Benefits of Visual Prompting in Differential Privacy</title>
            <abstract>Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Exploring_the_Benefits_of_Visual_Prompting_in_Differential_Privacy_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yizhe Li</author><author>Yu-Lin Tsai</author><author>Chia-Mu Yu</author><author>Pin-Yu Chen</author><author>Xuebin Ren</author>
            </authors>
        </paper>
        

        <paper>
            <title>Mining bias-target Alignment from Voronoi Cells</title>
            <abstract>Despite significant research efforts, deep neural networks remain vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of biases in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify &quot;bias alignment/misalignment&quot; on target classes and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method with supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, despite being bias-agnostic, even in the presence of multiple biases in the same sample.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Nahon_Mining_bias-target_Alignment_from_Voronoi_Cells_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>R mi Nahon</author><author>Van-Tam Nguyen</author><author>Enzo Tartaglione</author>
            </authors>
        </paper>
        

        <paper>
            <title>The Victim and The Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data</title>
            <abstract>Recently, backdoor attacks have posed a serious security threat to the training process of deep neural networks (DNNs). The attacked model behaves normally on benign samples but outputs a specific result when the trigger is present. However, compared with the rocketing progress of backdoor attacks, existing defenses are difficult to deal with these threats effectively or require benign samples to work, which may be unavailable in real scenarios. In this paper, we find that the poisoned samples and benign samples can be distinguished with prediction entropy. This inspires us to propose a novel dual-network training framework: The Victim and The Beneficiary (V&amp;B), which exploits a poisoned model to train a clean model without extra benign samples. Firstly, we sacrifice the Victim network to be a powerful poisoned sample detector by training on suspicious samples. Secondly, we train the Beneficiary network on the credible samples selected by the Victim to inhibit backdoor injection. Thirdly, a semi-supervised suppression strategy is adopted for erasing potential backdoors and improving model performance. Furthermore, to better inhibit missed poisoned samples, we propose a strong data augmentation method, AttentionMix, which works well with our proposed V&amp;B framework. Extensive experiments on two widely used datasets against 6 state-of-the-art attacks demonstrate that our framework is effective in preventing backdoor injection and robust to various attacks while maintaining the performance on benign samples. Our code is available at https://github.com/Zixuan-Zhu/VaB.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_The_Victim_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zixuan Zhu</author><author>Rui Wang</author><author>Cong Zou</author><author>Lihua Jing</author>
            </authors>
        </paper>
        

        <paper>
            <title>DIFFGUARD: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-Trained Diffusion Models</title>
            <abstract>Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and the original input image. We also present several test-time techniques to further strengthen such differences. Experimental results show that DiffGuard is effective on both Cifar-10 and hard cases of the large-scale ImageNet, and it can be easily combined with existing OOD detection techniques to achieve state-of-the-art OOD detection results.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gao_DIFFGUARD_Semantic_Mismatch-Guided_Out-of-Distribution_Detection_Using_Pre-Trained_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gao_DIFFGUARD_Semantic_Mismatch-Guided_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ruiyuan Gao</author><author>Chenchen Zhao</author><author>Lanqing Hong</author><author>Qiang Xu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Tracking Anything with Decoupled Video Segmentation</title>
            <abstract>Training data for video segmentation are expensive to annotate. This impedes extensions of end-to-end algorithms to new video segmentation tasks, especially in large-vocabulary settings. To &apos;track anything&apos; without training on video data for every individual task, we develop a decoupled video segmentation approach (DEVA), composed of task-specific image-level segmentation and class/task-agnostic bi-directional temporal propagation. Due to this design, we only need an image-level model for the target task (which is cheaper to train) and a universal temporal propagation model which is trained once and generalizes across tasks. To effectively combine these two modules, we use bi-directional propagation for (semi-)online fusion of segmentation hypotheses from different frames to generate a coherent segmentation. We show that this decoupled formulation compares favorably to end-to-end approaches in several data-scarce tasks including large-vocabulary video panoptic segmentation, open-world video segmentation, referring video segmentation, and unsupervised video object segmentation. Code is available at: https://hkchengrex.github.io/Tracking-Anything-with-DEVA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cheng_Tracking_Anything_with_Decoupled_Video_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cheng_Tracking_Anything_with_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ho Kei Cheng</author><author>Seoung Wug Oh</author><author>Brian Price</author><author>Alexander Schwing</author><author>Joon-Young Lee</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generative Gradient Inversion via Over-Parameterized Networks in Federated Learning</title>
            <abstract>Federated learning has gained recognitions as a secure approach for safeguarding local private data in collaborative learning. But the advent of gradient inversion research has posed significant challenges to this premise by enabling a third-party to recover groundtruth images via gradients. While prior research has predominantly focused on low-resolution images and small batch sizes, this study highlights the feasibility of reconstructing complex images with high resolutions and large batch sizes. The success of the proposed method is contingent on constructing an over-parameterized convolutional network, so that images are generated before fitting to the gradient matching requirement. Practical experiments demonstrate that the proposed algorithm achieves high-fidelity image recovery, surpassing state-of-the-art competitors that commonly fail in more intricate scenarios. Consequently, our study shows that local participants in a federated learning system are vulnerable to potential data leakage issues. Source code is available at https://github.com/czhang024/CI-Net.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Generative_Gradient_Inversion_via_Over-Parameterized_Networks_in_Federated_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Generative_Gradient_Inversion_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chi Zhang</author><author>Zhang Xiaoman</author><author>Ekanut Sotthiwat</author><author>Yanyu Xu</author><author>Ping Liu</author><author>Liangli Zhen</author><author>Yong Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>EQ-Net: Elastic Quantization Neural Networks</title>
            <abstract>Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural Networks (EQ-Net), which aims to train a robust weight-sharing quantization supernet. First of all, we propose an elastic quantization space (including elastic bit-width, granularity, and symmetry) to adapt to various mainstream quantitative forms. Secondly, we propose the Weight Distribution Regularization Loss (WDR-Loss) and Group Progressive Guidance Loss (GPG-Loss) to bridge the inconsistency of the distribution for weights and output logits in the elastic quantization space gap. Lastly, we incorporate genetic algorithms and the proposed Conditional Quantization-Aware Accuracy Predictor (CQAP) as an estimator to quickly search mixed-precision quantized neural networks in supernet. Extensive experiments demonstrate that our EQ-Net is close to or even better than its static counterparts as well as state-of-the-art robust bit-width methods. Code can be available at https://github.com/xuke225/EQ-Net.git</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_EQ-Net_Elastic_Quantization_Neural_Networks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_EQ-Net_Elastic_Quantization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ke Xu</author><author>Lei Han</author><author>Ye Tian</author><author>Shangshang Yang</author><author>Xingyi Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only</title>
            <abstract>Semantic segmentation is a crucial task in computer vision that involves segmenting images into semantically meaningful regions at the pixel level. However, existing approaches often rely on expensive human annotations as supervision for model training, limiting their scalability to large, unlabeled datasets. To address this challenge, we present ZeroSeg, a novel method that leverages the existing pretrained vision-language (VL) model (e.g. CLIP vision encoder) to train open-vocabulary zero-shot semantic segmentation models. Although acquired extensive knowledge of visual concepts, it is non-trivial to exploit knowledge from these VL models to the task of semantic segmentation, as they are usually trained at an image level. ZeroSeg overcomes this by distilling the visual concepts learned by VL models into a set of segment tokens, each summarizing a localized region of the target image. We evaluate ZeroSeg on multiple popular segmentation benchmarks, including PASCAL VOC 2012, PASCAL Context, and COCO, in a zero-shot manner Our approach achieves state-of-the-art performance when compared to other zero-shot segmentation methods under the same training data, while also performing competitively compared to strongly supervised methods. Finally, we also demonstrated the effectiveness of ZeroSeg on open-vocabulary segmentation, through both human studies and qualitative visualizations. The code is publicly available at https://github.com/facebookresearch/ZeroSeg</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Exploring_Open-Vocabulary_Semantic_Segmentation_from_CLIP_Vision_Encoder_Distillation_Only_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Exploring_Open-Vocabulary_Semantic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jun Chen</author><author>Deyao Zhu</author><author>Guocheng Qian</author><author>Bernard Ghanem</author><author>Zhicheng Yan</author><author>Chenchen Zhu</author><author>Fanyi Xiao</author><author>Sean Chang Culatana</author><author>Mohamed Elhoseiny</author>
            </authors>
        </paper>
        

        <paper>
            <title>Parallax-Tolerant Unsupervised Deep Image Stitching</title>
            <abstract>Traditional image stitching approaches tend to leverage increasingly complex geometric features (point, line, edge, etc.) for better performance. However, these hand-crafted features are only suitable for specific natural scenes with adequate geometric structures. In contrast, deep stitching schemes overcome adverse conditions by adaptively learning robust semantic features, but they cannot handle large-parallax cases. To solve these issues, we propose a parallax-tolerant unsupervised deep image stitching technique. First, we propose a robust and flexible warp to model the image registration from global homography to local thin-plate spline motion. It provides accurate alignment for overlapping regions and shape preservation for non-overlapping regions by joint optimization concerning alignment and distortion. Subsequently, to improve the generalization capability, we design a simple but effective iterative strategy to enhance the warp adaption in cross-dataset and cross-resolution applications. Finally, to further eliminate the parallax artifacts, we propose to composite the stitched image seamlessly by unsupervised learning for seam-driven composition masks. Compared with existing methods, our solution is parallax-tolerant and free from laborious designs of complicated geometric features for specific scenes. Extensive experiments show our superiority over the SoTA methods, both quantitatively and qualitatively. The code will be available soon.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Nie_Parallax-Tolerant_Unsupervised_Deep_Image_Stitching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Nie_Parallax-Tolerant_Unsupervised_Deep_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lang Nie</author><author>Chunyu Lin</author><author>Kang Liao</author><author>Shuaicheng Liu</author><author>Yao Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>M2T: Masking Transformers Twice for Faster Decoding</title>
            <abstract>We show how bidirectional transformers trained for masked token prediction can be applied to neural image compression to achieve state-of-the-art results. Such models were previously used for image_generation_ by progressive sampling groups of masked tokens according to uncertainty-adaptive schedules. Unlike these works, we demonstrate that predefined, deterministic schedules perform as well or better for image compression. This insight allows us to use masked attention during training in addition to masked inputs, and activation caching during inference, to significantly speed up our models (4x higher inference speed) at a small increase in bitrate.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mentzer_M2T_Masking_Transformers_Twice_for_Faster_Decoding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mentzer_M2T_Masking_Transformers_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Fabian Mentzer</author><author>Eirikur Agustson</author><author>Michael Tschannen</author>
            </authors>
        </paper>
        

        <paper>
            <title>CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations</title>
            <abstract>Recently, 3D object detection with sparse annotations has received great attention. However, current detectors usually perform poorly under very limited annotations. To address this problem, we propose a novel Contrastive Instance feature mining method, named CoIn. To better identify indistinguishable features learned through limited supervision, we design a Multi-Class contrastive learning module (MCcont) to enhance feature discrimination. Meanwhile, we propose a feature-level pseudo-label mining framework consisting of an instance feature mining module (InF-Mining) and a Labeled-to-Pseudo contrastive learning module (LPcont). These two modules exploit latent instances in feature space to supervise the training of detectors with limited annotations. Extensive experiments with KITTI dataset, Waymo open dataset, and nuScenes dataset show that under limited annotations, our method greatly improves the performance of baseline detectors: CenterPoint, Voxel-RCNN, and CasA. Combining CoIn with an iterative training strategy, we propose a CoIn++ pipeline, which requires only 2% annotations in the KITTI dataset to achieve performance comparable to the fully supervised methods. The code is available at https://github.com/xmuqimingxia/CoIn.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xia_CoIn_Contrastive_Instance_Feature_Mining_for_Outdoor_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xia_CoIn_Contrastive_Instance_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiming Xia</author><author>Jinhao Deng</author><author>Chenglu Wen</author><author>Hai Wu</author><author>Shaoshuai Shi</author><author>Xin Li</author><author>Cheng Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Computation and Data Efficient Backdoor Attacks</title>
            <abstract>Backdoor attacks against deep learning have been widely studied. Various attack techniques have been proposed for different domains and paradigms, e.g., image, point cloud, natural language processing, transfer learning, etc. These works normally adopt the data poisoning strategy to embed the backdoor. They randomly select samples from the benign training set for poisoning, without considering the distinct contribution of each sample to the backdoor effectiveness, making the attack less optimal. A recent work (IJCAI-22) proposed to use the forgetting score to measure the importance of each poisoned sample and then filter out redundant data for effective backdoor training. However, this method is empirically designed without theoretical proofing. It is also very time-consuming as it needs to go through almost all the training stages for data selection. To address such limitations, we propose a novel confidence-based scoring methodology, which can efficiently measure the contribution of each poisoning sample based on the distance posteriors. We further introduce a greedy search algorithm to find the most informative samples for backdoor injection more promptly. Experimental evaluations on both 2D image and 3D point cloud classification tasks show that our approach can achieve comparable performance or even surpass the forgetting score-based searching method while requiring only several extra epochs&apos; computation of a standard training process.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Computation_and_Data_Efficient_Backdoor_Attacks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yutong Wu</author><author>Xingshuo Han</author><author>Han Qiu</author><author>Tianwei Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering</title>
            <abstract>In the real world, a desirable Visual Question Answering model is expected to provide correct answers to new questions and images in a continual setting (recognized as CL-VQA). However, existing works formulate CLVQA from a vision-only or language-only perspective, and straightforwardly apply the uni-modal continual learning (CL) strategies to this multi-modal task, which is improper and suboptimal. On the one hand, such a partial formulation may result in limited evaluations. On the other hand, neglecting the interactions between modalities will lead to poor performance. To tackle these challenging issues, we propose a comprehensive formulation for CL-VQA from the perspective of multi-modal vision-language fusion. Based on our formulation, we further propose MulTi-Modal PRompt LearnIng with DecouPLing bEfore InTeraction (TRIPLET), a novel approach that builds on a pre-trained vision-language model and consists of decoupled prompts and prompt interaction strategies to capture the complex interactions between modalities. In particular, decoupled prompts contain learnable parameters that are decoupled w.r.t different aspects, and the prompt interaction strategies are in charge of modeling interactions between inputs and prompts. Additionally, we build two CL-VQA benchmarks for a more comprehensive evaluation. Extensive experiments demonstrate that our TRIPLET outperforms state-of-the-art methods in both uni-modal and multi-modal continual settings for CL-VQA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qian_Decouple_Before_Interact_Multi-Modal_Prompt_Learning_for_Continual_Visual_Question_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Qian_Decouple_Before_Interact_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zi Qian</author><author>Xin Wang</author><author>Xuguang Duan</author><author>Pengda Qin</author><author>Yuhong Li</author><author>Wenwu Zhu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Manifold Linearizing and Clustering</title>
            <abstract>We consider the problem of simultaneously clustering and learning a linear representation of data lying close to a union of low-dimensional manifolds, a fundamental task in machine learning and computer vision. When the manifolds are assumed to be linear subspaces, this reduces to the classical problem of subspace clustering, which has been studied extensively over the past two decades. Unfortunately, many real-world datasets such as natural images can not be well approximated by linear subspaces. On the other hand, numerous works have attempted to learn an appropriate transformation of the data, such that data is mapped from a union of general non-linear manifolds to a union of linear subspaces (with points from the same manifold being mapped to the same subspace). However, many existing works have limitations such as assuming knowledge of the membership of samples to clusters, requiring high sampling density, or being shown theoretically to learn trivial representations. In this paper, we propose to optimize the Maximal Coding Rate Reduction metric with respect to both the data representation and a novel doubly stochastic cluster membership, inspired by state-of-the-art subspace clustering results. We give a parameterization of such a representation and membership, allowing efficient mini-batching and one-shot initialization. Experiments on CIFAR-10, -20, -100, and TinyImageNet-200 datasets show that the proposed method is much more accurate and scalable than state-of-the-art deep clustering methods, and further learns a latent linear representation of the data.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ding_Unsupervised_Manifold_Linearizing_and_Clustering_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Tianjiao Ding</author><author>Shengbang Tong</author><author>Kwan Ho Ryan Chan</author><author>Xili Dai</author><author>Yi Ma</author><author>Benjamin D. Haeffele</author>
            </authors>
        </paper>
        

        <paper>
            <title>MMVP: Motion-Matrix-Based Video Prediction</title>
            <abstract>A central challenge of video prediction lies where the system has to reason the object&apos;s future motion from image frames while simultaneously maintaining the consistency of its appearance across frames. This work introduces an end-to-end trainable two-stream video prediction framework, Motion-Matrix-based Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that usually handle motion prediction and appearance maintenance within the same set of modules, MMVP decouples motion and appearance information by constructing appearance-agnostic motion matrices. The motion matrices represent the temporal similarity of each and every pair of feature patches in the input frames, and are the sole input of the motion prediction module in MMVP. This design improves video prediction in both accuracy and efficiency, and reduces the model size. Results of extensive experiments demonstrate that MMVP outperforms state-of-the-art systems on public data sets by non-negligible large margins (approx. 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the size or smaller). Please refer to https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the official code and the datasets used in this paper.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhong_MMVP_Motion-Matrix-Based_Video_Prediction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhong_MMVP_Motion-Matrix-Based_Video_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yiqi Zhong</author><author>Luming Liang</author><author>Ilya Zharkov</author><author>Ulrich Neumann</author>
            </authors>
        </paper>
        

        <paper>
            <title>Human Preference Score: Better Aligning Text-to-Image Models with Human Preference</title>
            <abstract>Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align_sd_web/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Human_Preference_Score_Better_Aligning_Text-to-Image_Models_with_Human_Preference_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Human_Preference_Score_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiaoshi Wu</author><author>Keqiang Sun</author><author>Feng Zhu</author><author>Rui Zhao</author><author>Hongsheng Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Guided Motion Diffusion for Controllable Human Motion Synthesis</title>
            <abstract>Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of \methodname, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Karunratanakul_Guided_Motion_Diffusion_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Korrawe Karunratanakul</author><author>Konpat Preechakul</author><author>Supasorn Suwajanakorn</author><author>Siyu Tang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models</title>
            <abstract>Collecting and annotating images with pixel-wise labels is time-consuming and laborious. In contrast, synthetic data can be freely available using a generative model (e.g., DALL-E, Stable Diffusion). In this paper, we show that it is possible to automatically obtain accurate semantic masks of synthetic images generated by the pre-trained Stable Diffusion, which uses only text-image pairs during training. Our approach, called DiffuMask, exploits the potential of the cross-attention map between text and image, which is natural and seamless to extend the text-driven image synthesis to semantic mask generation. DiffuMask uses text-guided cross-attention information to localize class/word-specific regions, which are combined with practical techniques to create a novel high-resolution and class-discriminative pixel-wise mask. The methods help to reduce data collection and annotation costs obviously. Experiments demonstrate that the existing segmentation methods trained on synthetic data of DiffuMask can achieve a competitive performance over the counterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird), DiffuMask presents promising performance, close to the state-of-the-art result of real data (within 3% mIoU gap). Moreover, in the open-vocabulary segmentation (zero-shot) setting, DiffuMask achieves a new SOTA result on Unseen class of VOC 2012.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Weijia Wu</author><author>Yuzhong Zhao</author><author>Mike Zheng Shou</author><author>Hong Zhou</author><author>Chunhua Shen</author>
            </authors>
        </paper>
        

        <paper>
            <title>StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-shot and Few-shot Domain Adaptation</title>
            <abstract>Domain adaptation of GANs is a problem of fine-tuning GAN models pretrained on a large dataset (e.g. StyleGAN) to a specific domain with few samples (e.g. painting faces, sketches, etc.). While there are many methods that tackle this problem in different ways, there are still many important questions that remain unanswered. In this paper, we provide a systematic and in-depth analysis of the domain adaptation problem of GANs, focusing on the StyleGAN model. We perform a detailed exploration of the most important parts of StyleGAN that are responsible for adapting the generator to a new domain depending on the similarity between the source and target domains. As a result of this study, we propose new efficient and lightweight parameterizations of StyleGAN for domain adaptation. Particularly, we show that there exist directions in StyleSpace (StyleDomain directions) that are sufficient for adapting to similar domains. For dissimilar domains, we propose Affine+ and AffineLight+ parameterizations that allows us to outperform existing baselines in few-shot adaptation while having significantly less training parameters. Finally, we examine StyleDomain directions and discover their many surprising properties that we apply for domain mixing and cross-domain image morphing. Source code can be found at https://github.com/AIRI-Institute/StyleDomain.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Alanov_StyleDomain_Efficient_and_Lightweight_Parameterizations_of_StyleGAN_for_One-shot_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Alanov_StyleDomain_Efficient_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Aibek Alanov</author><author>Vadim Titov</author><author>Maksim Nakhodnov</author><author>Dmitry Vetrov</author>
            </authors>
        </paper>
        

        <paper>
            <title>RankMixup: Ranking-Based Mixup Training for Network Calibration</title>
            <abstract>Network calibration aims to accurately estimate the level of confidences, which is particularly important for employing deep neural networks in real-world systems. Recent approaches leverage mixup to calibrate the network&apos;s predictions during training. However, they do not consider the problem that mixtures of labels in mixup may not accurately represent the actual distribution of augmented samples. In this paper, we present RankMixup, a novel mixup-based framework alleviating the problem of the mixture of labels for network calibration. To this end, we propose to use an ordinal ranking relationship between raw and mixup-augmented samples as an alternative supervisory signal to the label mixtures for network calibration. We hypothesize that the network should estimate a higher level of confidence for the raw samples than the augmented ones (Fig.1). To implement this idea, we introduce a mixup-based ranking loss (MRL) that encourages lower confidences for augmented samples compared to raw ones, maintaining the ranking relationship. We also propose to leverage the ranking relationship among multiple mixup-augmented samples to further improve the calibration capability. Augmented samples with larger mixing coefficients are expected to have higher confidences and vice versa (Fig.1). That is, the order of confidences should be aligned with that of mixing coefficients. To this end, we introduce a novel loss, M-NDCG, in order to reduce the number of misaligned pairs of the coefficients and confidences. Extensive experimental results on standard benchmarks for network calibration demonstrate the effectiveness of RankMixup.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Noh_RankMixup_Ranking-Based_Mixup_Training_for_Network_Calibration_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Noh_RankMixup_Ranking-Based_Mixup_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jongyoun Noh</author><author>Hyekang Park</author><author>Junghyup Lee</author><author>Bumsub Ham</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis</title>
            <abstract>Existing text-to-image generation approaches have set high standards for photorealism and text-image correspondence, largely benefiting from web-scale text-image datasets, which can include up to 5 billion pairs. However, text-to-image generation models trained on domain-specific datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspondence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a specific domain can be time-consuming and costly. Thus, ensuring high text-image correspondence without relying on web-scale text-image datasets remains a challenging task. In this paper, we present a novel approach for enhancing text-image correspondence by leveraging available semantic layouts. Specifically, we propose a Gaussian-categorical diffusion process that simultaneously generates both images and corresponding layout pairs. Our experiments reveal that we can guide text-to-image generation models to be aware of the semantics of different image regions, by training the model to generate semantic labels for each pixel. We demonstrate that our approach achieves higher text-image correspondence compared to existing text-to-image generation approaches in the Multi-Modal CelebA-HQ and the Cityscapes dataset, where text-image pairs are scarce.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Park_Learning_to_Generate_Semantic_Layouts_for_Higher_Text-Image_Correspondence_in_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Park_Learning_to_Generate_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Minho Park</author><author>Jooyeol Yun</author><author>Seunghwan Choi</author><author>Jaegul Choo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Erasing Concepts from Diffusion Models</title>
            <abstract>Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of specific concepts from diffusion model weights. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gandikota_Erasing_Concepts_from_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Rohit Gandikota</author><author>Joanna Materzynska</author><author>Jaden Fiotto-Kaufman</author><author>David Bau</author>
            </authors>
        </paper>
        

        <paper>
            <title>Fully Attentional Networks with Self-emerging Token Labeling</title>
            <abstract>Recent studies indicate that Vision Transformers (ViTs) are robust against out-of-distribution scenarios. In particular, the Fully Attentional Network (FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In this paper, we revisit the FAN models and improve their pre-training with a self-emerging token labeling (STL) framework. Our method contains a two-stage training framework. Specifically, we first train a FAN token labeler (FAN-TL) to generate semantically meaningful patch token labels, followed by a FAN student model training stage that uses both the token labels and the original class label. With the proposed STL framework, our best model based on FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A (46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the original FAN counterpart by significant margins. The proposed framework also demonstrates significantly enhanced performance on downstream tasks such as semantic segmentation, with up to 1.7% improvement in robustness over the counterpart model.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Fully_Attentional_Networks_with_Self-emerging_Token_Labeling_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Bingyin Zhao</author><author>Zhiding Yu</author><author>Shiyi Lan</author><author>Yutao Cheng</author><author>Anima Anandkumar</author><author>Yingjie Lao</author><author>Jose M. Alvarez</author>
            </authors>
        </paper>
        

        <paper>
            <title>ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion</title>
            <abstract>Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object&apos;s surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflage attack framework designed to generate universal and robust adversarial camouflage capable of concealing any 3D vehicle from detectors. Our framework incorporates innovative techniques to enhance universality and robustness, including a refined texture rendering that enables common texture application to different vehicles without being constrained to a specific texture map, a novel stealth loss that renders the vehicle undetectable, and a smooth and camouflage loss to enhance the naturalness of the adversarial camouflage. Our extensive experiments on 15 different models show that ACTIVE consistently outperforms existing works on various public detectors, including the latest YOLOv7. Notably, our universality evaluations reveal promising transferability to other vehicle classes, tasks (segmentation models), and the real world, not just other vehicles.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Suryanto_ACTIVE_Towards_Highly_Transferable_3D_Physical_Camouflage_for_Universal_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Suryanto_ACTIVE_Towards_Highly_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Naufal Suryanto</author><author>Yongsu Kim</author><author>Harashta Tatimma Larasati</author><author>Hyoeun Kang</author><author>Thi-Thu-Huong Le</author><author>Yoonyoung Hong</author><author>Hunmin Yang</author><author>Se-Yoon Oh</author><author>Howon Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Too Large; Data Reduction for Vision-Language Pre-Training</title>
            <abstract>This paper examines the problems of severe image-text misalignment and high redundancy in the widely-used large-scale Vision-Language Pre-Training (VLP) datasets. To address these issues, we propose an efficient and straightforward Vision-Language learning algorithm called TL;DR which aims to compress the existing large VLP data into a small, high-quality set. Our approach consists of two major steps. First, a codebook-based encoder-decoder captioner is developed to select representative samples. Second, a new caption is generated to complement the original captions for selected samples, mitigating the text-image misalignment problem while maintaining uniqueness. As the result, TL;DR enables us to reduce the large dataset into a small set of high-quality data, which can serve as an alternative pre-training dataset. This algorithm significantly speeds up the time-consuming pretraining process. Specifically, TL;DR can compress the mainstream VLP datasets at a high ratio, e.g., reduce well-cleaned CC3M dataset from 2.8M to 0.67M ( 24%) and noisy YFCC15M from 15M to 2.5M ( 16.7%). Extensive experiments with three popular VLP models over seven downstream tasks show that VLP model trained on the compressed dataset provided by TL;DR can perform similar or even better results compared with training on the full-scale dataset.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Too_Large_Data_Reduction_for_Vision-Language_Pre-Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Alex Jinpeng Wang</author><author>Kevin Qinghong Lin</author><author>David Junhao Zhang</author><author>Stan Weixian Lei</author><author>Mike Zheng Shou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning</title>
            <abstract>Depth-aware panoptic segmentation is an emerging topic in computer vision which combines semantic and geometric understanding for more robust scene interpretation. Recent works pursue unified frameworks to tackle this challenge but mostly still treat it as two individual learning tasks, which limits their potential for exploring cross-domain information. We propose a deeply unified framework for depth-aware panoptic segmentation, which performs joint segmentation and depth estimation both in a per-segment manner with identical object queries. To narrow the gap between the two tasks, we further design a geometric query enhancement method, which is able to integrate scene geometry into object queries using latent representations. In addition, we propose a bi-directional guidance learning approach to facilitate cross-task feature learning by taking advantage of their mutual relations. Our method sets the new state of the art for depth-aware panoptic segmentation on both Cityscapes-DVPS and SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shown to deliver performance improvement even under incomplete supervision labels. Code and models are available at https://github.com/jwh97nn/DeepDPS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/He_Towards_Deeply_Unified_Depth-aware_Panoptic_Segmentation_with_Bi-directional_Guidance_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/He_Towards_Deeply_Unified_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junwen He</author><author>Yifan Wang</author><author>Lijun Wang</author><author>Huchuan Lu</author><author>Bin Luo</author><author>Jun-Yan He</author><author>Jin-Peng Lan</author><author>Yifeng Geng</author><author>Xuansong Xie</author>
            </authors>
        </paper>
        

        <paper>
            <title>Point-Query Quadtree for Crowd Counting, Localization, and More</title>
            <abstract>We show that crowd counting can be viewed as a decomposable point querying process. This formulation enables arbitrary points as input and jointly reasons whether the points are crowd and where they locate. The querying processing, however, raises an underlying problem on the number of necessary querying points. Too few imply underestimation; too many increase computational overhead. To address this dilemma, we introduce a decomposable structure, i.e., the point-query quadtree, and propose a new counting model, termed Point quEry Transformer (PET). PET implements decomposable point querying via data-dependent quadtree splitting, where each querying point could split into four new points when necessary, thus enabling dynamic processing of sparse and dense regions. Such a querying process yields an intuitive, universal modeling of crowd as both the input and output are interpretable and steerable. We demonstrate the applications of PET on a number of crowd-related tasks, including fully-supervised crowd counting and localization, partial annotation learning, and point annotation refinement, and also report state-of-the-art performance. For the first time, we show that a single counting model can address multiple crowd-related tasks across different learning paradigms. Code is available at https://github.com/cxliu0/PET.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Point-Query_Quadtree_for_Crowd_Counting_Localization_and_More_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Point-Query_Quadtree_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chengxin Liu</author><author>Hao Lu</author><author>Zhiguo Cao</author><author>Tongliang Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models</title>
            <abstract>Large-scale text-to-image diffusion models have significantly improved the state of the art in generative image modeling and allow for an intuitive and powerful user interface to drive the image generation process. Expressing spatial constraints, e.g. to position specific objects in particular locations, is cumbersome using text; and current text-based image generation models are not able to accurately follow such instructions. In this paper we consider image generation from text associated with segments on the image canvas, which combines an intuitive natural language interface with precise spatial control over the generated content. We propose ZestGuide, a &quot;ZEro-shot&quot; SegmenTation Guidance approach that can be plugged into pre-trained text-to-image diffusion models, and does not require any additional training. It leverages implicit segmentation maps that can be extracted from cross-attention layers, and uses them to align the generation with input masks. Our experimental results combine high image quality with accurate alignment of generated content with input segmentations, and improve over prior work both quantitatively and qualitatively, including methods that require training on images with corresponding segmentations. Compared to Paint with Words, the previous state-of-the art in image generation with zero-shot segmentation conditioning, we improve by 5 to 10 mIoU points on the COCO dataset with similar FID scores.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Couairon_Zero-Shot_Spatial_Layout_Conditioning_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Couairon_Zero-Shot_Spatial_Layout_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guillaume Couairon</author><author>Marl ne Careil</author><author>Matthieu Cord</author><author>St phane Lathuili re</author><author>Jakob Verbeek</author>
            </authors>
        </paper>
        

        <paper>
            <title>SegGPT: Towards Segmenting Everything in Context</title>
            <abstract>We present SegGPT, a generalist model for segmenting everything in context. We unify various segmentation tasks into a generalist in-context learning framework that accommodates different kinds of segmentation data by transforming them into the same format of images. The training of SegGPT is formulated as an in-context coloring problem with random color mapping for each data sample. The objective is to accomplish diverse tasks according to the context, rather than relying on specific colors. After training, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_SegGPT_Towards_Segmenting_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xinlong Wang</author><author>Xiaosong Zhang</author><author>Yue Cao</author><author>Wen Wang</author><author>Chunhua Shen</author><author>Tiejun Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DDColor: Towards Photo-Realistic Image Colorization via Dual Decoders</title>
            <abstract>Image colorization is a challenging problem due to multi-modal uncertainty and high ill-posedness. Directly training a deep neural network usually leads to incorrect semantic colors and low color richness. While transformer-based methods can deliver better results, they often rely on manually designed priors, suffer from poor generalization ability, and introduce color bleeding effects. To address these issues, we propose DDColor, an end-to-end method with dual decoders for image colorization. Our approach includes a pixel decoder and a query-based color decoder. The former restores the spatial resolution of the image, while the latter utilizes rich visual features to refine color queries, thus avoiding hand-crafted priors. Our two decoders work together to establish correlations between color and multi-scale semantic representations via cross-attention, significantly alleviating the color bleeding effect. Additionally, a simple yet effective colorfulness loss is introduced to enhance the color richness. Extensive experiments demonstrate that DDColor achieves superior performance to existing state-of-the-art works both quantitatively and qualitatively. The codes and models are publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kang_DDColor_Towards_Photo-Realistic_Image_Colorization_via_Dual_Decoders_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kang_DDColor_Towards_Photo-Realistic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiaoyang Kang</author><author>Tao Yang</author><author>Wenqi Ouyang</author><author>Peiran Ren</author><author>Lingzhi Li</author><author>Xuansong Xie</author>
            </authors>
        </paper>
        

        <paper>
            <title>Visual Explanations via Iterated Integrated Attributions</title>
            <abstract>We introduce Iterated Integrated Attributions (IIA) - a generic method for explaining the predictions of vision models. IIA employs iterative integration across the input image, the internal representations generated by the model, and their gradients, yielding precise and focused explanation maps. We demonstrate the effectiveness of IIA through comprehensive evaluations across various tasks, datasets, and network architectures. Our results showcase that IIA produces accurate explanation maps, outperforming other state-of-the-art explanation techniques.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Barkan_Visual_Explanations_via_Iterated_Integrated_Attributions_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Barkan_Visual_Explanations_via_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Oren Barkan</author><author>Yehonatan Elisha</author><author>Yuval Asher</author><author>Amit Eshel</author><author>Noam Koenigstein</author>
            </authors>
        </paper>
        

        <paper>
            <title>Pairwise Similarity Learning is SimPLE</title>
            <abstract>In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wen_Pairwise_Similarity_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yandong Wen</author><author>Weiyang Liu</author><author>Yao Feng</author><author>Bhiksha Raj</author><author>Rita Singh</author><author>Adrian Weller</author><author>Michael J. Black</author><author>Bernhard Sch lkopf</author>
            </authors>
        </paper>
        

        <paper>
            <title>GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</title>
            <abstract>Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping (SLAM) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction. Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally optimizing poses and 3D reconstruction in real-time. Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction. Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy. Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_GO-SLAM_Global_Optimization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Youmin Zhang</author><author>Fabio Tosi</author><author>Stefano Mattoccia</author><author>Matteo Poggi</author>
            </authors>
        </paper>
        

        <paper>
            <title>FACTS: First Amplify Correlations and Then Slice to Discover Bias</title>
            <abstract>Computer vision datasets frequently contain spurious correlations between task-relevant labels and (easy to learn) latent task-irrelevant attributes (e.g. context). Models trained on such datasets learn &quot;shortcuts&quot; and underperform on bias-conflicting slices of data where the correlation does not hold. In this work, we study the problem of identifying such slices to inform downstream bias mitigation strategies. We propose First Amplify Correlations and Then Slice (FACTS), wherein we first amplify correlations to fit a simple bias-aligned hypothesis via strongly regularized empirical risk minimization. Next, we perform correlation-aware slicing via mixture modeling in bias-aligned feature space to discover underperforming data slices that capture distinct correlations. Despite its simplicity, our method considerably improves over prior work (by as much as 35% precision@10) in correlation bias identification across a range of diverse evaluation settings. Our code is available at https://github.com/yvsriram/FACTS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yenamandra_FACTS_First_Amplify_Correlations_and_Then_Slice_to_Discover_Bias_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yenamandra_FACTS_First_Amplify_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Sriram Yenamandra</author><author>Pratik Ramesh</author><author>Viraj Prabhu</author><author>Judy Hoffman</author>
            </authors>
        </paper>
        

        <paper>
            <title>Mask-Attention-Free Transformer for 3D Instance Segmentation</title>
            <abstract>Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lai_Mask-Attention-Free_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lai_Mask-Attention-Free_Transformer_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xin Lai</author><author>Yuhui Yuan</author><author>Ruihang Chu</author><author>Yukang Chen</author><author>Han Hu</author><author>Jiaya Jia</author>
            </authors>
        </paper>
        

        <paper>
            <title>EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries</title>
            <abstract>With the recent advances in video and 3D understanding, novel 4D spatio-temporal methods fusing both concepts have emerged. Towards this direction, the Ego4D Episodic Memory Benchmark proposed a task for Visual Queries with 3D Localization (VQ3D). Given an egocentric video clip and an image crop depicting a query object, the goal is to localize the 3D position of the center of that query object with respect to the camera pose of a query frame. Current methods tackle the problem of VQ3D by unprojecting the 2D localization results of the sibling task Visual Queries with 2D Localization (VQ2D) into 3D predictions. Yet, we point out that the low number of camera poses caused by camera re-localization from previous VQ3D methods severally hinders their overall success rate. In this work, we formalize a pipeline (we dub EgoLoc) that better entangles 3D multiview geometry with 2D object retrieval from egocentric videos. Our approach involves estimating more robust camera poses and aggregating multi-view 3D displacements by leveraging the 2D detection confidence, which enhances the success rate of object queries and leads to a significant improvement in the VQ3D baseline performance. Specifically, our approach achieves an overall success rate of up to 87.12%, which sets a new state-of-the-art result in the VQ3D task. We provide a comprehensive empirical analysis of the VQ3D task and existing solutions, and highlight the remaining challenges in VQ3D. The code is available at https://github.com/Wayne-Mai/EgoLoc.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mai_EgoLoc_Revisiting_3D_Object_Localization_from_Egocentric_Videos_with_Visual_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jinjie Mai</author><author>Abdullah Hamdi</author><author>Silvio Giancola</author><author>Chen Zhao</author><author>Bernard Ghanem</author>
            </authors>
        </paper>
        

        <paper>
            <title>FLatten Transformer: Vision Transformer using Focused Linear Attention</title>
            <abstract>The quadratic computation complexity of self-attention has been a persistent challenge when applying Transformer models to vision tasks. Linear attention, on the other hand, offers a much more efficient alternative with its linear complexity by approximating the Softmax operation through carefully designed mapping functions. However, current linear attention approaches either suffer from significant performance degradation or introduce additional computation overhead from the mapping functions. In this paper, we propose a novel Focused Linear Attention module to achieve both high efficiency and expressiveness. Specifically, we first analyze the factors contributing to the performance degradation of linear attention from two perspectives: the focus ability and feature diversity. To overcome these limitations, we introduce a simple yet effective mapping function and an efficient rank restoration module to enhance the expressiveness of self-attention while maintaining low computation complexity. Extensive experiments show that our linear attention module is applicable to a variety of advanced vision Transformers, and achieves consistently improved performances on multiple benchmarks. Code is available at https://github.com/LeapLabTHU/FLatten-Transformer.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_FLatten_Transformer_Vision_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dongchen Han</author><author>Xuran Pan</author><author>Yizeng Han</author><author>Shiji Song</author><author>Gao Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>ADNet: Lane Shape Prediction via Anchor Decomposition</title>
            <abstract>In this paper, we revisit the limitations of anchor-based lane detection methods, which have predominantly focused on fixed anchors that stem from the edges of the image, disregarding their versatility and quality. To overcome the inflexibility of anchors, we decompose them into learning the heat map of starting points and their associated directions. This decomposition removes the limitations on the starting point of anchors, making our algorithm adaptable to different lane types in various datasets. To enhance the quality of anchors, we introduce the Large Kernel Attention (LKA) for Feature Pyramid Network (FPN). This significantly increases the receptive field, which is crucial in capturing the sufficient context as lane lines typically run throughout the entire image. We have named our proposed system the Anchor Decomposition Network (ADNet). Additionally, we propose the General Lane IoU (GLIoU) loss, which significantly improves the performance of ADNet in complex scenarios. Experimental results on three widely used lane detection benchmarks, VIL-100, CULane, and TuSimple, demonstrate that our approach outperforms the state-of-the-art methods on VIL-100 and exhibits competitive accuracy on CULane and TuSimple. Code and models will be released on https://github.com/ Sephirex-X/ADNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xiao_ADNet_Lane_Shape_Prediction_via_Anchor_Decomposition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xiao_ADNet_Lane_Shape_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lingyu Xiao</author><author>Xiang Li</author><author>Sen Yang</author><author>Wankou Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation</title>
            <abstract>Neural radiance fields (NeRF) have garnered significant attention, with recent works such as Instant-NGP accelerating NeRF training and evaluation through a combination of hashgrid-based positional encoding and neural networks. However, effectively leveraging the spatial sparsity of 3D scenes remains a challenge. To cull away unnecessary regions of the feature grid, existing solutions rely on prior knowledge of object shape or periodically estimate object shape during training by repeated model evaluations, which are costly and wasteful. To address this issue, we propose HollowNeRF, a novel compression solution for hashgrid-based NeRF which automatically sparsifies the feature grid during the training phase. Instead of directly compressing dense features, HollowNeRF trains a coarse 3D saliency mask that guides efficient feature pruning, and employs an alternating direction method of multipliers (ADMM) pruner to sparsify the 3D saliency mask during training. By exploiting the sparsity in the 3D scene to redistribute hash collisions, HollowNeRF improves rendering quality while using a fraction of the parameters of comparable state-of-the-art solutions, leading to a better cost-accuracy trade-off. Our method delivers comparable rendering quality to Instant-NGP, while utilizing just 31% of the parameters. In addition, our solution can achieve a PSNR accuracy gain of up to 1dB using only 56% of the parameters.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xie_HollowNeRF_Pruning_Hashgrid-Based_NeRFs_with_Trainable_Collision_Mitigation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xiufeng Xie</author><author>Riccardo Gherardi</author><author>Zhihong Pan</author><author>Stephen Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Complete Recipe for Diffusion Generative Models</title>
            <abstract>Score-based Generative Models (SGMs) have demonstrated exceptional synthesis outcomes across various tasks. However, the current design landscape of the forward diffusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian posterior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that several existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empirical results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various competing approaches on established image synthesis benchmarks. Remarkably, PSLD achieves sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional CIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in conditional synthesis using pre-trained score networks, offering an appealing alternative as an SGM backbone for future advancements. Code and model checkpoints can be accessed at https://github.com/mandt-lab/PSLD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pandey_A_Complete_Recipe_for_Diffusion_Generative_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pandey_A_Complete_Recipe_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kushagra Pandey</author><author>Stephan Mandt</author>
            </authors>
        </paper>
        

        <paper>
            <title>The Devil is in the Crack Orientation: A New Perspective for Crack Detection</title>
            <abstract>Cracks are usually curve-like structures that are the focus of many computer-vision applications (e.g., road safety inspection and surface inspection of industrial facilities). The existing pixel-based crack segmentation methods rely on time-consuming and costly pixel-level annotations. And the object-based crack detection methods exploit the horizontal box to detect the crack without considering crack orientation, resulting in scale variation and intra-class variation. Considering this, we provide a new perspective for crack detection that models the cracks as a series of sub-cracks with the corresponding orientation. However, the vanilla adaptation of the existing oriented object detection methods to the crack detection tasks will result in limited performance, due to the boundary discontinuity issue and the ambiguities in sub-crack orientation. In this paper, we propose a first-of-its-kind oriented sub-crack detector, dubbed as CrackDet, which is derived from a novel piecewise angle definition, to ease the boundary discontinuity problem. And then, we propose a multi-branch angle regression loss for learning sub-crack orientation and variance together. Since there are no related benchmarks, we construct three fully annotated datasets, namely, ORC, ONPP, and OCCSD, which involve various cracks in road pavement and industrial facilities. Experiments show that our approach outperforms state-of-the-art crack detectors.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_The_Devil_is_in_the_Crack_Orientation_A_New_Perspective_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Zhuangzhuang Chen</author><author>Jin Zhang</author><author>Zhuonan Lai</author><author>Guanming Zhu</author><author>Zun Liu</author><author>Jie Chen</author><author>Jianqiang Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>FedPD: Federated Open Set Recognition with Parameter Disentanglement</title>
            <abstract>Existing federated learning (FL) approaches are deployed under the unrealistic closed-set setting, with both training and testing classes belong to the same set, which makes the global model fail to identify the unseen classes as `unknown&apos;. To this end, we aim to study a novel problem of federated open-set recognition (FedOSR), which learns an open-set recognition (OSR) model under federated paradigm such that it classifies seen classes while at the same time detects unknown classes. In this work, we propose a parameter disentanglement guided federated open-set recognition (FedPD) algorithm to address two core challenges of FedOSR: cross-client inter-set interference between learning closed-set and open-set knowledge and cross-client intra-set inconsistency by data heterogeneity. The proposed FedPD framework mainly leverages two modules, i.e., local parameter disentanglement (LPD) and global divide-and-conquer aggregation (GDCA), to first disentangle client OSR model into different subnetworks, then align the corresponding parts cross clients for matched model aggregation. Specifically, on the client side, LPD decouples an OSR model into a closed-set subnetwork and an open-set subnetwork by the task-related importance, thus preventing inter-set interference. On the server side, GDCA first partitions the two subnetworks into specific and shared parts, and subsequently aligns the corresponding parts through optimal transport to eliminate parameter misalignment. Extensive experiments on various datasets demonstrate the superior performance of our proposed method.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_FedPD_Federated_Open_Set_Recognition_with_Parameter_Disentanglement_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Chen Yang</author><author>Meilu Zhu</author><author>Yifan Liu</author><author>Yixuan Yuan</author>
            </authors>
        </paper>
        

        <paper>
            <title>WaterMask: Instance Segmentation for Underwater Imagery</title>
            <abstract>Underwater image instance segmentation is a fundamental and critical step in underwater image analysis and understanding. However, the paucity of general multiclass instance segmentation datasets has impeded the development of instance segmentation studies for underwater images. In this paper, we propose the first underwater image instance segmentation dataset (UIIS), which provides 4628 images for 7 categories with pixel-level annotations. Meanwhile, we also design WaterMask for underwater image instance segmentation for the first time. In Water- Mask, we first devise Difference Similarity Graph Attention Module (DSGAT) to recover lost detailed information due to image quality degradation and downsampling to help the network prediction. Then, we propose Multi-level Feature Refinement Module (MFRM) to predict foreground masks and boundary masks separately by features at different scales, and guide the network through Boundary Mask Strategy (BMS) with boundary learning loss to provide finer prediction results. Extensive experimental results demonstrates that WaterMask can achieve significant gains of 2.9, 3.8 mAP over Mask R-CNN when using ResNet-50 and ResNet-101. Code and Dataset are available at https: //github.com/LiamLian0727/WaterMask.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lian_WaterMask_Instance_Segmentation_for_Underwater_Imagery_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Shijie Lian</author><author>Hua Li</author><author>Runmin Cong</author><author>Suqi Li</author><author>Wei Zhang</author><author>Sam Kwong</author>
            </authors>
        </paper>
        

        <paper>
            <title>MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers</title>
            <abstract>Quantum machine learning and vision have come to the fore recently, with hardware advances enabling rapid advancement in the capabilities of quantum machines. Recently, quantum image generation has been explored with many potential advantages over non-quantum techniques; however, previous techniques have suffered from poor quality and robustness. To address these problems, we introduce MosaiQ a high-quality quantum image generation GAN framework that can be executed on today&apos;s Near-term Intermediate Scale Quantum (NISQ) computers.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Silver_MosaiQ_Quantum_Generative_Adversarial_Networks_for_Image_Generation_on_NISQ_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Daniel Silver</author><author>Tirthak Patel</author><author>William Cutler</author><author>Aditya Ranjan</author><author>Harshitta Gandhi</author><author>Devesh Tiwari</author>
            </authors>
        </paper>
        

        <paper>
            <title>DVIS: Decoupled Video Instance Segmentation Framework</title>
            <abstract>Video instance segmentation (VIS) is a critical task with diverse applications, including autonomous driving and video editing. Existing methods often underperform on complex and long videos in real world, primarily due to two factors. Firstly, offline methods are limited by the tightly-coupled modeling paradigm, which treats all frames equally and disregards the interdependencies between adjacent frames. Consequently, this leads to the introduction of excessive noise during long-term temporal alignment. Secondly, online methods suffer from inadequate utilization of temporal information. To tackle these challenges, we propose a decoupling strategy for VIS by dividing it into three independent sub-tasks: segmentation, tracking, and refinement. The efficacy of the decoupling strategy relies on two crucial elements: 1) attaining precise long-term alignment outcomes via frame-by-frame association during tracking, and 2) the effective utilization of temporal information predicated on the aforementioned accurate alignment outcomes during refinement. We introduce a novel referring tracker and temporal refiner to construct the Decoupled VIS framework (DVIS). DVIS achieves new SOTA performance in both VIS and VPS, surpassing the current SOTA methods by 7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the most challenging and realistic benchmarks. Moreover, thanks to the decoupling strategy, the referring tracker and temporal refiner are super light-weight (only 6% of the segmenter FLOPs), allowing for efficient training and inference on a single GPU with 11G memory. To promote reproducibility and facilitate further research, we will make the code publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_DVIS_Decoupled_Video_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Tao Zhang</author><author>Xingye Tian</author><author>Yu Wu</author><author>Shunping Ji</author><author>Xuebo Wang</author><author>Yuan Zhang</author><author>Pengfei Wan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-centric Representation</title>
            <abstract>Video amodal segmentation is a particularly challenging task in computer vision, which requires to deduce the full shape of an object from the visible parts of it. Recently, some studies have achieved promising performance by using motion flow to integrate information across frames under a self-supervised setting. However, motion flow has a clear limitation by the two factors of moving cameras and object deformation. This paper presents a rethinking to previous works. We particularly leverage the supervised signals with object-centric representation in real-world scenarios. The underlying idea is the supervision signal of the specific object and the features from different views can mutually benefit the deduction of the full mask in any specific frame. We thus propose an Efficient object-centric Representation amodal Segmentation (EoRaS). Specially, beyond solely relying on supervision signals, we design a translation module to project image features into the Bird&apos;s-Eye View (BEV), which introduces 3D information to improve current feature quality. Furthermore, we propose a multi-view fusion layer based temporal module which is equipped with a set of object slots and interacts with features from different views by attention mechanism to fulfill sufficient object representation completion. As a result, the full mask of the object can be decoded from image features updated by object slots. Extensive experiments on both real-world and synthetic benchmarks demonstrate the superiority of our proposed method, achieving state-of-the-art performance. Our code will be released at https://github.com/kfan21/EoRaS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fan_Rethinking_Amodal_Video_Segmentation_from_Learning_Supervised_Signals_with_Object-centric_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fan_Rethinking_Amodal_Video_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Ke Fan</author><author>Jingshi Lei</author><author>Xuelin Qian</author><author>Miaopeng Yu</author><author>Tianjun Xiao</author><author>Tong He</author><author>Zheng Zhang</author><author>Yanwei Fu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Distilled Reverse Attention Network for Open-world Compositional Zero-Shot Learning</title>
            <abstract>Open-World Compositional Zero-Shot Learning (OW-CZSL) aims to recognize new compositions of seen attributes and objects. In OW-CZSL, methods built on the conventional closed-world setting degrade severely due to the unconstrained OW test space. While previous works alleviate the issue by pruning compositions according to external knowledge or correlations in seen pairs, they introduce biases that harm the generalization. Some methods thus predict state and object with independently constructed and trained classifiers, ignoring that attributes are highly context-dependent and visually entangled with objects. In this paper, we propose a novel Distilled Reverse Attention Network to address the challenges. We also model attributes and objects separately but with different motivations, capturing contextuality and locality, respectively. We further design a reverse-and-distill strategy that learns disentangled representations of elementary components in training data supervised by reverse attention and knowledge distillation. We conduct experiments on three datasets and consistently achieve state-of-the-art (SOTA) performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Distilled_Reverse_Attention_Network_for_Open-world_Compositional_Zero-Shot_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yun Li</author><author>Zhe Liu</author><author>Saurav Jha</author><author>Lina Yao</author>
            </authors>
        </paper>
        

        <paper>
            <title>TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models</title>
            <abstract>We present TexFusion(Texture Diffusion), a new method to synthesize textures for given 3D geometries, using only large-scale text-guided image diffusion models. In contrast to recent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent generation technique specifically designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Specifically, we leverage latent diffusion models, apply the diffusion model&apos;s denoiser on a set of 2D renders of the 3D object, and aggregate the different denoising predictions on a shared latent texture map. Final RGB output textures are produced by optimizing an intermediate neural color field on the decodings of 2D renders of the latent texture. We thoroughly validate TexFusion and show that we can efficiently generate diverse, high quality and globally coherent textures. We achieve state-of-the-art text-guided texture synthesis performance using only image diffusion models, while avoiding the pitfalls of previous distillation-based methods. The text-conditioning offers detailed control and we also do not rely on any ground truth 3D textures for training. This makes our method very versatile and applicable to a broad range of geometries and texture types. We hope that TexFusion will advance AI-based texturing of 3D assets for applications in virtual reality, game design, simulation, and more.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cao_TexFusion_Synthesizing_3D_Textures_with_Text-Guided_Image_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cao_TexFusion_Synthesizing_3D_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Tianshi Cao</author><author>Karsten Kreis</author><author>Sanja Fidler</author><author>Nicholas Sharp</author><author>Kangxue Yin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Shift from Texture-bias to Shape-bias: Edge Deformation-based Augmentation for Robust Object Recognition</title>
            <abstract>Recent studies have shown the vulnerability of CNNs under perturbation noises, which is partially caused by the reason that the well-trained CNNs are too biased toward the object texture, i.e., they make predictions mainly based on texture cues. To reduce this texture-bias, current studies resort to learning augmented samples with heavily perturbed texture to make networks be more biased toward relatively stable shape cues. However, such methods usually fail to achieve real shape-biased networks due to the insufficient diversity of the shape cues. In this paper, we propose to augment the training dataset by generating semantically meaningful shapes and samples, via a shape deformation-based online augmentation, namely as SDbOA. The samples generated by our SDbOA have two main merits. First, the augmented samples with more diverse shape variations enable networks to learn the shape cues more elaborately, which encourages the network to be shape-biased. Second, semantic-meaningful shape-augmentation samples could be produced by jointly regularizing the generator with object texture and edge-guidance soft constraint, where the edges are represented more robustly with a self information guided map to better against the noises on them. Extensive experiments under various perturbation noises demonstrate the obvious superiority of our shape-bias-motivated model over the state of the arts in terms of robustness performance. Our code is appended in the supplementary material.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/He_Shift_from_Texture-bias_to_Shape-bias_Edge_Deformation-based_Augmentation_for_Robust_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/He_Shift_from_Texture-bias_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xilin He</author><author>Qinliang Lin</author><author>Cheng Luo</author><author>Weicheng Xie</author><author>Siyang Song</author><author>Feng Liu</author><author>Linlin Shen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Data-free Knowledge Distillation for Fine-grained Visual Categorization</title>
            <abstract>Data-free knowledge distillation (DFKD) is a promising approach for addressing issues related to model compression, security privacy, and transmission restrictions. Although the existing methods exploiting DFKD have achieved inspiring achievements in coarse-grained classification, in practical applications involving fine-grained classification tasks that require more detailed distinctions between similar categories, sub-optimal results are obtained. To address this issue, we propose an approach called DFKD-FGVC that extends DFKD to fine-grained vision categorization (FGVC) tasks. Our approach utilizes an adversarial distillation framework with attention generator, mixed high-order attention distillation, and semantic feature contrast learning. Specifically, we introduce a spatial-wise attention mechanism to the generator to synthesize fine-grained images with more details of discriminative parts. We also utilize the mixed high-order attention mechanism to capture complex interactions among parts and the subtle differences among discriminative features of the fine-grained categories, paying attention to both local features and semantic context relationships. Moreover, we leverage the teacher and student models of the distillation framework to contrast high-level semantic feature maps in the hyperspace, comparing variances of different categories. We evaluate our approach on three widely-used FGVC benchmarks (Aircraft, Cars196, and CUB200) and demonstrate its superior performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shao_Data-free_Knowledge_Distillation_for_Fine-grained_Visual_Categorization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shao_Data-free_Knowledge_Distillation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Renrong Shao</author><author>Wei Zhang</author><author>Jianhua Yin</author><author>Jun Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding</title>
            <abstract>With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI understanding. Code and data are available at https://mvig-rhos.com/ego_pca.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_EgoPCA_A_New_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yue Xu</author><author>Yong-Lu Li</author><author>Zhemin Huang</author><author>Michael Xu Liu</author><author>Cewu Lu</author><author>Yu-Wing Tai</author><author>Chi-Keung Tang</author>
            </authors>
        </paper>
        

        <paper>
            <title>I Can&apos;t Believe There&apos;s No Images! Learning Visual Tasks Using only Language Supervision</title>
            <abstract>Many high-level skills that are required for computer vision tasks, such as parsing questions, comparing and contrasting semantics, and writing descriptions, are also required in other domains such as natural language processing. In this paper, we ask whether it is possible to learn those skills from text data and then transfer them to vision tasks without ever training on visual training data. Key to our approach is exploiting the joint embedding space of contrastively trained vision and language encoders. In practice, there can be systematic differences between embedding spaces for different modalities in contrastive models, and we analyze how these differences affect our approach and study strategies to mitigate this concern. We produce models using only text training data on four representative tasks: image captioning, visual entailment, visual question answering and visual news captioning, and evaluate them on standard benchmarks using images. We find these models perform close to models trained on images, while surpassing prior work for captioning and visual entailment in this text-only setting by over 9 points, and outperforming all prior work on visual news by over 30 points. We also showcase a variety of stylistic image captioning models that are trained using no image data and no human-curated language data, but instead using readily-available text data from books, the web, or language models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gu_I_Cant_Believe_Theres_No_Images_Learning_Visual_Tasks_Using_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gu_I_Cant_Believe_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sophia Gu</author><author>Christopher Clark</author><author>Aniruddha Kembhavi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Feature Prediction Diffusion Model for Video Anomaly Detection</title>
            <abstract>Anomaly detection in the video is an important research area and a challenging task in real applications. Due to the unavailability of large-scale annotated anomaly events, most existing video anomaly detection (VAD) methods focus on learning the distribution of normal samples to detect the substantially deviated samples as anomalies. To well learn the distribution of normal motion and appearance, many auxiliary networks are employed to extract foreground object or action information. These high-level semantic features effectively filter the noise from the background to decrease its influence on detection models. However, the capability of these extra semantic models heavily affects the performance of the VAD methods. Motivated by the impressive generative and anti-noise capacity of diffusion model (DM), in this work, we introduce a novel DM-based method to predict the features of video frames for anomaly detection. We aim to learn the distribution of normal samples without any extra high-level semantic feature extraction models involved. To this end, we build two denoising diffusion implicit modules to predict and refine the features. The first module concentrates on feature motion learning, while the last focuses on feature appearance learning. To the best of our knowledge, it is the first DM-based method to predict frame features for VAD. The strong capacity of DMs also enables our method to more accurately predict the normal features than non-DM-based feature prediction-based VAD methods. Extensive experiments show that the proposed approach substantially outperforms state-of-the-art competing methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yan_Feature_Prediction_Diffusion_Model_for_Video_Anomaly_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Cheng Yan</author><author>Shiyu Zhang</author><author>Yang Liu</author><author>Guansong Pang</author><author>Wenjun Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>MasQCLIP for Open-Vocabulary Universal Image Segmentation</title>
            <abstract>We present a new method for open-vocabulary universal image segmentation, which is capable of performing instance, semantic, and panoptic segmentation under a unified framework. Our approach, called MasQCLIP, seamlessly integrates with a pre-trained CLIP model by utilizing its dense features, thereby circumventing the need for extensive parameter training. MasQCLIP emphasizes two new aspects when building an image segmentation method with a CLIP model: 1) a student-teacher module to deal with masks of the novel (unseen) classes by distilling information from the base (seen) classes; 2) a fine-tuning process to update model parameters for the queries Q within the CLIP model. Thanks to these two simple and intuitive designs, MasQCLIP is able to achieve state-of-the-art performances with a substantial gain over the competing methods by a large margin across all three tasks, including open-vocabulary instance, semantic, and panoptic segmentation. Project page is at https://masqclip.github.io/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_MasQCLIP_for_Open-Vocabulary_Universal_Image_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xin Xu</author><author>Tianyi Xiong</author><author>Zheng Ding</author><author>Zhuowen Tu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search</title>
            <abstract>Weakly supervised person search aims to jointly detect and match persons with only bounding box annotations. Existing approaches typically focus on improving the features by exploring the relations of persons. However, scale variation problem is a more severe obstacle and under-studied that a person often owns images with different scales (resolutions). For one thing, small-scale images contain less information of a person, thus affecting the accuracy of the generated pseudo labels. For another, different similarities between cross-scale images of a person increase the difficulty of matching. In this paper, we address it by proposing a novel one-step framework, named Self-similarity driven Scale-invariant Learning (SSL). Scale invariance can be explored based on the self-similarity prior that it shows the same statistical properties of an image at different scales. To this end, we introduce a Multi-scale Exemplar Branch to guide the network in concentrating on the foreground and learning scale-invariant features by hard exemplars mining. To enhance the discriminative power of the learned features, we further introduce a dynamic pseudo label prediction that progressively seeks true labels for training. Experimental results on two standard benchmarks, i.e., PRW and CUHK-SYSU datasets, demonstrate that the proposed method can solve scale variation problem effectively and perform favorably against state-of-the-art methods. Code is available at https://github.com/Wangbenzhi/SSL.git.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Self-similarity_Driven_Scale-invariant_Learning_for_Weakly_Supervised_Person_Search_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Benzhi Wang</author><author>Yang Yang</author><author>Jinlin Wu</author><author>Guo-jun Qi</author><author>Zhen Lei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction</title>
            <abstract>Ordinal regression refers to classifying object instances into ordinal categories. It has been widely studied in many scenarios, such as medical disease grading and movie rating. Known methods focused only on learning inter-class ordinal relationships, but still incur limitations in distinguishing adjacent categories thus far. In this paper, we propose a simple sequence prediction framework for ordinal regression called Ord2Seq, which, for the first time, transforms each ordinal category label into a special label sequence and thus regards an ordinal regression task as a sequence prediction process. In this way, we decompose an ordinal regression task into a series of recursive binary classification steps, so as to subtly distinguish adjacent categories. Comprehensive experiments show the effectiveness of distinguishing adjacent categories for performance improvement and our new approach exceeds state-of-the-art performances in four different scenarios. Codes are available at https://github.com/wjh892521292/Ord2Seq.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Ord2Seq_Regarding_Ordinal_Regression_as_Label_Sequence_Prediction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Ord2Seq_Regarding_Ordinal_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jinhong Wang</author><author>Yi Cheng</author><author>Jintai Chen</author><author>TingTing Chen</author><author>Danny Chen</author><author>Jian Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Controllable Visual-Tactile Synthesis</title>
            <abstract>Deep generative models have various content creation applications such as graphic design, e-commerce, and virtual try-on. However, current works mainly focus on synthesizing realistic visual outputs, often ignoring other sensory modalities, such as touch, which limits physical interaction with users. In this work, we leverage deep generative models to create a multi-sensory experience where users can touch and see the synthesized object when sliding their fingers on a haptic surface. The main challenges lie in the significant scale discrepancy between vision and touch sensing and the lack of explicit mapping from touch sensing data to a haptic rendering device. To bridge this gap, we collect high-resolution tactile data with a GelSight sensor and create a new visuotactile clothing dataset. We then develop a conditional generative model that synthesizes both visual and tactile outputs from a single sketch. We evaluate our method regarding image quality and tactile rendering accuracy. Finally, we introduce a pipeline to render high-quality visual and tactile outputs on an electroadhesion-based haptic device for an immersive experience, allowing for challenging materials and editable sketch inputs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gao_Controllable_Visual-Tactile_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ruihan Gao</author><author>Wenzhen Yuan</author><author>Jun-Yan Zhu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?</title>
            <abstract>Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem? In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: https://github.com/billpsomas/simpool.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Psomas_Keep_It_SimPool_Who_Said_Supervised_Transformers_Suffer_from_Attention_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Psomas_Keep_It_SimPool_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Bill Psomas</author><author>Ioannis Kakogeorgiou</author><author>Konstantinos Karantzalos</author><author>Yannis Avrithis</author>
            </authors>
        </paper>
        

        <paper>
            <title>LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models</title>
            <abstract>Prompt engineering is a powerful tool used to enhance the performance of pre-trained models on downstream tasks. For example, providing the prompt &quot;Let&apos;s think step by step&quot; improved GPT-3&apos;s reasoning accuracy to 63% on MutiArith while prompting &quot;a photo of&quot; filled with a class name enables CLIP to achieve 80% zero-shot accuracy on ImageNet. While previous research has explored prompt learning for the visual modality, analyzing what constitutes a good visual prompt specifically for image recognition is limited. In addition, existing visual prompt tuning methods&apos; generalization ability is worse than text-only prompting tuning. This paper explores our key insight: synthetic text images are good visual prompts for vision-language models! To achieve that, we propose our LoGoPrompt, which reformulates the classification objective to the visual prompt selection and addresses the chicken-and-egg challenge of first adding synthetic text images as class-wise visual prompts or predicting the class first. Without any trainable visual prompt parameters, experimental results on 16 datasets demonstrate that our method consistently outperforms state-of-the-art methods in few-shot learning, base-to-new generalization, and domain generalization. The code will be publicly available upon publication.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shi_LoGoPrompt_Synthetic_Text_Images_Can_Be_Good_Visual_Prompts_for_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Cheng Shi</author><author>Sibei Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision</title>
            <abstract>Extracting useful visual cues for the downstream tasks is especially challenging under low-light vision. Prior works create enhanced representations by either correlating visual quality with machine perception or designing illumination-degrading transformation methods that require pre-training on synthetic datasets. We argue that optimizing enhanced image representation pertaining to the loss of the downstream task can result in more expressive representations. Therefore, in this work, we propose a novel module, FeatEnHancer, that hierarchically combines multiscale features using multiheaded attention guided by task-related loss function to create suitable representations. Furthermore, our intra-scale enhancement improves the quality of features extracted at each scale or level, as well as combines features from different scales in a way that reflects their relative importance for the task at hand. FeatEnHancer is a general-purpose plug-and-play module and can be incorporated into any low-light vision pipeline. We show with extensive experimentation that the enhanced representation produced with FeatEnHancer significantly and consistently improves results in several low-light vision tasks, including dark object detection (+5.7 mAP on ExDark), face detection (+1.5 mAP on DARK FACE), nighttime semantic segmentation (+5.1 mIoU on ACDC ), and video object detection (+1.8 mAP on DarkVision), highlighting the effectiveness of enhancing hierarchical features under low-light vision.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hashmi_FeatEnHancer_Enhancing_Hierarchical_Features_for_Object_Detection_and_Beyond_Under_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hashmi_FeatEnHancer_Enhancing_Hierarchical_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Khurram Azeem Hashmi</author><author>Goutham Kallempudi</author><author>Didier Stricker</author><author>Muhammad Zeshan Afzal</author>
            </authors>
        </paper>
        

        <paper>
            <title>Saliency Regularization for Self-Training with Partial Annotations</title>
            <abstract>Partially annotated images are easy to obtain in multi-label classification. However, unknown labels in partially annotated images exacerbate the positive-negative imbalance inherent in multi-label classification, which affects supervised learning of known labels. Most current methods require sufficient image annotations, and do not focus on the imbalance of the labels in the supervised training phase. In this paper, we propose saliency regularization (SR) for a novel self-training framework. In particular, we model saliency on the class-specific maps, and strengthen the saliency of object regions corresponding to the present labels. Besides, we introduce consistency regularization to mine unlabeled information to complement unknown labels with the help of SR. It is verified to alleviate the negative dominance caused by the imbalance, and achieve state-of-the-art performance on Pascal VOC 2007, MS-COCO, VG-200, and OpenImages V3.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Saliency_Regularization_for_Self-Training_with_Partial_Annotations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Saliency_Regularization_for_Self-Training_with_Partial_Annotations_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shouwen Wang</author><author>Qian Wan</author><author>Xiang Xiang</author><author>Zhigang Zeng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation</title>
            <abstract>Vision-based reinforcement learning (RL) depends on discriminative representation encoders to abstract the observation states. Despite the great success of increasing CNN parameters for many supervised computer vision tasks, reinforcement learning with temporal-difference (TD) losses cannot benefit from it in most complex environments. In this paper, we analyze that the training instability arises from the oscillating self-overfitting of the heavy-optimizable encoder. We argue that serious oscillation will occur to the parameters when enforced to fit the sensitive TD targets, causing uncertain drifting of the latent state space and thus transmitting these perturbations to the policy learning. To alleviate this phenomenon, we propose a novel asymmetric interactive cooperation approach with the interaction between a heavy-optimizable encoder and a supportive light-optimizable encoder, in which both their advantages are integrated including the highly discriminative capability as well as the training stability. We also present a greedy bootstrapping optimization to isolate the visual perturbations from policy learning, where representation and policy are trained sufficiently by turns. Finally, we demonstrate the effectiveness of our method in utilizing larger visual models by first-person highway driving task CARLA and Vizdoom environments.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhai_Stabilizing_Visual_Reinforcement_Learning_via_Asymmetric_Interactive_Cooperation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yunpeng Zhai</author><author>Peixi Peng</author><author>Yifan Zhao</author><author>Yangru Huang</author><author>Yonghong Tian</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Hierarchical Features with Joint Latent Space Energy-Based Prior</title>
            <abstract>This paper studies the fundamental problem of multi-layer generator models in learning hierarchical representations. The multi-layer generator model that consists of multiple layers of latent variables organized in a top-down architecture tends to learn multiple levels of data abstraction. However, such multi-layer latent variables are typically parameterized to be Gaussian, which can be less informative in capturing complex abstractions, resulting in limited success in hierarchical representation learning. On the other hand, the energy-based (EBM) prior is known to be expressive in capturing the data regularities, but it often lacks the hierarchical structure to capture different levels of hierarchical representations. In this paper, we propose a joint latent space EBM prior model with multi-layer latent variables for effective hierarchical representation learning. We develop a variational joint learning scheme that seamlessly integrates an inference model for efficient inference. Our experiments demonstrate that the proposed joint EBM prior is effective and expressive in capturing hierarchical representations and modelling data distribution.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cui_Learning_Hierarchical_Features_with_Joint_Latent_Space_Energy-Based_Prior_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cui_Learning_Hierarchical_Features_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiali Cui</author><author>Ying Nian Wu</author><author>Tian Han</author>
            </authors>
        </paper>
        

        <paper>
            <title>UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding</title>
            <abstract>The prolific performances of Vision Transformers (ViTs) in image tasks have prompted research into adapting the image ViTs for video tasks. However, the substantial gap between image and video impedes the spatiotemporal learning of these image-pretrained models. Though video-specialized models like UniFormer can transfer to the video domain more seamlessly, their unique architectures require prolonged image pretraining, limiting the scalability. Given the emergence of powerful open-source image ViTs, we propose unlocking their potential for video understanding with efficient UniFormer designs. We call the resulting model UniFormerV2, since it inherits the concise style of the UniFormer block, while redesigning local and global relation aggregators that seamlessly integrate advantages from both ViTs and UniFormer. Our UniFormerV2 achieves state-of-the-art performances on 8 popular video benchmarks, including scene-related Kinetics-400/600/700, heterogeneous Moments in Time, temporal-related Something-Something V1/V2, and untrimmed ActivityNet and HACS. It is noteworthy that to the best of our knowledge, UniFormerV2 is the first to elicit 90% top-1 accuracy on Kinetics-400.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_UniFormerV2_Unlocking_the_Potential_of_Image_ViTs_for_Video_Understanding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_UniFormerV2_Unlocking_the_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Kunchang Li</author><author>Yali Wang</author><author>Yinan He</author><author>Yizhuo Li</author><author>Yi Wang</author><author>Limin Wang</author><author>Yu Qiao</author>
            </authors>
        </paper>
        

        <paper>
            <title>TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation</title>
            <abstract>This paper focuses on an under-explored yet important problem: Federated Class-Continual Learning (FCCL), where new classes are dynamically added in federated learning. Existing FCCL works suffer from various limitations, such as requiring additional datasets or storing the private data from previous tasks. In response, we first demonstrate that non-IID data exacerbates catastrophic forgetting issue in FL. Then we propose a novel method called TARGET (federatTed clAss-continual leaRninG via Exemplar-free disTillation), which alleviates catastrophic forgetting in FCCL while preserving client data privacy. Our proposed method leverages the previously trained global model to transfer knowledge of old tasks to the current task at the model level. Moreover, a generator is trained to produce synthetic data to simulate the global distribution of data on each client at the data level. Compared to previous FCCL methods, TARGET does not require any additional datasets or storing real data from previous tasks, which makes it ideal for data-sensitive scenarios.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_TARGET_Federated_Class-Continual_Learning_via_Exemplar-Free_Distillation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jie Zhang</author><author>Chen Chen</author><author>Weiming Zhuang</author><author>Lingjuan Lyu</author>
            </authors>
        </paper>
        

        <paper>
            <title>DiffV2S: Diffusion-Based Video-to-Speech Synthesis with Vision-Guided Speaker Embedding</title>
            <abstract>Recent research has demonstrated impressive results in video-to-speech synthesis which involves reconstructing speech solely from visual input. However, previous works have struggled to accurately synthesize speech due to a lack of sufficient guidance for the model to infer the correct content with the appropriate sound. To resolve the issue, they have adopted an extra speaker embedding as a speaking style guidance from a reference auditory information. Nevertheless, it is not always possible to obtain the audio information from the corresponding video input, especially during the inference time. In this paper, we present a novel vision-guided speaker embedding extractor using a self-supervised pre-trained model and prompt tuning technique. In doing so, the rich speaker embedding information can be produced solely from input visual information, and the extra audio information is not necessary during the inference time. Using the extracted vision-guided speaker embedding representations, we further develop a diffusion-based video-to-speech synthesis model, so called DiffV2S, conditioned on those speaker embeddings and the visual representation extracted from the input video. The proposed DiffV2S not only maintains phoneme details contained in the input video frames, but also creates a highly intelligible mel-spectrogram in which the speaker identities of the multiple speakers are all preserved. Our experimental results show that DiffV2S achieves the state-of-the-art performance compared to the previous video-to-speech synthesis technique.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Choi_DiffV2S_Diffusion-Based_Video-to-Speech_Synthesis_with_Vision-Guided_Speaker_Embedding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Choi_DiffV2S_Diffusion-Based_Video-to-Speech_Synthesis_with_Vision-Guided_Speaker_Embedding_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jeongsoo Choi</author><author>Joanna Hong</author><author>Yong Man Ro</author>
            </authors>
        </paper>
        

        <paper>
            <title>The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining</title>
            <abstract>This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of labels). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video recognition, object detection, low-shot classification and zero-shot recognition. Our largest model achieves new state-of-the-art results on iNaturalist-18 (91.3%), 1-shot ImageNet-1k (62.1%), and zero-shot transfer on Food-101 (96.2%). Our study reveals that model initialization plays a significant role, even for web-scale pretraining with billions of images.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Singh_The_Effectiveness_of_MAE_Pre-Pretraining_for_Billion-Scale_Pretraining_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Singh_The_Effectiveness_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mannat Singh</author><author>Quentin Duval</author><author>Kalyan Vasudev Alwala</author><author>Haoqi Fan</author><author>Vaibhav Aggarwal</author><author>Aaron Adcock</author><author>Armand Joulin</author><author>Piotr Dollar</author><author>Christoph Feichtenhofer</author><author>Ross Girshick</author><author>Rohit Girdhar</author><author>Ishan Misra</author>
            </authors>
        </paper>
        

        <paper>
            <title>GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds</title>
            <abstract>LiDAR-based 3D detection has made great progress in recent years. However, the performance of 3D detectors is considerably limited when deployed in unseen environments, owing to the severe domain gap problem. Existing domain adaptive 3D detection methods do not adequately consider the problem of the distributional discrepancy in feature space, thereby hindering the generalization of detectors across domains. In this work, we propose a novel unsupervised domain adaptive 3D detection framework, namely Geometry-aware Prototype Alignment (GPA-3D), which explicitly leverages the intrinsic geometric relationship from point cloud objects to reduce the feature discrepancy, thus facilitating cross-domain transferring. Specifically, GPA-3D assigns a series of tailored and learnable prototypes to point cloud objects with distinct geometric structures. Each prototype aligns BEV (bird&apos;s-eye-view) features derived from corresponding point cloud objects on source and target domains, reducing the distributional discrepancy and achieving better adaptation. The evaluation results obtained on various benchmarks, including Waymo, nuScenes and KITTI, demonstrate the superiority of our GPA-3D over the state-of-the-art approaches for different adaptation scenarios. The MindSpore version code will be publicly available at https://github.com/Liz66666/GPA3D.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_GPA-3D_Geometry-aware_Prototype_Alignment_for_Unsupervised_Domain_Adaptive_3D_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_GPA-3D_Geometry-aware_Prototype_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ziyu Li</author><author>Jingming Guo</author><author>Tongtong Cao</author><author>Liu Bingbing</author><author>Wankou Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering</title>
            <abstract>In this paper, we focus on the task of generalizable neural human rendering which trains conditional Neural Radiance Fields (NeRF) from multi-view videos of different characters. To handle the dynamic human motion, previous methods have primarily used a SparseConvNet (SPC)-based human representation to process the painted SMPL. However, such SPC-based representation i) optimizes under the volatile observation space which leads to the pose-misalignment between training and inference stages, and ii) lacks the global relationships among human parts that is critical for handling the incomplete painted SMPL. Tackling these issues, we present a brand-new framework named TransHuman, which learns the painted SMPL under the canonical space and captures the global relationships between human parts with transformers. Specifically, TransHuman is mainly composed of Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE first processes the painted SMPL under the canonical space via transformers for capturing the global relationships between human parts. Then, DPaRF binds each output token with a deformable radiance field for encoding the query point under the observation space. Finally, the FDI is employed to further integrate fine-grained information from reference images. Extensive experiments on ZJU-MoCap and H36M show that our TransHuman achieves a significantly new state-of-the-art performance with high efficiency. Project page: https://pansanity666.github.io/TransHuman/</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pan_TransHuman_A_Transformer-based_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Xiao Pan</author><author>Zongxin Yang</author><author>Jianxin Ma</author><author>Chang Zhou</author><author>Yi Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model</title>
            <abstract>Unsupervised surface anomaly detection aims at discovering and localizing anomalous patterns using only anomaly-free training samples. Reconstruction-based models are among the most popular and successful methods, which rely on the assumption that anomaly regions are more difficult to reconstruct. However, there are three major challenges to the practical application of this approach: 1) the reconstruction quality needs to be further improved since it has a great impact on the final result, especially for images with structural changes; 2) it is observed that for many neural networks, the anomalies can also be well reconstructed, which severely violates the underlying assumption; 3) since reconstruction is an ill-conditioned problem, a test instance may correspond to multiple normal patterns, but most current reconstruction-based methods have ignored this critical fact. In this paper, we propose DiffAD, a method for unsupervised anomaly detection based on the latent diffusion model, inspired by its ability to generate high-quality and diverse images. We further propose noisy condition embedding and interpolated channels to address the aforementioned challenges in the general reconstruction-based pipeline. Extensive experiments show that our method achieves state-of-the-art performance on the challenging MVTec dataset, especially in localization accuracy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Unsupervised_Surface_Anomaly_Detection_with_Diffusion_Probabilistic_Model_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Unsupervised_Surface_Anomaly_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xinyi Zhang</author><author>Naiqi Li</author><author>Jiawei Li</author><author>Tao Dai</author><author>Yong Jiang</author><author>Shu-Tao Xia</author>
            </authors>
        </paper>
        

        <paper>
            <title>Simoun: Synergizing Interactive Motion-appearance Understanding for Vision-based Reinforcement Learning</title>
            <abstract>Efficient motion and appearance modeling are critical for vision-based Reinforcement Learning (RL). However, existing methods struggle to reconcile motion and appearance information within the state representations learned from a single observation encoder. To address the problem, we present Synergizing Interactive Motion-appearance Understanding (Simoun), a unified framework for vision-based RL. Given consecutive observation frames, Simoun deliberately and interactively learns both motion and appearance features through a dual-path network architecture. The learning process collaborates with a structural interactive module, which explores the latent motion-appearance structures from the two network paths to leverage their complementarity. To promote sample efficiency, we further design a consistency-guided curiosity module to encourage the exploration of under-learned observations. During training, the curiosity module provides intrinsic rewards according to the consistency of environmental temporal dynamics, which are deduced from both motion and appearance network paths. Experiments conducted on the DeepMind control suite and CARLA automatic driving benchmarks demonstrate the effectiveness of Simoun, where it performs favorably against state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Simoun_Synergizing_Interactive_Motion-appearance_Understanding_for_Vision-based_Reinforcement_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Simoun_Synergizing_Interactive_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yangru Huang</author><author>Peixi Peng</author><author>Yifan Zhao</author><author>Yunpeng Zhai</author><author>Haoran Xu</author><author>Yonghong Tian</author>
            </authors>
        </paper>
        

        <paper>
            <title>Representation Disparity-aware Distillation for 3D Object Detection</title>
            <abstract>In this paper, we focus on developing knowledge distillation (KD) for compact 3D detectors. We observe that off-the-shelf KD methods manifest their efficacy only when the teacher model and student counterpart share similar intermediate feature representations. This might explain why they are less effective in building extreme-compact 3D detectors where significant representation disparity arises due primarily to the intrinsic sparsity and irregularity in 3D point clouds. This paper presents a novel representation disparity-aware distillation (RDD) method to address the representation disparity issue and reduce performance gap between compact students and over-parameterized teachers. This is accomplished by building our RDD from an innovative perspective of information bottleneck (IB), which can effectively minimize the disparity of proposal region pairs from student and teacher in features and logits. Extensive experiments are performed to demonstrate the superiority of our RDD over existing KD methods. For example, our RDD increases mAP of CP-Voxel-S to 57.1% on nuScenes dataset, which even surpasses teacher performance while taking up only 42% FLOPs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Representation_Disparity-aware_Distillation_for_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yanjing Li</author><author>Sheng Xu</author><author>Mingbao Lin</author><author>Jihao Yin</author><author>Baochang Zhang</author><author>Xianbin Cao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions</title>
            <abstract>Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions. Differing from the majority of previous works, which regard actions as single entities and can only generate short sequences for simple motions, we propose EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions. It generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (e.g., motion direction, speed, and body parts) and enables users to generate sequences of unseen complex actions from unique sequences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yijun Qian</author><author>Jack Urbanek</author><author>Alexander G. Hauptmann</author><author>Jungdam Won</author>
            </authors>
        </paper>
        

        <paper>
            <title>VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control</title>
            <abstract>As the model size of pre-trained language models (PLMs) grows rapidly, full fine-tuning becomes prohibitively expensive for model training and storage. In vision-and-language (VL), parameter-efficient tuning (PET) techniques are proposed to integrate modular modifications (e.g., Adapter) into encoder-decoder PLMs. By tuning a small set of trainable parameters, these techniques perform on par with full fine-tuning. However, excessive modular modifications and neglecting the unique abilities of the encoders and decoders can lead to performance degradation, while existing PET techniques (e.g., VL-Adapter) overlook these issues. In this paper, we propose a Vision-and-Language Parameter-Efficient Tuning (VL-PET) framework to impose effective control over modular modifications via a novel granularity-controlled mechanism. Considering different granularity-controlled matrices generated by this mechanism, a variety of model-agnostic VL-PET modules can be instantiated from our framework for better efficiency and effectiveness trade-offs. We further propose lightweight designs to enhance VL alignment and modeling for the encoders and maintain text generation for the decoders. Extensive experiments conducted on four image-text tasks and four video-text tasks demonstrate the efficiency, effectiveness, scalability and transferability of our VL-PET framework. In particular, our VL-PET-large significantly outperforms full fine-tuning by 2.39% (2.61%) and VL-Adapter by 2.92% (3.41%) with BART-base (T5-base) on image-text tasks, while utilizing fewer trainable parameters. Furthermore, we validate the enhanced effect of employing our VL-PET designs (e.g., granularity-controlled mechanism and lightweight designs) on existing PET techniques, enabling them to achieve significant performance improvements.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hu_VL-PET_Vision-and-Language_Parameter-Efficient_Tuning_via_Granularity_Control_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hu_VL-PET_Vision-and-Language_Parameter-Efficient_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zi-Yuan Hu</author><author>Yanyang Li</author><author>Michael R. Lyu</author><author>Liwei Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation</title>
            <abstract>Albeit being a prevalent architecture searching approach, differentiable architecture search (DARTS) is largely hindered by its substantial memory cost since the entire supernet resides in the memory. This is where the single-path DARTS comes in, which only chooses a single-path submodel at each step. While being memory-friendly, it also comes with low computational costs. Nonetheless, we discover a critical issue of single-path DARTS that has not been primarily noticed. Namely, it also suffers from severe performance collapse since too many parameter-free operations like skip connections are derived, just like DARTS does. In this paper, we propose a new algorithm called RObustifying Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology search from the operation search to make searching and evaluation consistent. We then adopt Gumbel-Top2 reparameterization and gradient accumulation to robustify the unwieldy bi-level optimization. We verify ROME extensively across 15 benchmarks to demonstrate its effectiveness and robustness.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_ROME_Robustifying_Memory-Efficient_NAS_via_Topology_Disentanglement_and_Gradient_Accumulation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_ROME_Robustifying_Memory-Efficient_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiaoxing Wang</author><author>Xiangxiang Chu</author><author>Yuda Fan</author><author>Zhexi Zhang</author><author>Bo Zhang</author><author>Xiaokang Yang</author><author>Junchi Yan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge</title>
            <abstract>Answering visual questions requires the ability to parse visual observations and correlate them with a variety of knowledge. Existing visual question answering (VQA) models either pay little attention to the role of knowledge or do not take into account the granularity of knowledge, e.g., attaching the color of &quot;grassland&quot; to &quot;ground&quot;). They have yet to develop the capability of modeling knowledge of multiple granularity, and are also vulnerable to spurious data biases. To fill the gap, this paper makes progresses from two distinct perspectives: (1) It presents a Hierarchical Concept Graph (HCG) that discriminates and associates multi-granularity concepts with a multi-layered hierarchical structure, aligning visual observations with knowledge across different levels to alleviate data biases. (2) To facilitate a comprehensive understanding of how knowledge contributes throughout the decision-making process, we further propose an interpretable Hierarchical Concept Neural Module Network (HCNMN) It explicitly propagates multi-granularity knowledge across the hierarchical structure and incorporates them with a sequence of reasoning steps, providing a transparent interface to elaborate on the integration of observations and knowledge. Through extensive experiments on multiple challenging datasets (i.e., GQA,VQA,FVQA,OK-VQA) , we demonstrate the effectiveness of our method in answering questions in different scenarios. Our code is available at https://github.com/SuperJohnZhang/HCNMN.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Toward_Multi-Granularity_Decision-Making_Explicit_Visual_Reasoning_with_Hierarchical_Knowledge_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Toward_Multi-Granularity_Decision-Making_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yifeng Zhang</author><author>Shi Chen</author><author>Qi Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>3D-aware Image Generation using 2D Diffusion Models</title>
            <abstract>In this paper, we introduce a novel 3D-aware image generation method that leverages 2D diffusion models. We formulate the 3D-aware image generation task as multiview 2D image set generation, and further to a sequential unconditional-conditional multiview image generation process. This allows us to utilize 2D diffusion models to boost the generative modelling power of the method. Additionally, we incorporate depth information from monocular depth estimators to construct the training data for the conditional diffusion model using only still images. We train our method on a large-scale unstructured dataset, i.e., ImageNet, which is not addressed by previous methods. It produces high-quality images that significantly outperform prior methods. Furthermore, our approach showcases its capability to generate instances with large view angles, even though the training images are diverse and unaligned, gathered from &quot;in-the-wild&quot; real-world environments.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xiang_3D-aware_Image_Generation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jianfeng Xiang</author><author>Jiaolong Yang</author><author>Binbin Huang</author><author>Xin Tong</author>
            </authors>
        </paper>
        

        <paper>
            <title>ICE-NeRF: Interactive Color Editing of NeRFs via Decomposition-Aware Weight Optimization</title>
            <abstract>Neural Radiance Fields (NeRFs) have gained considerable attention for their high-quality results in 3D scene reconstruction and rendering. Recently, there have been active studies on various tasks such as novel view synthesis and scene editing. However, editing NeRFs is challenging as accurately decomposing the desired area of 3D space and ensuring the consistency of edited results from different angles is difficult. In this paper, we propose ICE-NeRF, an Interactive Color Editing framework that performs color editing by taking a pre-trained NeRF and a rough user mask as input. Our proposed method performs the entire color editing process in only under a minute using a partial fine-tuning approach. To perform effective color editing, we address two issues: (1) the entanglement of the implicit representation that causes unwanted color changes in undesired areas when learning weights, and (2) the loss of multi-view consistency when fine-tuning for a single or a few views. To address these issues, we introduce two techniques: Activation Field-based Regularization (AFR) and Single-mask Multi-view Rendering (SMR). The AFR performs weight regularization during fine-tuning based on the assumption that not all weights have an equal impact on the desired area. The SMR maps the 2D mask to 3D space through inverse projection and renders it from other views to generate multi-view masks. ICE-NeRF not only enables well-decomposed, multi-view consistent color editing but also significantly reduces processing time compared to existing methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lee_ICE-NeRF_Interactive_Color_Editing_of_NeRFs_via_Decomposition-Aware_Weight_Optimization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jae-Hyeok Lee</author><author>Dae-Shik Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation</title>
            <abstract>Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement. To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced representations of both high- and low-frequency components can enhance the performance of models. We verify this by decomposing visual features into the frequency domain and combining them in a balanced manner. To handle this, we replace the balancing problem with a mask filtering problem in the frequency domain. Then, we introduce a novel token-mixer named SPAM and leverage it to derive a MetaFormer model termed as SPANet. Experimental results show that the proposed method provides a way to achieve this balance, and the balanced representations of both high- and low-frequency components can improve the performance of models on multiple computer vision tasks. Our code is available at https://doranlyong.github.io/projects/spanet/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yun_SPANet_Frequency-balancing_Token_Mixer_using_Spectral_Pooling_Aggregation_Modulation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yun_SPANet_Frequency-balancing_Token_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guhnoo Yun</author><author>Juhan Yoo</author><author>Kijung Kim</author><author>Jeongho Lee</author><author>Dong Hwan Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation</title>
            <abstract>Recent sparse detectors with multiple, e.g. six, decoder layers achieve promising performance but much inference time due to complex heads. Previous works have explored using dense priors as initialization and built one-decoder-layer detectors. Although they gain remarkable acceleration, their performance still lags behind their six-decoder-layer counterparts by a large margin. In this work, we aim to bridge this performance gap while retaining fast speed. We find that the architecture discrepancy between dense and sparse detectors leads to feature conflict, hampering the performance of one-decoder-layer detectors. Thus we propose Adaptive Sparse Anchor Generator (ASAG) which predicts dynamic anchors on patches rather than grids in a sparse way so that it alleviates the feature conflict problem. For each image, ASAG dynamically selects which feature maps and which locations to predict, forming a fully adaptive way to generate image-specific anchors. Further, a simple and effective Query Weighting method eases the training instability from adaptiveness. Extensive experiments show that our method outperforms dense-initialized ones and achieves a better speed-accuracy trade-off. The code is available at https://github.com/iSEE-Laboratory/ASAG.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fu_ASAG_Building_Strong_One-Decoder-Layer_Sparse_Detectors_via_Adaptive_Sparse_Anchor_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fu_ASAG_Building_Strong_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shenghao Fu</author><author>Junkai Yan</author><author>Yipeng Gao</author><author>Xiaohua Xie</author><author>Wei-Shi Zheng</author>
            </authors>
        </paper>
        

        <paper>
            <title>The Perils of Learning From Unlabeled Data: Backdoor Attacks on Semi-supervised Learning</title>
            <abstract>Semi-supervised learning (SSL) is gaining popularity as it reduces cost of machine learning (ML) by training high performance models using unlabeled data. In this paper, we reveal that the key feature of SSL, i.e., learning from (non-inspected) unlabeled data, exposes SSL to strong poisoning attacks that can significantly damage its security. Poisoning is a long-standing problem in conventional supervised ML, but we argue that, as SSL relies on non-inspected unlabeled data, poisoning poses a more significant threat to SSL. We demonstrate this by designing a backdoor poisoning attack on SSL that can be conducted by a weak adversary with no knowledge of the target SSL pipeline. This is unlike prior poisoning attacks on supervised ML that assume strong adversaries with impractical capabilities. We show that by poisoning only 0.2% of the unlabeled training data, our (weak) adversary can successfully cause misclassification on more than 80% of test inputs (when they contain the backdoor trigger). Our attack remains effective across different benchmark datasets and SSL algorithms, and even circumvents state-of-the-art defenses against backdoor attacks. Our work raises significant concerns about the security of SSL in real-world security critical applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shejwalkar_The_Perils_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Virat Shejwalkar</author><author>Lingjuan Lyu</author><author>Amir Houmansadr</author>
            </authors>
        </paper>
        

        <paper>
            <title>StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models</title>
            <abstract>Content and style (C-S) disentanglement is a fundamental problem and critical challenge of style transfer. Existing approaches based on explicit definitions (e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretable nor easy to control, resulting in entangled representations and less satisfying results. In this paper, we propose a new C-S disentangled framework for style transfer without using previous assumptions. The key insight is to explicitly extract the content information and implicitly learn the complementary style information, yielding interpretable and controllable C-S disentanglement and style transfer. A simple yet effective CLIP-based style disentanglement loss coordinated with a style reconstruction prior is introduced to disentangle C-S in the CLIP image space. By further leveraging the powerful style removal and generative ability of diffusion models, our framework achieves superior results than state of the art and flexible C-S disentanglement and trade-off control. Our work provides new insights into the C-S disentanglement in style transfer and demonstrates the potential of diffusion models for learning well-disentangled C-S characteristics.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_StyleDiffusion_Controllable_Disentangled_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhizhong Wang</author><author>Lei Zhao</author><author>Wei Xing</author>
            </authors>
        </paper>
        

        <paper>
            <title>AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models</title>
            <abstract>Previous work on adversarial examples typically involves a fixed norm perturbation budget, which fails to capture the way humans perceive perturbations. Recent work has shifted towards investigating natural unrestricted adversarial examples (UAEs) that breaks l_p perturbation bounds but nonetheless remain semantically plausible. Current methods use GAN or VAE to generate UAEs by perturbing latent codes. However, this leads to loss of high-level information, resulting in low-quality and unnatural UAEs. In light of this, we propose AddDiffuser, a new method for synthesizing natural UAEs using diffusion models. It can generate UAEs from scratch or conditionally based on reference images. To generate natural UAEs, we perturb predicted images to steer their latent code towards the adversarial sample space of a particular classifier. In addition, we propose adversarial inpainting based on class activation mapping to retain the salient regions of the image while perturbing less important areas. Our method achieves impressive results on CIFAR-10, CelebA and ImageNet, and we demonstrate that it can defeat the most robust models on the RobustBench leaderboard with near 100% success rates. Furthermore, The synthesized UAEs are not only more natural but also stronger compared to the current state-of-the-art attacks. Specifically, compared with GA-attack, the UAEs generated with AdvDiffuser exhibit 6xsmaller LPIPS perturbations, 2 ~ 3 xsmaller FID scores and 0.28 higher in SSIM metrics, making them perceptually stealthier. Lastly, it is capable of generating an unlimited number of natural adversarial examples. For more please visit our project page: Link to follow.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_AdvDiffuser_Natural_Adversarial_Example_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xinquan Chen</author><author>Xitong Gao</author><author>Juanjuan Zhao</author><author>Kejiang Ye</author><author>Cheng-Zhong Xu</author>
            </authors>
        </paper>
        

        <paper>
            <title>DarSwin: Distortion Aware Radial Swin Transformer</title>
            <abstract>Wide-angle lenses are commonly used in perception tasks requiring a large field of view. Unfortunately, these lenses produce significant distortions making conventional models that ignore the distortion effects unable to adapt to wide-angle images. In this paper, we present a novel transformer-based model that automatically adapts to the distortion produced by wide-angle lenses. We leverage the physical characteristics of such lenses, which are analytically defined by the radial distortion profile (assumed to be known), to develop a distortion aware radial swin transformer (DarSwin). In contrast to conventional transformer-based architectures, DarSwin comprises a radial patch partitioning, a distortion-based sampling technique for creating token embeddings, and an angular position encoding for radial patch merging. We validate our method on classification tasks using synthetically distorted ImageNet data and show through extensive experiments that DarSwin can perform zero-shot adaptation to unseen distortions of different wide-angle lenses. Compared to other baselines, DarSwin achieves the best results (in terms of Top-1 accuracy) with significant gains when trained on bounded levels of distortions (very-low, low, medium, and high) and tested on all including out-of-distribution distortions. The code and models are publicly available at https://lvsn.github.io/darswin/</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Athwale_DarSwin_Distortion_Aware_Radial_Swin_Transformer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Akshaya Athwale</author><author>Arman Afrasiyabi</author><author>Justin Lag e</author><author>Ichrak Shili</author><author>Ola Ahmad</author><author>Jean-Fran ois Lalonde</author>
            </authors>
        </paper>
        

        <paper>
            <title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models</title>
            <abstract>With the overwhelming trend of mask image modeling led by MAE, generative pre-training has shown a remarkable potential to boost the performance of fundamental models in 2D vision. However, in 3D vision, the over-reliance on Transformer-based backbones and the unordered nature of point clouds have restricted the further development of gen- erative pre-training. In this paper, we propose a novel 3D-to- 2D generative pre-training method that is adaptable to any point cloud model. We propose to generate view images from different instructed poses via the cross-attention mechanism as the pre-training scheme. Generating view images has more precise supervision than its point cloud counterpart, thus assisting 3D backbones to have a finer comprehension of the geometrical structure and stereoscopic relations of the point cloud. Experimental results have proved the su- periority of our proposed 3D-to-2D generative pre-training over previous pre-training methods. Our method is also ef- fective in boosting the performance of architecture-oriented approaches, achieving state-of-the-art performance when fine-tuning on ScanObjectNN classification and ShapeNet- Part segmentation tasks. Code is available at https: //github.com/wangzy22/TakeAPhoto.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Take-A-Photo_3D-to-2D_Generative_Pre-training_of_Point_Cloud_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Take-A-Photo_3D-to-2D_Generative_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ziyi Wang</author><author>Xumin Yu</author><author>Yongming Rao</author><author>Jie Zhou</author><author>Jiwen Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Open-vocabulary Panoptic Segmentation with Embedding Modulation</title>
            <abstract>Open-vocabulary segmentation is attracting increasing attention due to its critical applications in the real world. Traditional closed-vocabulary segmentation methods are not able to characterize novel objects, whereas several recent open-vocabulary attempts obtain unsatisfactory results, i.e., notable performance reduction on the closed-vocabulary and massive demand for extra training data. To this end, we propose OPSNet, an omnipotent and data-efficient framework for Open-vocabulary Panoptic Segmentation. Specifically, the exquisitely designed Embedding Modulation module, together with several meticulous components, enables adequate embedding enhancement and information exchange between the segmentation backbone and the visual-linguistic well-aligned CLIP encoder, resulting in superior segmentation performance under both open- and closed vocabulary settings and much fewer need of additional data. Extensive experimental evaluations are conducted across multiple datasets(e.g., COCO, ADE20K, Cityscapes, and PascalContext) under various circumstances, where the proposed OPSNet achieves state-of-the-art results, which demonstrates the effectiveness and generality of the proposed approach. The code and trained models will be made publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xi Chen</author><author>Shuang Li</author><author>Ser-Nam Lim</author><author>Antonio Torralba</author><author>Hengshuang Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Beyond Single Path Integrated Gradients for Reliable Input Attribution via Randomized Path Sampling</title>
            <abstract>Input attribution is a widely used explanation method for deep neural networks, especially in visual tasks. Among various attribution methods, Integrated Gradients (IG) is frequently used because of its model-agnostic applicability and desirable axioms. However, previous work has shown that such method often produces noisy and unreliable attributions during the integration of the gradients over the path defined in the input space. In this paper, we tackle this issue by estimating the distribution of the possible attributions according to the integrating path selection. We show that such noisy attribution can be reduced by aggregating attributions from the multiple paths instead of using a single path. Inspired by Stick-Breaking Process (SBP), we suggest a random process to generate rich and various sampling of the gradient integrating path. Using multiple input attributions obtained from randomized path, we propose a novel attribution measure using the distribution of attributions at each input features. We identify proposed method qualitatively show less-noisy and object-aligned attribution and its feasibility through the quantitative evaluations.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jeon_Beyond_Single_Path_Integrated_Gradients_for_Reliable_Input_Attribution_via_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jeon_Beyond_Single_Path_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Giyoung Jeon</author><author>Haedong Jeong</author><author>Jaesik Choi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models</title>
            <abstract>Curating datasets for object segmentation is a difficult task. With the advent of large-scale pre-trained generative models, conditional image generation has been given a significant boost in result quality and ease of use. In this paper, we present a novel method that enables the generation of general foreground-background segmentation models from simple textual descriptions, without requiring segmentation labels. We leverage and explore pre-trained latent diffusion models, to automatically generate weak segmentation masks for concepts and objects. The masks are then used to fine-tune the diffusion model on an inpainting task, which enables fine-grained removal of the object, while at the same time providing a synthetic foreground and background dataset. We demonstrate that using this method beats previous methods in both discriminative and generative performance and closes the gap with fully supervised training while requiring no pixel-wise object labels. We show results on the task of segmenting four different objects (humans, dogs, cars, birds) and a use case scenario in medical image analysis. The code is available at https://github.com/MischaD/fobadiffusion.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Dombrowski_Foreground-Background_Separation_through_Concept_Distillation_from_Generative_Image_Foundation_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Dombrowski_Foreground-Background_Separation_through_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Mischa Dombrowski</author><author>Hadrien Reynaud</author><author>Matthew Baugh</author><author>Bernhard Kainz</author>
            </authors>
        </paper>
        

        <paper>
            <title>ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting</title>
            <abstract>Recent advances in neural rendering have shown great potential for reconstructing scenes from multiview images. However, accurately representing objects with glossy surfaces remains a challenge for existing methods. In this work, we introduce ENVIDR, a rendering and modeling framework for high-quality rendering and reconstruction of surfaces with challenging specular reflections. To achieve this, we first propose a novel neural renderer with decomposed rendering components to learn the interaction between surface and environment lighting. This renderer is trained using existing physically based renderers and is decoupled from actual scene representations. We then propose an SDF-based neural surface model that leverages this learned neural renderer to represent general scenes. Our model additionally synthesizes indirect illuminations caused by inter-reflections from shiny surfaces by marching surface-reflected rays. We demonstrate that our method outperforms state-of-art methods on challenging shiny scenes, providing high-quality rendering of specular reflections while also enabling material editing and scene relighting.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ruofan Liang</author><author>Huiting Chen</author><author>Chunlin Li</author><author>Fan Chen</author><author>Selvakumar Panneer</author><author>Nandita Vijaykumar</author>
            </authors>
        </paper>
        

        <paper>
            <title>Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation</title>
            <abstract>Conditional diffusion models have demonstrated impressive performance in image manipulation tasks. The general pipeline involves adding noise to the image and then denoising it. However, this method faces a trade-off problem: adding too much noise affects the fidelity of the image while adding too little affects its editability. This largely limits their practical applicability. In this paper, we propose a novel framework, Selective Diffusion Distillation (SDD), that ensures both the fidelity and editability of images. Instead of directly editing images with a diffusion model, we train a feedforward image manipulation network under the guidance of the diffusion model. Besides, we propose an effective indicator to select the semantic-related timestep to obtain the correct semantic guidance from the diffusion model. This approach successfully avoids the dilemma caused by the diffusion process. Our extensive experiments demonstrate the advantages of our framework.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Not_All_Steps_are_Created_Equal_Selective_Diffusion_Distillation_for_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Not_All_Steps_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Luozhou Wang</author><author>Shuai Yang</author><author>Shu Liu</author><author>Ying-cong Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption</title>
            <abstract>Contrastive Language-Image Pre-training (CLIP) has significantly boosted the performance of various vision-language tasks by scaling up the dataset with image-text pairs collected from the web. However, the presence of intrinsic noise and unmatched image-text pairs in web data can potentially affect the performance of representation learning. To address this issue, we first utilize the OFA model to generate synthetic captions that focus on the image content. The generated captions contain complementary information that is beneficial for pre-training. Then, we propose an Adaptive Language-Image Pre-training (ALIP), a bi-path model that integrates supervision from both raw text and synthetic caption. As the core components of ALIP, the Language Consistency Gate (LCG) and Description Consistency Gate (DCG) dynamically adjust the weights of samples and image-text/caption pairs during the training process. Meanwhile, the adaptive contrastive loss can effectively reduce the impact of noise data and enhances the efficiency of pre-training data. We validate ALIP with experiments on different scales of models and pre-training datasets. Experiments results show that ALIP achieves state-of-the-art performance on multiple downstream tasks including zero-shot image-text retrieval and linear probe. To facilitate future research, the code and pre-trained models are released at https://github.com/deepglint/ALIP.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_ALIP_Adaptive_Language-Image_Pre-Training_with_Synthetic_Caption_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kaicheng Yang</author><author>Jiankang Deng</author><author>Xiang An</author><author>Jiawei Li</author><author>Ziyong Feng</author><author>Jia Guo</author><author>Jing Yang</author><author>Tongliang Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization</title>
            <abstract>Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token embedding and PE, and delivers the same PE to each layer. This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated to PE, and the PE cannot be adjusted on a per-layer basis. To overcome these limitations, we propose using two independent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By implementing this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptive Position Embedding, abbreviated as LaPE, which is simple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the default PE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible parameters, memory, and computational cost.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_LaPE_Layer-adaptive_Position_Embedding_for_Vision_Transformers_with_Independent_Layer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yu_LaPE_Layer-adaptive_Position_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Runyi Yu</author><author>Zhennan Wang</author><author>Yinhuai Wang</author><author>Kehan Li</author><author>Chang Liu</author><author>Haoyi Duan</author><author>Xiangyang Ji</author><author>Jie Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>SA-BEV: Generating Semantic-Aware Bird&apos;s-Eye-View Feature for Multi-view 3D Object Detection</title>
            <abstract>Recently, the pure camera-based Bird&apos;s-Eye-View (BEV) perception provides a feasible solution for economical autonomous driving. However, the existing BEV-based multi-view 3D detectors generally transform all image features into BEV features, without considering the problem that the large proportion of background information may submerge the object information. In this paper, we propose Semantic-Aware BEV Pooling (SA-BEVPool), which can filter out background information according to the semantic segmentation of image features and transform image features into semantic-aware BEV features. Accordingly, we propose BEV-Paste, an effective data augmentation strategy that closely matches with semantic-aware BEV feature. In addition, we design a Multi-Scale Cross-Task (MSCT) head, which combines task-specific and cross-task information to predict depth distribution and semantic segmentation more accurately, further improving the quality of semantic-aware BEV feature. Finally, we integrate the above modules into a novel multi-view 3D object detection framework, namely SA-BEV. Experiments on nuScenes show that SA-BEV achieves state-of-the-art performance. Code has been available at https://github.com/mengtan00/SA-BEV.git.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_SA-BEV_Generating_Semantic-Aware_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jinqing Zhang</author><author>Yanan Zhang</author><author>Qingjie Liu</author><author>Yunhong Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</title>
            <abstract>Recent advancements in pre-trained vision-language models, such as CLIP, have enabled the segmentation of arbitrary concepts solely from textual inputs, a process commonly referred to as open-vocabulary semantic segmentation (OVS). However, existing OVS techniques confront a fundamental challenge: the trained classifier tends to overfit on the base classes observed during training, resulting in suboptimal generalization performance to unseen classes. To mitigate this issue, recent studies have proposed the use of an additional frozen pre-trained CLIP for classification. Nonetheless, this approach incurs heavy computational overheads as the CLIP vision encoder must be repeatedly forward-passed for each mask, rendering it impractical for real-world applications. To address this challenge, our objective is to develop a fast OVS model that can perform comparably or better without the extra computational burden of the CLIP image encoder during inference. To this end, we propose a core idea of preserving the generalizable representation when fine-tuning on known classes. Specifically, we introduce a text diversification strategy that generates a set of synonyms for each training category, which prevents the learned representation from collapsing onto specific known category names. Additionally, we employ a text-guided knowledge distillation method to preserve the generalizable knowledge of CLIP. Extensive experiments demonstrate that our proposed model achieves robust generalization performance across various datasets. Furthermore, we perform a preliminary exploration of open-vocabulary video segmentation and present a benchmark that can facilitate future open-vocabulary research in the video domain.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_Global_Knowledge_Calibration_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kunyang Han</author><author>Yong Liu</author><author>Jun Hao Liew</author><author>Henghui Ding</author><author>Jiajun Liu</author><author>Yitong Wang</author><author>Yansong Tang</author><author>Yujiu Yang</author><author>Jiashi Feng</author><author>Yao Zhao</author><author>Yunchao Wei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Compatibility of Fundamental Matrices for Complete Viewing Graphs</title>
            <abstract>This paper studies the problem of recovering cameras from a set of fundamental matrices. A set of fundamental matrices is said to be compatible if a set of cameras exists for which they are the fundamental matrices. We focus on the complete graph, where fundamental matrices for each pair of cameras are given. Previous work has established necessary and sufficient conditions for compatibility as rank and eigenvalue conditions on the n-view fundamental matrix obtained by concatenating the individual fundamental matrices. In this work, we show that the eigenvalue condition is redundant in the generic and collinear cases. We provide explicit homogeneous polynomials that describe necessary and sufficient conditions for compatibility in terms of the fundamental matrices and their epipoles. In this direction, we find that quadruple-wise compatibility is enough to ensure global compatibility for any number of cameras. We demonstrate that for four cameras, compatibility is generically described by triple-wise conditions and one additional equation involving all fundamental matrices.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bratelund_Compatibility_of_Fundamental_Matrices_for_Complete_Viewing_Graphs_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bratelund_Compatibility_of_Fundamental_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Martin Br telund</author><author>Felix Rydell</author>
            </authors>
        </paper>
        

        <paper>
            <title>MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge</title>
            <abstract>Large scale Vision-Language (VL) models have shown tremendous success in aligning representations between visual and text modalities. This enables remarkable progress in zero-shot recognition, image generation &amp; editing, and many other exciting tasks. However, VL models tend to over-represent objects while paying much less attention to verbs, and require additional tuning on video data for best zero-shot action recognition performance. While previous work relied on large-scale, fully-annotated data, in this work we propose an unsupervised approach. We adapt a VL model for zero-shot and few-shot action recognition using a collection of unlabeled videos and an unpaired action dictionary. Based on that, we leverage Large Language Models and VL models to build a text bag for each unlabeled video via matching, text expansion and captioning. We use those bags in a Multiple Instance Learning setup to adapt an image-text backbone to video data. Although finetuned on unlabeled video data, our resulting models demonstrate high transferability to numerous unseen zero-shot downstream tasks, improving the base VL model performance by up to 14%, and even comparing favorably to fully-supervised baselines in both zero-shot and few-shot video recognition transfer. The code is provided in supplementary and will be released upon acceptance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lin_MAtch_eXpand_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wei Lin</author><author>Leonid Karlinsky</author><author>Nina Shvetsova</author><author>Horst Possegger</author><author>Mateusz Kozinski</author><author>Rameswar Panda</author><author>Rogerio Feris</author><author>Hilde Kuehne</author><author>Horst Bischof</author>
            </authors>
        </paper>
        

        <paper>
            <title>Space Engage: Collaborative Space Supervision for Contrastive-Based Semi-Supervised Semantic Segmentation</title>
            <abstract>Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model with limited labeled images and a substantial volume of unlabeled images. To improve the robustness of representations, powerful methods introduce a pixel-wise contrastive learning approach in latent space (i.e., representation space) that aggregates the representations to their prototypes in a fully supervised manner. However, previous contrastive-based S4 methods merely rely on the supervision from the model&apos;s output (logits) in logit space during unlabeled training. In contrast, we utilize the outputs in both logit space and representation space to obtain supervision in a collaborative way. The supervision from two spaces plays two roles: 1) reduces the risk of over-fitting to incorrect semantic information in logits with the help of representations; 2) enhances the knowledge exchange between the two spaces. Furthermore, unlike previous approaches, we use the similarity between representations and prototypes as a new indicator to tilt training those under-performing representations and achieve a more efficient contrastive learning process. Results on two public benchmarks demonstrate the competitive performance of our method compared with state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Space_Engage_Collaborative_Space_Supervision_for_Contrastive-Based_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Space_Engage_Collaborative_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Changqi Wang</author><author>Haoyu Xie</author><author>Yuhui Yuan</author><author>Chong Fu</author><author>Xiangyu Yue</author>
            </authors>
        </paper>
        

        <paper>
            <title>Delving into Motion-Aware Matching for Monocular 3D Object Tracking</title>
            <abstract>Recent advances of monocular 3D object detection facilitate the 3D multi-object tracking task based on low-cost camera sensors. In this paper, we find that the motion cue of objects along different time frames is critical in 3D multi-object tracking, which is less explored in existing monocular-based approaches. To this end, we propose MoMA-M3T, a framework that mainly consists of three motion-aware components. First, we represent the possible movement of an object related to all object tracklets in the feature space as its motion features. Then, we further model the historical object tracklet along the time frame in a spatial-temporal perspective via a motion transformer. Finally, we propose a motion-aware matching module to associate historical object tracklets and current observations as final tracking results. We conduct extensive experiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3T achieves competitive performance against state-of-the-art methods. Moreover, the proposed tracker is flexible and can be easily plugged into existing image-based 3D object detectors without re-training. Code and models are available at https://github.com/kuanchihhuang/MoMA-M3T.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Delving_into_Motion-Aware_Matching_for_Monocular_3D_Object_Tracking_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Delving_into_Motion-Aware_Matching_for_Monocular_3D_Object_Tracking_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kuan-Chih Huang</author><author>Ming-Hsuan Yang</author><author>Yi-Hsuan Tsai</author>
            </authors>
        </paper>
        

        <paper>
            <title>Fast Adversarial Training with Smooth Convergence</title>
            <abstract>Fast adversarial training (FAT) is beneficial for improving the adversarial robustness of neural networks. However, previous FAT work has encountered a significant issue known as catastrophic overfitting when dealing with large perturbation budgets, i.e. the adversarial robustness of models declines to near zero during training. To address this, we analyze the training process of prior FAT work and observe that catastrophic overfitting is accompanied by the appearance of loss convergence outliers. Therefore, we argue a moderately smooth loss convergence process will be a stable FAT process that solves catastrophic overfitting. To obtain a smooth loss convergence process, we propose a novel oscillatory constraint (dubbed ConvergeSmooth) to limit the loss difference between adjacent epochs. The convergence stride of ConvergeSmooth is introduced to balance convergence and smoothing. Likewise, we design weight centralization without introducing additional hyperparameters other than the loss balance coefficient. Our proposed methods are attack-agnostic and thus can improve the training stability of various FAT techniques. Extensive experiments on popular datasets show that the proposed methods efficiently avoid catastrophic overfitting and outperform all previous FAT methods. Code is available at https://github.com/FAT-CS/ConvergeSmooth.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Fast_Adversarial_Training_with_Smooth_Convergence_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhao_Fast_Adversarial_Training_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mengnan Zhao</author><author>Lihe Zhang</author><author>Yuqiu Kong</author><author>Baocai Yin</author>
            </authors>
        </paper>
        

        <paper>
            <title>A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis</title>
            <abstract>While recent developments in text-to-image generative models have led to a suite of high-performing methods capable of producing creative imagery from free-form text, there are several limitations. By analyzing the cross-attention representations of these models, we notice two key issues. First, for text prompts that contain multiple concepts, there is a significant amount of pixel-space overlap (i.e., same spatial regions) among pairs of different concepts. This eventually leads to the model being unable to distinguish between the two concepts and one of them being ignored in the final generation. Next, while these models attempt to capture all such concepts during the beginning of denoising (e.g., first few steps) as evidenced by cross-attention maps, this knowledge is not retained by the end of denoising (e.g., last few steps). Such loss of knowledge eventually leads to inaccurate generation outputs. To address these issues, our key innovations include two test-time attention-based loss functions that substantially improve the performance of pretrained baseline text-to-image diffusion models. First, our attention segregation loss reduces the cross-attention overlap between attention maps of different concepts in the text prompt, thereby reducing the confusion/conflict among various concepts and the eventual capture of all concepts in the generated output. Next, our attention retention loss explicitly forces text-to-image diffusion models to retain cross-attention information for all concepts across all denoising time steps, thereby leading to reduced information loss and the preservation of all concepts in the generated output. We conduct extensive experiments with the proposed loss functions on a variety of text prompts and demonstrate they lead to generated images that are significantly semantically closer to the input text when compared to baseline text-to-image diffusion models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Agarwal_A-STAR_Test-time_Attention_Segregation_and_Retention_for_Text-to-image_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Agarwal_A-STAR_Test-time_Attention_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Aishwarya Agarwal</author><author>Srikrishna Karanam</author><author>K J Joseph</author><author>Apoorv Saxena</author><author>Koustava Goswami</author><author>Balaji Vasan Srinivasan</author>
            </authors>
        </paper>
        

        <paper>
            <title>FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields</title>
            <abstract>As recent advances in Neural Radiance Fields (NeRF) have enabled high-fidelity 3D face reconstruction and novel view synthesis, its manipulation also became an essential task in 3D vision. However, existing manipulation methods require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing local deformations observed in different instances. As so, our proposed Position-conditional Anchor Compositor (PAC) learns to represent a manipulated scene with spatially varying latent codes. Their renderings with the scene manipulator are then optimized to yield high cosine similarity to a target text in CLIP embedding space for text-driven manipulation. To the best of our knowledge, our approach is the first to address the text-driven manipulation of a face reconstructed with NeRF. Extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hwang_FaceCLIPNeRF_Text-driven_3D_Face_Manipulation_using_Deformable_Neural_Radiance_Fields_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hwang_FaceCLIPNeRF_Text-driven_3D_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Sungwon Hwang</author><author>Junha Hyung</author><author>Daejin Kim</author><author>Min-Jung Kim</author><author>Jaegul Choo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Shape Primitives via Implicit Convexity Regularization</title>
            <abstract>Shape primitives decomposition has been an important and long-standing task in 3D shape analysis. Prior arts heavily rely on 3D point clouds or voxel data for shape primitives extraction, which are less practical in real-world scenarios. This paper proposes to learn shape primitives from multi-view images by introducing implicit surface rendering. It is challenging since implicit shapes have a high degree of freedom, which violates the simplicity property of shape primitives. In this work, a novel regularization term named Implicit Convexity Regularization (ICR) imposed on implicit primitive learning is proposed to tackle this problem. We start with the convexity definition of general 3D shapes, and then derive the equivalent expression for implicit shapes represented by signed distance functions (SDFs). Further, instead of directly constraining the output SDF values which cause unstable optimization, we alternatively impose constraint on second order directional derivatives on line segments inside the shapes, which proves to be a tighter condition for 3D convexity. Implicit primitives constrained by the proposed ICR are combined into a whole object via softmax-weighted-sum operation over all primitive SDFs. Experiments on synthetic and real-world datasets show that our method is able to decompose objects into simple and reasonable shape primitives without the need of segmentation labels or 3D data. Code and data is publicly available in https://github.com/seanywang0408/ICR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Learning_Shape_Primitives_via_Implicit_Convexity_Regularization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Learning_Shape_Primitives_via_Implicit_Convexity_Regularization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiaoyang Huang</author><author>Yi Zhang</author><author>Kai Chen</author><author>Teng Li</author><author>Wenjun Zhang</author><author>Bingbing Ni</author>
            </authors>
        </paper>
        

        <paper>
            <title>ITI-GEN: Inclusive Text-to-Image Generation</title>
            <abstract>Text-to-image generative models often reflect the biases of the training data, leading to unequal representations of underrepresented groups. This study investigates inclusive text-to-image generative models that generate images based on human-written prompts and ensure the resulting images are uniformly distributed across attributes of interest. Unfortunately, directly expressing the desired attributes in the prompt often leads to sub-optimal results due to linguistic ambiguity or model misrepresentation. Hence, this paper proposes a drastically different approach that adheres to the maxim that &quot;a picture is worth a thousand words&quot;. We show that, for some attributes, images can represent concepts more expressively than text. For instance, categories of skin tones are typically hard to specify by text but can be easily represented by example images. Building upon these insights, we propose a novel approach, ITI-GEN, that leverages readily available reference images for Inclusive Text-to-Image GENeration. The key idea is learning a set of prompt embeddings to generate images that can effectively represent all desired attribute categories. More importantly, ITI-GEN requires no model fine-tuning, making it computationally efficient to augment existing text-to-image models. Extensive experiments demonstrate that ITI-GEN largely improves over state-of-the-art models to generate inclusive images from a prompt.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_ITI-GEN_Inclusive_Text-to-Image_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_ITI-GEN_Inclusive_Text-to-Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Cheng Zhang</author><author>Xuanbai Chen</author><author>Siqi Chai</author><author>Chen Henry Wu</author><author>Dmitry Lagun</author><author>Thabo Beeler</author><author>Fernando De la Torre</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation</title>
            <abstract>Unsupervised semantic segmentation is a long-standing challenge in computer vision with great significance. Spectral clustering is a theoretically grounded solution to it where the spectral embeddings for pixels are computed to construct distinct clusters. Despite recent progress in enhancing spectral clustering with powerful pre-trained models, current approaches still suffer from inefficiencies in spectral decomposition and inflexibility in applying them to the test data. This work addresses these issues by casting spectral clustering as a parametric approach that employs neural network-based eigenfunctions to produce spectral embeddings. The outputs of the neural eigenfunctions are further restricted to discrete vectors that indicate clustering assignments directly. As a result, an end-to-end NN-based paradigm of spectral clustering emerges. In practice, the neural eigenfunctions are lightweight and take the features from pre-trained models as inputs, improving training efficiency and unleashing the potential of pre-trained models for dense prediction. We conduct extensive empirical studies to validate the effectiveness of our approach and observe significant performance gains over competitive baselines on Pascal Context, Cityscapes, and ADE20K benchmarks. The code is available at https://github.com/thudzj/NeuralEigenfunctionSegmentor.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Deng_Learning_Neural_Eigenfunctions_for_Unsupervised_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Deng_Learning_Neural_Eigenfunctions_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhijie Deng</author><author>Yucen Luo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Shape Analysis of Euclidean Curves under Frenet-Serret Framework</title>
            <abstract>Geometric frameworks for analyzing curves are common in applications as they focus on invariant features and provide visually satisfying solutions to standard problems such as computing invariant distances, averaging curves, or registering curves. We show that for any smooth curve in R^d, d&gt;1, the generalized curvatures associated with the Frenet-Serret equation can be used to define a Riemannian geometry that takes into account all the geometric features of the shape. This geometry is based on a Square Root Curvature Transform that extends the square root-velocity transform for Euclidean curves (in any dimensions) and provides likely geodesics that avoid artefacts encountered by representations using only first-order geometric information. Our analysis is supported by simulated data and is especially relevant for analyzing human motions. We consider trajectories acquired from sign language, and show the interest of considering curvature and also torsion in their analysis, both being physically meaningful.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chassat_Shape_Analysis_of_Euclidean_Curves_under_Frenet-Serret_Framework_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chassat_Shape_Analysis_of_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Perrine Chassat</author><author>Juhyun Park</author><author>Nicolas Brunel</author>
            </authors>
        </paper>
        

        <paper>
            <title>Efficient Diffusion Training via Min-SNR Weighting Strategy</title>
            <abstract>Denoising diffusion models have been a mainstream approach for image generation, however, training these models often suffers from slow convergence. In this paper, we discovered that the slow convergence is partly due to conflicting optimization directions between timesteps. To address this issue, we treat the diffusion training as a multi-task learning problem, and introduce a simple yet effective approach referred to as Min-SNR-g. This method adapts loss weights of timesteps based on clamped signal-to-noise ratios, which effectively balances the conflicts among timesteps. Our results demonstrate a significant improvement in converging speed, 3.4x faster than previous weighting strategies. It is also more effective, achieving a new record FID score of 2.06 on the ImageNet 256x256 benchmark using smaller architectures than that employed in previous state-of-the-art.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hang_Efficient_Diffusion_Training_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Tiankai Hang</author><author>Shuyang Gu</author><author>Chen Li</author><author>Jianmin Bao</author><author>Dong Chen</author><author>Han Hu</author><author>Xin Geng</author><author>Baining Guo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Perceptual Grouping in Contrastive Vision-Language Models</title>
            <abstract>Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robustness analyses. We find that the resulting model achieves state-of-the-art results in terms of unsupervised segmentation, and demonstrate that the learned representations are uniquely robust to spurious correlations in datasets designed to probe the causal behavior of vision models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ranasinghe_Perceptual_Grouping_in_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kanchana Ranasinghe</author><author>Brandon McKinzie</author><author>Sachin Ravi</author><author>Yinfei Yang</author><author>Alexander Toshev</author><author>Jonathon Shlens</author>
            </authors>
        </paper>
        

        <paper>
            <title>Dynamic Perceiver for Efficient Visual Recognition</title>
            <abstract>Early exiting has become a promising approach to im- proving the inference efficiency of deep networks. By structuring models with multiple classifiers (exits), predictions for &quot;easy&quot; samples can be generated at earlier exits, negating the need for executing deeper layers. Current multi-exit networks typically implement linear classifiers at intermediate layers, compelling low-level features to encapsulate high-level semantics. This sub-optimal design invariably undermines the performance of later exits. In this paper, we propose Dynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedure and the early classification task with a novel dual-branch architecture. A feature branch serves to extract image features, while a classification branch processes a latent code assigned for classification tasks. Bi-directional cross-attention layers are established to progressively fuse the information of both branches. Early exits are placed exclusively within the classification branch, thus eliminating the need for linear separability in low-level features. Dyn-Perceiver constitutes a versatile and adaptable framework that can be built upon various architectures. Experiments on image classification, action recognition, and object detection demonstrate that our method significantly improves the inference efficiency of different backbones, outperforming numerous competitive approaches across a broad range of computational budgets. Evaluation on both CPU and GPU platforms substantiate the superior practical efficiency of Dyn-Perceiver. Code is available at https://www.github. com/LeapLabTHU/Dynamic_Perceiver.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_Dynamic_Perceiver_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yizeng Han</author><author>Dongchen Han</author><author>Zeyu Liu</author><author>Yulin Wang</author><author>Xuran Pan</author><author>Yifan Pu</author><author>Chao Deng</author><author>Junlan Feng</author><author>Shiji Song</author><author>Gao Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption</title>
            <abstract>Training a generative model with limited number of samples is a challenging task. Current methods primarily rely on few-shot model adaption to train the network. However, in scenarios where data is extremely limited (less than 10), the generative network tends to overfit and suffers from content degradation. To address these problems, we propose a novel phasic content fusing few-shot diffusion model with directional distribution consistency loss, which targets different learning objectives at distinct training stages of the diffusion model. Specifically, we design a phasic training strategy with phasic content fusion to help our model learn content and style information when t is large, and learn local details of target domain when t is small, leading to an improvement in the capture of content, style and local details. Furthermore, we introduce a novel directional distribution consistency loss that ensures the consistency between the generated and source distributions more efficiently and stably than the prior methods, preventing our model from overfitting. Finally, we propose a cross-domain structure guidance strategy that enhances structure consistency during domain adaptation. Theoretical analysis, qualitative and quantitative experiments demonstrate the superiority of our approach in few-shot generative model adaption tasks compared to state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Teng Hu</author><author>Jiangning Zhang</author><author>Liang Liu</author><author>Ran Yi</author><author>Siqi Kou</author><author>Haokun Zhu</author><author>Xu Chen</author><author>Yabiao Wang</author><author>Chengjie Wang</author><author>Lizhuang Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling</title>
            <abstract>We present the first active learning tool for fine-grained 3D part labeling, a problem which challenges even the most advanced deep learning (DL) methods due to the significant structural variations among the intricate parts. For the same reason, the necessary effort to annotate training data is tremendous, motivating approaches to minimize human involvement. Our labeling tool iteratively verifies or modifies part labels predicted by a deep neural network, with human feedback continually improving the network prediction. To effectively reduce human efforts, we develop two novel features in our tool, hierarchical and symmetry-aware active labeling. Our human-in-the-loop approach, coined HAL3D, achieves close to error-free fine-grained annotations on any test set with pre-defined hierarchical part labels, with 80% time-saving over manual effort. We will release the finely labeled models to serve the community.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_HAL3D_Hierarchical_Active_Learning_for_Fine-Grained_3D_Part_Labeling_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yu_HAL3D_Hierarchical_Active_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Fenggen Yu</author><author>Yiming Qian</author><author>Francisca Gil-Ureta</author><author>Brian Jackson</author><author>Eric Bennett</author><author>Hao Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning</title>
            <abstract>Personalized Federated Learning (PFL) represents a promising solution for decentralized learning in heterogeneous data environments. Partial model personalization has been proposed to improve the efficiency of PFL by selectively updating local model parameters instead of aggregating all of them. However, previous work on partial model personalization has mainly focused on Convolutional Neural Networks (CNNs), leaving a gap in understanding how it can be applied to other popular models such as Vision Transformers (ViTs). In this work, we investigate where and how to partially personalize a ViT model. Specifically, we empirically evaluate the sensitivity to data distribution of each type of layer. Based on the insights that the self-attention layer and the classification head are the most sensitive parts of a ViT, we propose a novel approach called FedPerfix, which leverages plugins to transfer information from the aggregated model to the local client as a personalization. Finally, we evaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Home datasets and demonstrate its effectiveness in improving the model&apos;s performance compared to several advanced PFL methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sun_FedPerfix_Towards_Partial_Model_Personalization_of_Vision_Transformers_in_Federated_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sun_FedPerfix_Towards_Partial_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guangyu Sun</author><author>Matias Mendieta</author><author>Jun Luo</author><author>Shandong Wu</author><author>Chen Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration</title>
            <abstract>In this paper, we address the problem of conditional scene decoration for 360deg images. Our method takes a 360deg background photograph of an indoor scene and generates decorated images of the same scene in the panorama view. To do this, we develop a 360-aware object layout generator that learns latent object vectors in the 360deg view to enable a variety of furniture arrangements for an input 360deg background image. We use this object layout to condition a generative adversarial network to synthesize images of an input scene. To further reinforce the generation capability of our model, we develop a simple yet effective scene emptier that removes the generated furniture and produces an emptied scene for our model to learn a cyclic constraint. We train the model on the Structure3D dataset and show that our model can generate diverse decorations with controllable object layout. Our method achieves state-of-the-art performance on the Structure3D dataset and generalizes well to the Zillow indoor scene dataset. Our user study confirms the immersive experiences provided by the realistic image quality and furniture layout in our generation results. Our implementation is available at https://github.com/kcshum/neural_360_decoration.git.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shum_Conditional_360-degree_Image_Synthesis_for_Immersive_Indoor_Scene_Decoration_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shum_Conditional_360-degree_Image_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Ka Chun Shum</author><author>Hong-Wing Pang</author><author>Binh-Son Hua</author><author>Duc Thanh Nguyen</author><author>Sai-Kit Yeung</author>
            </authors>
        </paper>
        

        <paper>
            <title>SIDGAN: High-Resolution Dubbed Video Generation via Shift-Invariant Learning</title>
            <abstract>Dubbed video generation aims to accurately synchronize mouth movements of a given facial video with driving audio while preserving identity and scene-specific visual dynamics, such as head pose and lighting. Despite the accurate lip generation of previous approaches that adopts a pretrained audio-video synchronization metric as an objective function, called Sync-Loss, extending it to high-resolution videos was challenging due to shift biases in the loss landscape that inhibit tandem optimization of Sync-Loss and visual quality, leading to a loss of detail. To address this issue, we introduce shift-invariant learning, which generates photo-realistic high-resolution videos with accurate Lip-Sync. Further, we employ a pyramid network with coarse-to-fine image generation to improve stability and lip syncronization. Our model outperforms state-of-the-art methods on multiple benchmark datasets, including AVSpeech, HDTF, and LRW, in terms of photo-realism, identity preservation, and Lip-Sync accuracy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Muaz_SIDGAN_High-Resolution_Dubbed_Video_Generation_via_Shift-Invariant_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Muaz_SIDGAN_High-Resolution_Dubbed_Video_Generation_via_Shift-Invariant_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Urwa Muaz</author><author>Wondong Jang</author><author>Rohun Tripathi</author><author>Santhosh Mani</author><author>Wenbin Ouyang</author><author>Ravi Teja Gadde</author><author>Baris Gecer</author><author>Sergio Elizondo</author><author>Reza Madad</author><author>Naveen Nair</author>
            </authors>
        </paper>
        

        <paper>
            <title>Meta-ZSDETR: Zero-shot DETR with Meta-learning</title>
            <abstract>Zero-shot object detection aims to localize and recognize objects of unseen classes. Most of existing works face two problems: the low recall of RPN in unseen classes and the confusion of unseen classes with background. In this paper, we present the first method that combines DETR and meta-learning to perform zero-shot object detection, named Meta-ZSDETR, where model training is formalized as an individual episode based meta-learning task. Different from Faster R-CNN based methods that firstly generate class-agnostic proposals, and then classify them with visual-semantic alignment module, Meta-ZSDETR directly predict class-specific boxes with class-specific queries and further filter them with the predicted accuracy from classification head. The model is optimized with meta-contrastive learning, which contains a regression head to generate the coordinates of class-specific boxes, a classification head to predict the accuracy of generated boxes, and a contrastive head that utilizes the proposed contrastive-reconstruction loss to further separate different classes in visual space. We conduct extensive experiments on two benchmark datasets MS COCO and PASCAL VOC. Experimental results show that our method outperforms the existing ZSD methods by a large margin.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Meta-ZSDETR_Zero-shot_DETR_with_Meta-learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Meta-ZSDETR_Zero-shot_DETR_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lu Zhang</author><author>Chenbo Zhang</author><author>Jiajia Zhao</author><author>Jihong Guan</author><author>Shuigeng Zhou</author>
            </authors>
        </paper>
        

        <paper>
            <title>STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition</title>
            <abstract>Existing methods of privacy-preserving action recognition (PPAR) mainly focus on frame-level (spatial) privacy removal through 2D CNNs. Unfortunately, they have two major drawbacks. First, they may compromise temporal dynamics in input videos, which are critical for accurate action recognition. Second, they are vulnerable to practical attacking scenarios where attackers probe for privacy from an entire video rather than individual frames. To address these issues, we propose a novel framework STPrivacy to perform video-level PPAR. For the first time, we introduce vision Transformers into PPAR by treating a video as a tubelet sequence, and accordingly design two complementary mechanisms, i.e., sparsification and anonymization, to remove privacy from a spatio-temporal perspective. In specific, our privacy sparsification mechanism applies adaptive token selection to abandon action-irrelevant tubelets. Then, our anonymization mechanism implicitly manipulates the remaining action-tubelets to erase privacy in the embedding space through adversarial learning. These mechanisms provide significant advantages in terms of privacy preservation for human eyes and action-privacy trade-off adjustment during deployment. We additionally contribute the first two large-scale PPAR benchmarks, VP-HMDB51 and VP-UCF101, to the community. Extensive evaluations on them, as well as two other tasks, validate the effectiveness and generalization capability of our framework.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_STPrivacy_Spatio-Temporal_Privacy-Preserving_Action_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ming Li</author><author>Xiangyu Xu</author><author>Hehe Fan</author><author>Pan Zhou</author><author>Jun Liu</author><author>Jia-Wei Liu</author><author>Jiahe Li</author><author>Jussi Keppo</author><author>Mike Zheng Shou</author><author>Shuicheng Yan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Computationally-Efficient Neural Image Compression with Shallow Decoders</title>
            <abstract>Neural image compression methods have seen increasingly strong performance in recent years. However, they suffer orders of magnitude higher computational complexity compared to traditional codecs, which hinders their real-world deployment. This paper takes a step forward in closing this gap in decoding complexity by adopting shallow or even linear decoding transforms. To compensate for the resulting drop in compression performance, we exploit the often asymmetrical computation budget between encoding and decoding, by adopting more powerful encoder networks and iterative encoding. We theoretically formalize the intuition behind, and our experimental results establish a new frontier in the trade-off between rate-distortion and decoding complexity for neural image compression. Specifically, we achieve rate-distortion performance competitive with the established mean-scale hyperprior architecture of Minnen et al. (2018) at less than 50K decoding FLOPs/pixel, reducing the baseline&apos;s overall decoding complexity by 80%, or over 90% for the synthesis transform alone. Our code can be found at https://github.com/mandt-lab/shallow-ntc.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_Computationally-Efficient_Neural_Image_Compression_with_Shallow_Decoders_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_Computationally-Efficient_Neural_Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yibo Yang</author><author>Stephan Mandt</author>
            </authors>
        </paper>
        

        <paper>
            <title>Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence</title>
            <abstract>Deep neural networks are vulnerable to adversarial attacks. In this paper, we take the role of investigators who want to trace the attack and identify the source, that is, the particular model which the adversarial examples are generated from. Techniques derived would aid forensic investigation of attack incidents and serve as deterrence to potential attacks. We consider the buyers-seller setting where a machine learning model is to be distributed to various buyers and each buyer receives a slightly different copy with same functionality. A malicious buyer generates adversarial examples from a particular copy &quot;Mi&quot; and uses them to attack other copies. From these adversarial examples, the investigator wants to identify the source &quot;Mi&quot;. To address this problem, we propose a two-stage separate-and-trace framework. The model separation stage generates multiple copies of a model for a same classification task. This process injects unique characteristics into each copy so that adversarial examples generated have distinct and traceable features. We give a parallel structure which pairs a unique tracer with the original classification model in each copy and a variational autoencoder (VAE)-based training method to achieve this goal. The tracing stage takes in adversarial examples and a few candidate models, and identifies the likely source. Based on the unique features induced by the tracer, we could effectively trace the potential adversarial copy by considering the output logits from each tracer. Empirical results show that it is possible to trace the origin of the adversarial example and the mechanism can be applied to a wide range of architectures and datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fang_Tracing_the_Origin_of_Adversarial_Attack_for_Forensic_Investigation_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fang_Tracing_the_Origin_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Han Fang</author><author>Jiyi Zhang</author><author>Yupeng Qiu</author><author>Jiayang Liu</author><author>Ke Xu</author><author>Chengfang Fang</author><author>Ee-Chien Chang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation</title>
            <abstract>Automatic high-quality rendering of anime scenes from complex real-world images is of significant practical value. The challenges of this task lie in the complexity of the scenes, the unique features of anime style, and the lack of high-quality datasets to bridge the domain gap. Despite promising attempts, previous efforts are still incompetent in achieving satisfactory results with consistent semantic preservation, evident stylization, and fine details. In this study, we propose Scenimefy, a novel semi-supervised image-to-image translation framework that addresses these challenges. Our approach guides the learning with structure-consistent pseudo paired data, simplifying the pure unsupervised setting. The pseudo data are derived uniquely from a semantic-constrained StyleGAN leveraging rich model priors like CLIP. We further apply segmentation-guided data selection to obtain high-quality pseudo supervision. A patch-wise contrastive style loss is introduced to improve stylization and fine details. Besides, we contribute a high-resolution anime scene dataset to facilitate future research. Our extensive experiments demonstrate the superiority of our method over state-of-the-art baselines in terms of both perceptual quality and quantitative performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jiang_Scenimefy_Learning_to_Craft_Anime_Scene_via_Semi-Supervised_Image-to-Image_Translation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jiang_Scenimefy_Learning_to_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuxin Jiang</author><author>Liming Jiang</author><author>Shuai Yang</author><author>Chen Change Loy</author>
            </authors>
        </paper>
        

        <paper>
            <title>DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration</title>
            <abstract>The visual models pretrained on large-scale benchmarks encode general knowledge and prove effective in building more powerful representations for downstream tasks. Most existing approaches follow the fine-tuning paradigm, either by initializing or regularizing the downstream model based on the pretrained one. The former fails to retain the knowledge in the successive fine-tuning phase, thereby prone to be over-fitting, and the latter imposes strong constraints to the weights or feature maps of the downstream model without considering semantic drift, often incurring insufficient optimization. To deal with these issues, we propose a novel fine-tuning framework, namely distribution regularization with semantic calibration (DR-Tune). It employs distribution regularization by enforcing the downstream task head to decrease its classification error on the pretrained feature distribution, which prevents it from over-fitting while enabling sufficient training of downstream encoders. Furthermore, to alleviate the interference by semantic drift, we develop the semantic calibration (SC) module to align the global shape and class centers of the pretrained and downstream feature distributions. Extensive experiments on widely used image classification datasets show that DR-Tune consistently improves the performance when combing with various backbones under different pretraining strategies. Code is available at: https://github.com/weeknan/DR-Tune.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_DR-Tune_Improving_Fine-tuning_of_Pretrained_Visual_Models_by_Distribution_Regularization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhou_DR-Tune_Improving_Fine-tuning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nan Zhou</author><author>Jiaxin Chen</author><author>Di Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation</title>
            <abstract>Sound can convey significant information for spatial reasoning in our daily lives. To endow deep networks with such ability, we address the challenge of dense indoor prediction with sound in both 2D and 3D via cross-modal knowledge distillation. In this work, we propose a Spatial Alignment via Matching (SAM) distillation framework that elicits local correspondence between the two modalities in vision-to-audio knowledge transfer. SAM integrates audio features with visually coherent learnable spatial embeddings to resolve inconsistencies in multiple layers of a student model. Our approach does not rely on a specific input representation, allowing for flexibility in the input shapes or dimensions without performance degradation. With a newly curated benchmark named Dense Auditory Prediction of Surroundings (DAPS), we are the first to tackle dense indoor prediction of omnidirectional surroundings in both 2D and 3D with audio observations. Specifically, for audio-based depth estimation, semantic segmentation, and challenging 3D scene reconstruction, the proposed distillation framework consistently achieves state-of-the-art performance across various metrics and backbone architectures.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yun_Dense_2D-3D_Indoor_Prediction_with_Sound_via_Aligned_Cross-Modal_Distillation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yun_Dense_2D-3D_Indoor_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Heeseung Yun</author><author>Joonil Na</author><author>Gunhee Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>EverLight: Indoor-Outdoor Editable HDR Lighting Estimation</title>
            <abstract>Because of the diversity in lighting environments, existing illumination estimation techniques have been designed explicitly on indoor or outdoor environments. Methods have focused specifically on capturing accurate energy (e.g., through parametric lighting models), which emphasizes shading and strong cast shadows; or producing plausible texture (e.g., with GANs), which prioritizes plausible reflections. Approaches which provide editable lighting capabilities have been proposed, but these tend to be with simplified lighting models, offering limited realism. In this work, we propose to bridge the gap between these recent trends in the literature, and propose a method which combines a parametric light model with 360deg panoramas, ready to use as HDRI in rendering engines. We leverage recent advances in GAN-based LDR panorama extrapolation from a regular image, which we extend to HDR using parametric spherical gaussians. To achieve this, we introduce a novel lighting co-modulation method that injects lighting-related features throughout the generator, tightly coupling the original or edited scene illumination within the panorama generation process. In our representation, users can easily edit light direction, intensity, number, etc. to impact shading while providing rich, complex reflections while seamlessly blending with the edits. Furthermore, our method encompasses indoor and outdoor environments, demonstrating state-of-the-art results even when compared to domain-specific methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Dastjerdi_EverLight_Indoor-Outdoor_Editable_HDR_Lighting_Estimation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Mohammad Reza Karimi Dastjerdi</author><author>Jonathan Eisenmann</author><author>Yannick Hold-Geoffroy</author><author>Jean-Fran ois Lalonde</author>
            </authors>
        </paper>
        

        <paper>
            <title>MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation</title>
            <abstract>Weakly-supervised semantic segmentation aims to reduce labeling costs by training semantic segmentation models using weak supervision, such as image-level class labels. However, most approaches struggle to produce accurate localization maps and suffer from false predictions in class-related backgrounds (i.e., biased objects), such as detecting a railroad with the train class. Recent methods that remove biased objects require additional supervision for manually identifying biased objects for each problematic class and collecting their datasets by reviewing predictions, limiting their applicability to the real-world dataset with multiple labels and complex relationships for biasing. Following the first observation that biased features can be separated and eliminated by matching biased objects with backgrounds in the same dataset, we propose a fully-automatic/model-agnostic biased removal framework called MARS (Model-Agnostic biased object Removal without additional Supervision), which utilizes semantically consistent features of an unsupervised technique to eliminate biased objects in pseudo labels. Surprisingly, we show that MARS achieves new state-of-the-art results on two popular benchmarks, PASCAL VOC 2012 (val: 77.7%, test: 77.2%) and MS COCO 2014 (val: 49.4%), by consistently improving the performance of various WSSS models by at least 30% without additional supervision. Code is available at https://github.com/shjo-april/MARS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jo_MARS_Model-agnostic_Biased_Object_Removal_without_Additional_Supervision_for_Weakly-Supervised_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jo_MARS_Model-agnostic_Biased_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sanghyun Jo</author><author>In-Jae Yu</author><author>Kyungsu Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation</title>
            <abstract>We study the problem of few-shot physically-aware articulated mesh generation. By observing an articulated object dataset containing only a few examples, we wish to learn a model that can generate diverse meshes with high visual fidelity and physical validity. Previous mesh generative models either have difficulties in depicting a diverse data space from only a few examples or fail to ensure physical validity of their samples. Regarding the above challenges, we propose two key innovations, including 1) a hierarchical mesh deformation-based generative model based upon the divide-and-conquer philosophy to alleviate the few-shot challenge by borrowing transferrable deformation patterns from large scale rigid meshes and 2) a physics-aware deformation correction scheme to encourage physically plausible generations. We conduct extensive experiments on 6 articulated categories to demonstrate the superiority of our method in generating articulated meshes with better diversity, higher visual fidelity, and better physical validity over previous methods in the few-shot setting. Further, we validate solid contributions of our two innovations in the ablation study. Project page with code is available at https://meowuu7.github.io/few-arti-obj-gen.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Few-Shot_Physically-Aware_Articulated_Mesh_Generation_via_Hierarchical_Deformation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Few-Shot_Physically-Aware_Articulated_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Xueyi Liu</author><author>Bin Wang</author><author>He Wang</author><author>Li Yi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction</title>
            <abstract>3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, developing a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generalizable prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models. In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent diffusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for unconditional generation, or combine it with arbitrary observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Single-Stage_Diffusion_NeRF_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Hansheng Chen</author><author>Jiatao Gu</author><author>Anpei Chen</author><author>Wei Tian</author><author>Zhuowen Tu</author><author>Lingjie Liu</author><author>Hao Su</author>
            </authors>
        </paper>
        

        <paper>
            <title>One-Shot Generative Domain Adaptation</title>
            <abstract>This work aims to transfer a Generative Adversarial Network (GAN) pre-trained on one image domain to another domain referred to as few as just one reference image. The challenge is that, under limited supervision, it is extremely difficult to synthesize photo realistic and highly diverse images while retaining the representative characters of the target domain. Different from existing approaches that adopt the vanilla fine-tuning strategy, we design two lightweight modules in the generator and the discriminator respectively. We first introduce an attribute adaptor in the generator and freeze the generator&apos;s original parameters, which can reuse the prior knowledge to the most extent and maintain the synthesis quality and diversity. We then equip the well-learned discriminator with an attribute classifier to ensure that the generator with the attribute adaptor captures the appropriate characters of the reference image. Furthermore, considering the very limited diversity of the training data (i.e., as few as only one image), we propose to constrain the diversity of the latent space through truncation in the training process, alleviating the optimization difficulty. Our approach brings appealing results under various settings, substantially surpassing state-of-the-art alternatives, especially in terms of synthesis diversity. Noticeably, our method works well even with large domain gaps and robustly converges within a few minutes for each experiment. Code and models are available at https://genforce.github.io/genda/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_One-Shot_Generative_Domain_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ceyuan Yang</author><author>Yujun Shen</author><author>Zhiyi Zhang</author><author>Yinghao Xu</author><author>Jiapeng Zhu</author><author>Zhirong Wu</author><author>Bolei Zhou</author>
            </authors>
        </paper>
        

        <paper>
            <title>HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness</title>
            <abstract>Convolutional Neural Networks (CNN) are known to exhibit poor generalization performance under distribution shifts. Their generalization have been studied extensively, and one line of work approaches the problem from a frequency-centric perspective. These studies highlight the fact that humans and CNNs might focus on different frequency components of an image. First, inspired by these observations, we propose a simple yet effective data augmentation method HybridAugment that reduces the reliance of CNNs on high-frequency components, and thus improves their robustness while keeping their clean accuracy high. Second, we propose HybridAugment++, which is a hierarchical augmentation method that attempts to unify various frequency-spectrum augmentations. HybridAugment++ builds on HybridAugment, and also reduces the reliance of CNNs on the amplitude component of images, and promotes phase information instead. This unification results in competitive to or better than state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet), corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial robustness on CIFAR-10 and out-of-distribution detection on various datasets. HybridAugment and HybridAugment++ are implemented in a few lines of code, does not require extra data, ensemble models or additional networks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yucel_HybridAugment_Unified_Frequency_Spectra_Perturbations_for_Model_Robustness_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yucel_HybridAugment_Unified_Frequency_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mehmet Kerim Yucel</author><author>Ramazan Gokberk Cinbis</author><author>Pinar Duygulu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Doppelgangers: Learning to Disambiguate Images of Similar Structures</title>
            <abstract>We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cai_Doppelgangers_Learning_to_Disambiguate_Images_of_Similar_Structures_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cai_Doppelgangers_Learning_to_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ruojin Cai</author><author>Joseph Tung</author><author>Qianqian Wang</author><author>Hadar Averbuch-Elor</author><author>Bharath Hariharan</author><author>Noah Snavely</author>
            </authors>
        </paper>
        

        <paper>
            <title>Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning</title>
            <abstract>Deep neural networks are susceptible to adversarial examples, posing a significant security risk in critical applications. Adversarial Training (AT) is a well-established technique to enhance adversarial robustness, but it often comes at the cost of decreased generalization ability. This paper proposes Robustness Critical Fine-Tuning (RiFT), a novel approach to enhance generalization without compromising adversarial robustness. The core idea of RiFT is to exploit the redundant capacity for robustness by fine-tuning the adversarially trained model on its non-robust-critical module. To do so, we introduce module robust criticality (MRC), a measure that evaluates the significance of a given module to model robustness under worst-case weight perturbations. Using this measure, we identify the module with the lowest MRC value as the non-robust-critical module and fine-tune its weights to obtain fine-tuned weights. Subsequently, we linearly interpolate between the adversarially trained weights and fine-tuned weights to derive the optimal fine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18, ResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. Our experiments show that RiFT can significantly improve both generalization and out-of-distribution robust- ness by around 1.5% while maintaining or even slightly enhancing adversarial robustness. Code is available at https://github.com/Immortalise/RiFT .</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Improving_Generalization_of_Adversarial_Training_via_Robust_Critical_Fine-Tuning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Improving_Generalization_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kaijie Zhu</author><author>Xixu Hu</author><author>Jindong Wang</author><author>Xing Xie</author><author>Ge Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Understanding the Feature Norm for Out-of-Distribution Detection</title>
            <abstract>A neural network trained on a classification dataset often exhibits a higher vector norm of hidden layer features for in-distribution (ID) samples, while producing relatively lower norm values on unseen instances from out-of-distribution (OOD). Despite this intriguing phenomenon being utilized in many applications, the underlying cause has not been thoroughly investigated. In this study, we demystify this very phenomenon by scrutinizing the discriminative structures concealed in the intermediate layers of a neural network. Our analysis leads to the following discoveries: (1) The feature norm is a confidence value of a classifier hidden in the network layer, specifically its maximum logit. Hence, the feature norm distinguishes OOD from ID in the same manner that a classifier confidence does. (2) The feature norm is class-agnostic, thus it can detect OOD samples across diverse discriminative models. (3) The conventional feature norm fails to capture the deactivation tendency of hidden layer neurons, which may lead to misidentification of ID samples as OOD instances. To resolve this drawback, we propose a novel negative-aware norm (NAN) that can capture both the activation and deactivation tendencies of hidden layer neurons. We conduct extensive experiments on NAN, demonstrating its efficacy and compatibility with existing OOD detectors, as well as its capability in label-free environments.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Park_Understanding_the_Feature_Norm_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Park_Understanding_the_Feature_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jaewoo Park</author><author>Jacky Chen Long Chai</author><author>Jaeho Yoon</author><author>Andrew Beng Jin Teoh</author>
            </authors>
        </paper>
        

        <paper>
            <title>Knowledge Proxy Intervention for Deconfounded Video Question Answering</title>
            <abstract>Recently, Video Question-Answering (VideoQA) has drawn more and more attention from both industry and research community. Despite all the success achieved by recent works, dataset bias always harmfully misleads current methods focusing on spurious correlations in training data. To analyze the effects of dataset bias, we frame the VideoQA pipeline into a causal graph, which shows the causalities among video, question, aligned feature between video and question, answer, and underlying confounder. Through the causal graph, we prove that the confounder and the backdoor path lead to spurious causality. To tackle the challenge that the confounder in VideoQA is unobserved and non-enumerable in general, we propose a model-agnostic framework called Knowledge Proxy Intervention (KPI), which introduces an extra knowledge proxy variable in the causal graph to cut the backdoor path and remove the confounder. Our KPI framework exploits the front-door adjustment, which requires no prior knowledge about the confounder. The effectiveness of our KPI framework is corroborated by three baseline methods on five benchmark datasets, including MSVD-QA, MSRVTT-QA, TGIF-QA, NExT-QA, and Causal-VidQA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Knowledge_Proxy_Intervention_for_Deconfounded_Video_Question_Answering_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Knowledge_Proxy_Intervention_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiangtong Li</author><author>Li Niu</author><author>Liqing Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds</title>
            <abstract>Existing offboard 3D detectors always follow a modular pipeline design to take advantage of unlimited sequential point clouds. We have found that the full potential of offboard 3D detectors is not explored mainly due to two reasons: (1) the onboard multi-object tracker cannot generate sufficient complete object trajectories, and (2) the motion state of objects poses an inevitable challenge for the object-centric refining stage in leveraging the long-term temporal context representation. To tackle these problems, we propose a novel paradigm of offboard 3D object detection, named DetZero. Concretely, an offline tracker coupled with a multi-frame detector is proposed to focus on the completeness of generated object tracks. An attention-mechanism refining module is proposed to strengthen contextual information interaction across long-term sequential point clouds for object refining with decomposed regression methods. Extensive experiments on Waymo Open Dataset show our DetZero outperforms all state-of-the-art onboard and offboard 3D detection methods. Notably, DetZero ranks 1st place on Waymo 3D object detection leaderboard with 85.15 mAPH (L2) detection performance. Further experiments validate the application of taking the place of human labels with such high-quality results. Our empirical study leads to rethinking conventions and interesting findings that can guide future research on offboard 3D object detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ma_DetZero_Rethinking_Offboard_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Tao Ma</author><author>Xuemeng Yang</author><author>Hongbin Zhou</author><author>Xin Li</author><author>Botian Shi</author><author>Junjie Liu</author><author>Yuchen Yang</author><author>Zhizheng Liu</author><author>Liang He</author><author>Yu Qiao</author><author>Yikang Li</author><author>Hongsheng Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning from Noisy Data for Semi-Supervised 3D Object Detection</title>
            <abstract>Pseudo-Labeling (PL) is a critical approach in semi-supervised 3D object detection (SSOD). In PL, delicately selected pseudo-labels, generated by the teacher model, are provided for the student model to supervise the semi-supervised detection framework. However, such a paradigm may introduce misclassified labels or loose localized box predictions, resulting in a sub-optimal solution of detection performance. In this paper, we take PL from a noisy learning perspective: instead of directly applying vanilla pseudo-labels, we design a noise-resistant instance supervision module for better generalization. Specifically, we soften the classification targets by considering both the quality of pseudo labels and the network learning ability, and convert the regression task into a probabilistic modeling problem. Besides, considering that self-supervised learning works in the absence of labels, we incorporate dense pixel-wise feature consistency constraints to eliminate the negative impact of noisy labels. To this end, we propose NoiseDet, a simple yet effective framework for semi-supervised 3D object detection. Extensive experiments on competitive ONCE and Waymo benchmarks demonstrate that our method outperforms current semi-supervised approaches by a large margin. Notably, our NoiseDet achieves state-of-the-art performance under various dataset scales on ONCE dataset. For example, NoiseDet improves its NoiseyStudent baseline from 55.5 mAP to 58.0 mAP, and further reaches 60.2 mAP with enhanced pseudo-label generation. Code will be available at https://github.com/zehuichen123/NoiseDet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Learning_from_Noisy_Data_for_Semi-Supervised_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Zehui Chen</author><author>Zhenyu Li</author><author>Shuo Wang</author><author>Dengpan Fu</author><author>Feng Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond</title>
            <abstract>An authentic face restoration system is becoming increasingly demanding in many computer vision applications, e.g., image enhancement, video communication, and taking portrait. Most of the advanced face restoration models can recover high-quality faces from low-quality ones but usually fail to faithfully generate realistic and high-frequency details that are favored by users. To achieve authentic restoration, we propose IDM, an Iteratively learned face restoration system based on denoising Diffusion Models (DDMs). We define the criterion of an authentic face restoration system, and argue that denoising diffusion models are naturally endowed with this property from two aspects: intrinsic iterative refinement and extrinsic iterative enhancement. Intrinsic learning can preserve the content well and gradually refine the high-quality details, while extrinsic enhancement helps clean the data and improve the restoration task one step further. We demonstrate superior performance on blind face restoration tasks. Beyond restoration, we find the authentically cleaned data by the proposed restoration system is also helpful to image generation tasks in terms of training stabilization and sample quality. Without modifying the baseline models, we achieve better quality than state-of-the-art on FFHQ and ImageNet generation using either GANs or diffusion models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Towards_Authentic_Face_Restoration_with_Iterative_Diffusion_Models_and_Beyond_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhao_Towards_Authentic_Face_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yang Zhao</author><author>Tingbo Hou</author><author>Yu-Chuan Su</author><author>Xuhui Jia</author><author>Yandong Li</author><author>Matthias Grundmann</author>
            </authors>
        </paper>
        

        <paper>
            <title>Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment</title>
            <abstract>Detection transformer (DETR) relies on one-to-one assignment, assigning one ground-truth object to one prediction, for end-to-end detection without NMS post-processing. It is known that one-to-many assignment, assigning one ground-truth object to multiple predictions, succeeds in detection methods such as Faster R-CNN and FCOS. While the naive one-to-many assignment does not work for DETR, and it remains challenging to apply one-to-many assignment for DETR training. In this paper, we introduce Group DETR, a simple yet efficient DETR training approach that introduces a group-wise way for one-to-many assignment. This approach involves using multiple groups of object queries, conducting one-to-one assignment within each group, and performing decoder self-attention separately. It resembles data augmentation with automatically-learned object query augmentation. It is also equivalent to simultaneously training parameter-sharing networks of the same architecture, introducing more supervision and thus improving DETR training. The inference process is the same as DETR trained normally and only needs one group of queries without any architecture modification. Group DETR is versatile and is applicable to various DETR variants. The experiments show that Group DETR significantly speeds up the training convergence and improves the performance of various DETR-based models. Code will be available at https://github.com/Atten4Vis/GroupDETR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Group_DETR_Fast_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiang Chen</author><author>Xiaokang Chen</author><author>Jian Wang</author><author>Shan Zhang</author><author>Kun Yao</author><author>Haocheng Feng</author><author>Junyu Han</author><author>Errui Ding</author><author>Gang Zeng</author><author>Jingdong Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DETRs with Collaborative Hybrid Assignments Training</title>
            <abstract>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervision on the encoder&apos;s output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder&apos;s learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on COCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at https://github.com/Sense-X/Co-DETR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zong_DETRs_with_Collaborative_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Zhuofan Zong</author><author>Guanglu Song</author><author>Yu Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor</title>
            <abstract>Light-weight time-of-flight (ToF) depth sensors are compact and cost-efficient, and thus widely used on mobile devices for tasks such as autofocus and obstacle detection. However, due to the sparse and noisy depth measurements, these sensors have rarely been considered for dense geometry reconstruction. In this work, we present the first dense SLAM system with a monocular camera and a light-weight ToF sensor. Specifically, we propose a multi-modal implicit scene representation that supports rendering both the signals from the RGB camera and light-weight ToF sensor which drives the optimization by comparing with the raw sensor inputs. Moreover, in order to guarantee successful pose tracking and reconstruction, we exploit a predicted depth as an intermediate supervision and develop a coarse-to-fine optimization strategy for efficient learning of the implicit representation. At last, the temporal information is explicitly exploited to deal with the noisy signals from light-weight ToF sensors to improve the accuracy and robustness of the system. Experiments demonstrate that our system well exploits the signals of light-weight ToF sensors and achieves competitive results both on camera tracking and dense scene reconstruction. Project page: https://zju3dv.github.io/tof_slam/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Multi-Modal_Neural_Radiance_Field_for_Monocular_Dense_SLAM_with_a_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Multi-Modal_Neural_Radiance_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Xinyang Liu</author><author>Yijin Li</author><author>Yanbin Teng</author><author>Hujun Bao</author><author>Guofeng Zhang</author><author>Yinda Zhang</author><author>Zhaopeng Cui</author>
            </authors>
        </paper>
        

        <paper>
            <title>MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection</title>
            <abstract>In the field of monocular 3D detection, it is common practice to utilize scene geometric clues to enhance the detector&apos;s performance. However, many existing works adopt these clues explicitly such as estimating a depth map and back-projecting it into 3D space. This explicit methodology induces sparsity in 3D representations due to the increased dimensionality from 2D to 3D, and leads to substantial information loss, especially for distant and occluded objects. To alleviate this issue, we propose MonoNeRD, a novel detection framework that can infer dense 3D geometry and occupancy. Specifically, we model scenes with Signed Distance Functions (SDF), facilitating the production of dense 3D representations. We treat these representations as Neural Radiance Fields (NeRF) and then employ volume rendering to recover RGB images and depth maps. To the best of our knowledge, this work is the first to introduce volume rendering for M3D, and demonstrates the potential of implicit reconstruction for image-based 3D perception. Extensive experiments conducted on the KITTI-3D benchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD. Codes are available at https://github.com/cskkxjk/MonoNeRD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_MonoNeRD_NeRF-like_Representations_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_MonoNeRD_NeRF-like_Representations_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junkai Xu</author><author>Liang Peng</author><author>Haoran Cheng</author><author>Hao Li</author><author>Wei Qian</author><author>Ke Li</author><author>Wenxiao Wang</author><author>Deng Cai</author>
            </authors>
        </paper>
        

        <paper>
            <title>Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver</title>
            <abstract>The main challenge of monocular 3D object detection is the accurate localization of 3D center. Motivated by a new and strong observation that this challenge can be remedied by a 3D-space local-grid search scheme in an ideal case, we propose a stage-wise approach, which combines the information flow from 2D-to-3D (3D bounding box proposal generation with a single 2D image) and 3D-to-2D (proposal verification by denoising with 3D-to-2D contexts) in a top-down manner. Specifically, we first obtain initial proposals from off-the-shelf backbone monocular 3D detectors. Then, we generate a 3D anchor space by local-grid sampling from the initial proposals. Finally, we perform 3D bounding box denoising at the 3D-to-2D proposal verification stage. To effectively learn discriminative features for denoising highly overlapped proposals, this paper presents a method of using the Perceiver I/O model to fuse the 3D-to-2D geometric information and the 2D appearance information. With the encoded latent representation of a proposal, the verification head is implemented with a self-attention module. Our method, named as MonoXiver, is generic and can be easily adapted to any backbone monocular 3D detectors. Experimental results on the well-established KITTI dataset and the challenging large-scale Waymo dataset show that MonoXiver consistently achieves improvement with limited computation overhead.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Monocular_3D_Object_Detection_with_Bounding_Box_Denoising_in_3D_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Monocular_3D_Object_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xianpeng Liu</author><author>Ce Zheng</author><author>Kelvin B Cheng</author><author>Nan Xue</author><author>Guo-Jun Qi</author><author>Tianfu Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>WaveIPT: Joint Attention and Flow Alignment in the Wavelet domain for Pose Transfer</title>
            <abstract>Human pose transfer aims to generate a new image of the source person in a target pose. Among the existing methods, attention and flow are two of the most popular and effective approaches. Attention excels in preserving the semantic structure of the source image, which is more reflected in the low-frequency domain. Contrastively, flow is better at retaining fine-grained texture details in the high-frequency domain. To leverage the advantages of both attention and flow simultaneously, we propose Wavelet-aware Image-based Pose Transfer (WaveIPT) to fuse the attention and flow in the wavelet domain. To improve the fusion effect and avoid interference from irrelevant information between different frequencies, WaveIPT first applies Intra-scale Local Correlation (ILC) to adaptively fuse attention and flow in the same scale according to their strengths in low and high-frequency domains, and then uses Inter-scale Feature Interaction (IFI) module to explore inter-scale frequency features for effective information transfer across different scales. We further introduce an effective Progressive Flow Regularization to alleviate the challenges of flow estimation under large pose differences. Our experiments on the DeepFashion dataset demonstrate that WaveIPT achieves a new state-of-the-art in terms of FID and LPIPS, with improvements of 4.97% and 3.89%, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ma_WaveIPT_Joint_Attention_and_Flow_Alignment_in_the_Wavelet_domain_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ma_WaveIPT_Joint_Attention_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Liyuan Ma</author><author>Tingwei Gao</author><author>Haitian Jiang</author><author>Haibin Shen</author><author>Kejie Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection</title>
            <abstract>Recently, polar-based representation has shown promising properties in perceptual tasks. In addition to Cartesian-based approaches, which separate point clouds unevenly, representing point clouds as polar grids has been recognized as an alternative due to (1) its advantage in robust performance under different resolutions and (2) its superiority in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Extensive experiments show overwhelming advantages in streaming-based detection and different resolutions. Furthermore, our method outperforms the previous polar-based works with remarkable margins of 3.68% and 9.15% on Waymo and ONCE validation set, thus achieving competitive results over the state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Nie_PARTNER_Level_up_the_Polar_Representation_for_LiDAR_3D_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ming Nie</author><author>Yujing Xue</author><author>Chunwei Wang</author><author>Chaoqiang Ye</author><author>Hang Xu</author><author>Xinge Zhu</author><author>Qingqiu Huang</author><author>Michael Bi Mi</author><author>Xinchao Wang</author><author>Li Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Corrupting Neuron Explanations of Deep Visual Features</title>
            <abstract>The inability of DNNs to explain their black-box behavior has led to a recent surge of explainability methods. However, there are growing concerns that these explainability methods are not robust and trustworthy. In this work, we perform the first robustness analysis of Neuron Explanation Methods under a unified pipeline and show that these explanations can be significantly corrupted by random noises and well-designed perturbations added to their probing data. We find that even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers. Furthermore, we devise a novel corruption algorithm and show that our algorithm can manipulate the explanation of more than 80% neurons by poisoning less than 10% of probing data. This raises the concern of trusting Neuron Explanation Methods in real-life safety and fairness critical applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Srivastava_Corrupting_Neuron_Explanations_of_Deep_Visual_Features_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Srivastava_Corrupting_Neuron_Explanations_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Divyansh Srivastava</author><author>Tuomas Oikarinen</author><author>Tsui-Wei Weng</author>
            </authors>
        </paper>
        

        <paper>
            <title>PNI : Industrial Anomaly Detection using Position and Neighborhood Information</title>
            <abstract>Because anomalous samples cannot be used for training, many anomaly detection and localization methods use pre-trained networks and non-parametric modeling to estimate encoded feature distribution. However, these methods neglect the impact of position and neighborhood information on the distribution of normal features. To overcome this, we propose a new algorithm, PNI, which estimates the normal distribution using conditional probability given neighborhood features, modeled with a multi-layer perceptron network. Moreover, position information is utilized by creating a histogram of representative features at each position. Instead of simply resizing the anomaly map, the proposed method employs an additional refine network trained on synthetic anomaly images to better interpolate and account for the shape and edge of the input image. We conducted experiments on the MVTec AD benchmark dataset and achieved state-of-the-art performance, with 99.56% and 98.98% AUROC scores in anomaly detection and localization, respectively. Code is available at https://github.com/wogur110/PNI_Anomaly_Detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bae_PNI__Industrial_Anomaly_Detection_using_Position_and_Neighborhood_Information_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bae_PNI__Industrial_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jaehyeok Bae</author><author>Jae-Han Lee</author><author>Seyun Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer</title>
            <abstract>Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propagation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superiority over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_Bidirectionally_Deformable_Motion_Modulation_For_Video-based_Human_Pose_Transfer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yu_Bidirectionally_Deformable_Motion_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Wing-Yin Yu</author><author>Lai-Man Po</author><author>Ray C.C. Cheung</author><author>Yuzhi Zhao</author><author>Yu Xue</author><author>Kun Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Objects Do Not Disappear: Video Object Detection by Single-Frame Object Location Anticipation</title>
            <abstract>Objects in videos are typically characterized by continuous smooth motion. We exploit continuous smooth motion in three ways. 1) Improved accuracy by using object motion as an additional source of supervision, which we obtain by anticipating object locations from a static keyframe. 2) Improved efficiency by only doing the expensive feature computations on a small subset of all frames. Because neighboring video frames are often redundant, we only compute features for a single static keyframe and predict object locations in subsequent frames. 3) Reduced annotation cost, where we only annotate the keyframe and use smooth pseudo-motion between keyframes. We demonstrate computational efficiency, annotation efficiency, and improved mean average precision compared to the state-of-the-art on four datasets: ImageNet VID, EPIC KITCHENS-55, YouTube-BoundingBoxes and Waymo Open dataset. Our source code is available at https://github.com/L-KID/Videoobject-detection-by-location-anticipation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Objects_Do_Not_Disappear_Video_Object_Detection_by_Single-Frame_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Objects_Do_Not_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xin Liu</author><author>Fatemeh Karimi Nejadasl</author><author>Jan C. van Gemert</author><author>Olaf Booij</author><author>Silvia L. Pintea</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition</title>
            <abstract>We are concerned with a challenging scenario in unpaired multiview video learning. In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations. We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem. The key idea is to build cross-view pseudo-pairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning. Our code is available at https://github.com/wqtwjt1996/SUM-L.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Learning_from_Semantic_Alignment_between_Unpaired_Multiviews_for_Egocentric_Video_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Qitong Wang</author><author>Long Zhao</author><author>Liangzhe Yuan</author><author>Ting Liu</author><author>Xi Peng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Source-free Depth for Object Pop-out</title>
            <abstract>Depth cues are known to be useful for visual perception. However, direct measurement of depth is often impracticable. Fortunately, though, modern learning-based methods offer promising depth maps by inference in the wild. In this work, we adapt such depth inference models for object segmentation using the objects&apos; &quot;pop-out&quot; prior in 3D. The &quot;pop-out&quot; is a simple composition prior that assumes objects reside on the background surface. Such compositional prior allows us to reason about objects in the 3D space. More specifically, we adapt the inferred depth maps such that objects can be localized using only 3D information. Such separation, however, requires knowledge about contact surface which we learn using the weak supervision of the segmentation mask. Our intermediate representation of contact surface, and thereby reasoning about objects purely in 3D, allows us to better transfer the depth knowledge into semantics. The proposed adaptation method uses only the depth model without needing the source data used for training, making the learning process efficient and practical. Our experiments on eight datasets of two challenging tasks, namely salient object detection and camouflaged object detection, consistently demonstrate the benefit of our method in terms of both performance and generalizability. The source code is publicly available at https://github.com/Zongwei97/PopNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/WU_Source-free_Depth_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zongwei WU</author><author>Danda Pani Paudel</author><author>Deng-Ping Fan</author><author>Jingjing Wang</author><author>Shuo Wang</author><author>C dric Demonceaux</author><author>Radu Timofte</author><author>Luc Van Gool</author>
            </authors>
        </paper>
        

        <paper>
            <title>Token-Label Alignment for Vision Transformers</title>
            <abstract>Data mixing strategies (e.g., CutMix) have shown the ability to greatly improve the performance of convolutional neural networks (CNNs). They mix two images as inputs for training and assign them with a mixed label with the same ratio. While they are shown effective for vision transformers (ViTs), we identify a token fluctuation phenomenon that has suppressed the potential of data mixing strategies. We empirically observe that the contributions of input tokens fluctuate as forward propagating, which might induce a different mixing ratio in the output tokens. The training target computed by the original data mixing strategy can thus be inaccurate, resulting in less effective training. To address this, we propose a token-label alignment (TL-Align) method to trace the correspondence between transformed tokens and the original tokens to maintain a label for each token. We reuse the computed attention at each layer for efficient token-label alignment, introducing only negligible additional training costs. Extensive experiments demonstrate that our method improves the performance of ViTs on image classification, semantic segmentation, objective detection, and transfer learning tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xiao_Token-Label_Alignment_for_Vision_Transformers_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xiao_Token-Label_Alignment_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Han Xiao</author><author>Wenzhao Zheng</author><author>Zheng Zhu</author><author>Jie Zhou</author><author>Jiwen Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Gabor Texture Features for Fine-Grained Recognition</title>
            <abstract>Extracting and using class-discriminative features is critical for fine-grained recognition. Existing works have demonstrated the possibility of applying deep CNNs to exploit features that distinguish similar classes. However, CNNs suffer from problems including frequency bias and loss of detailed local information, which restricts the performance of recognizing fine-grained categories. To address the challenge, we propose a novel texture branch as complimentary to the CNN branch for feature extraction. We innovatively utilize Gabor filters as a powerful extractor to exploit texture features, motivated by the capability of Gabor filters in effectively capturing multi-frequency features and detailed local information. We implement several designs to enhance the effectiveness of Gabor filters, including imposing constraints on parameter values and developing a learning method to determine the optimal parameters. Moreover, we introduce a statistical feature extractor to utilize informative statistical information from the signals captured by Gabor filters, and a gate selection mechanism to enable efficient computation by only considering qualified regions as input for texture extraction. Through the integration of features from the Gabor-filter-based texture branch and CNN-based semantic branch, we achieve comprehensive information extraction. We demonstrate the efficacy of our method on multiple datasets, including CUB-200-2011, NA-bird, Stanford Dogs, and GTOS-mobile. State-of-the-art performance is achieved using our approach.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Learning_Gabor_Texture_Features_for_Fine-Grained_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Learning_Gabor_Texture_Features_for_Fine-Grained_Recognition_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lanyun Zhu</author><author>Tianrun Chen</author><author>Jianxiong Yin</author><author>Simon See</author><author>Jun Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>An Embarrassingly Simple Backdoor Attack on Self-supervised Learning</title>
            <abstract>As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question. We explore this question in the context of backdoor attacks. Specifically, we design and evaluate CTRL, an embarrassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (&lt;1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the adversary&apos;s designated class with a high probability (&gt;99%) at inference time. Our findings suggest that SSL and supervised learning are comparably vulnerable to backdoor attacks. More importantly, through the lens of CTRL, we study the inherent vulnerability of SSL to backdoor attacks. With both empirical and analytical evidence, we reveal that the representation invariance property of SSL, which benefits adversarial robustness, may also be the very reason making SSL highly susceptible to backdoor attacks. Our findings also imply that the existing defenses against supervised backdoor attacks are not easily retrofitted to the unique vulnerability of SSL.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_An_Embarrassingly_Simple_Backdoor_Attack_on_Self-supervised_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_An_Embarrassingly_Simple_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Changjiang Li</author><author>Ren Pang</author><author>Zhaohan Xi</author><author>Tianyu Du</author><author>Shouling Ji</author><author>Yuan Yao</author><author>Ting Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Partition Speeds Up Learning Implicit Neural Representations Based on Exponential-Increase Hypothesis</title>
            <abstract>Implicit neural representations (INRs) aim to learn a continuous function (i.e., a neural network) to represent an image, where the input and output of the function are pixel coordinates and RGB/Gray values, respectively. However, images tend to consist of many objects whose colors are not perfectly consistent, resulting in the challenge that image is actually a discontinuous piecewise function and cannot be well estimated by a continuous function. In this paper, we empirically investigate that if a neural network is enforced to fit a discontinuous piecewise function to reach a fixed small error, the time costs will increase exponentially with respect to the boundaries in the spatial domain of the target signal. We name this phenomenon the exponential-increase hypothesis. Under the exponential-increase hypothesis, learning INRs for images with many objects will converge very slowly. To address this issue, we first prove that partitioning a complex signal into several sub-regions and utilizing piecewise INRs to fit that signal can significantly speed up the convergence. Based on this fact, we introduce a simple partition mechanism to boost the performance of two INR methods for image reconstruction: one for learning INRs, and the other for learning-to-learn INRs. In both cases, we partition an image into different sub-regions and dedicate smaller networks for each part. In addition, we further propose two partition rules based on regular grids and semantic segmentation maps, respectively. Extensive experiments validate the effectiveness of the proposed partitioning methods in terms of learning INR for a single image (ordinary learning framework) and the learning-to-learn framework.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Partition_Speeds_Up_Learning_Implicit_Neural_Representations_Based_on_Exponential-Increase_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Partition_Speeds_Up_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ke Liu</author><author>Feng Liu</author><author>Haishuai Wang</author><author>Ning Ma</author><author>Jiajun Bu</author><author>Bo Han</author>
            </authors>
        </paper>
        

        <paper>
            <title>Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching</title>
            <abstract>Correlation based stereo matching has achieved outstanding performance, which pursues cost volume between two feature maps. Unfortunately, current methods with a fixed trained model do not work uniformly well across various datasets, greatly limiting their real-world applicability. To tackle this issue, this paper proposes a new perspective to dynamically calculate correlation for robust stereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module is introduced to robustly adapt the same model for different scenarios. Specifically, a variance-based uncertainty estimation is employed to adaptively adjust the sampling area during warping operation. Additionally, we improve the traditional non-parametric warping with learnable parameters, such that the position-specific weights can be learned. We show that by empowering the recurrent network with the UGAC module, stereo matching can be exploited more robustly and effectively. Extensive experiments demonstrate that our method achieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury datasets when employing the same fixed model over these datasets without any retraining procedure. To target real-time applications, we further design a lightweight model based on UGAC, which also outperforms other methods over KITTI benchmarks with only 0.6 M parameters.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jing_Uncertainty_Guided_Adaptive_Warping_for_Robust_and_Efficient_Stereo_Matching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jing_Uncertainty_Guided_Adaptive_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junpeng Jing</author><author>Jiankun Li</author><author>Pengfei Xiong</author><author>Jiangyu Liu</author><author>Shuaicheng Liu</author><author>Yichen Guo</author><author>Xin Deng</author><author>Mai Xu</author><author>Lai Jiang</author><author>Leonid Sigal</author>
            </authors>
        </paper>
        

        <paper>
            <title>CGBA: Curvature-aware Geometric Black-box Attack</title>
            <abstract>Decision-based black-box attacks often necessitate a large number of queries to craft an adversarial example. Moreover, decision-based attacks based on querying boundary points in the estimated normal vector direction often suffer from inefficiency and convergence issues. In this paper, we propose a novel query-efficient curvature-aware geometric decision-based black-box attack (CGBA) that conducts boundary search along a semicircular path on a restricted 2D plane to ensure finding a boundary point successfully irrespective of the boundary curvature. While the proposed CGBA attack can work effectively for an arbitrary decision boundary, it is particularly efficient in exploiting the low curvature to craft high-quality adversarial examples, which is widely seen and experimentally verified in commonly used classifiers under non-targeted attacks. In contrast, the decision boundaries often exhibit higher curvature under targeted attacks. Thus, we develop a new query-efficient variant, CGBA-H, that is adapted for the targeted attack. In addition, we further design an algorithm to obtain a better initial boundary point at the expense of some extra queries, which considerably enhances the performance of the targeted attack. Extensive experiments are conducted to evaluate the performance of our proposed methods against some well-known classifiers on the ImageNet and CIFAR10 datasets, demonstrating the superiority of CGBA and CGBA-H over state-of-the-art non-targeted and targeted attacks, respectively. The source code is available at https://github.com/Farhamdur/CGBA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Reza_CGBA_Curvature-aware_Geometric_Black-box_Attack_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Reza_CGBA_Curvature-aware_Geometric_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Md Farhamdur Reza</author><author>Ali Rahmati</author><author>Tianfu Wu</author><author>Huaiyu Dai</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations</title>
            <abstract>High-fidelity virtual human avatar applications create a need for photorealistic video face synthesis with controllable semantic editing over facial features. While recent generative neural methods have shown significant progress in portrait video synthesis, intuitive facial control, e.g., of mouth interior and gaze at different levels of details, remains a challenge. In this work, we present a novel face editing framework that combines a 3D face model with StyleGAN vector-quantization to learn multi-level semantic facial control. We show that vector quantization of StyleGAN features unveils richer semantic facial representations, e.g., teeth and pupils, which are difficult to model with 3D tracking priors. Such representations along with 3D tracking can be used as self-supervision to train a generator with control over coarse expressions and finer facial attributes. Learned representations can be combined with user-defined masks to create semantic segmentations that act as custom detail handles for semantic-aware video editing. Our formulation allows video face manipulation with precise local control over facial attributes, such as eyes and teeth, opening up a number of face reenactment and visual expression articulation applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kicanaoglu_Unsupervised_Facial_Performance_Editing_via_Vector-Quantized_StyleGAN_Representations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kicanaoglu_Unsupervised_Facial_Performance_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Berkay Kicanaoglu</author><author>Pablo Garrido</author><author>Gaurav Bharaj</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Multidimensional Analysis of Social Biases in Vision Transformers</title>
            <abstract>The embedding spaces of image models have been shown to encode a range of social biases such as racism and sexism. Here, we investigate specific factors that contribute to the emergence of these biases in Vision Transformers (ViT). Therefore, we measure the impact of training data, model architecture, and training objectives on social biases in the learned representations of ViTs. Our findings indicate that counterfactual augmentation training using diffusion-based image editing can mitigate biases, but does not eliminate them. Moreover, we find that larger models are less biased than smaller models, and that models trained using discriminative objectives are less biased than those trained using generative objectives. In addition, we observe inconsistencies in the learned social biases. To our surprise, ViTs can exhibit opposite biases when trained on the same data set using different self-supervised objectives. Our findings give insights into the factors that contribute to the emergence of social biases and suggests that we could achieve substantial fairness improvements based on model design choices.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Brinkmann_A_Multidimensional_Analysis_of_Social_Biases_in_Vision_Transformers_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jannik Brinkmann</author><author>Paul Swoboda</author><author>Christian Bartelt</author>
            </authors>
        </paper>
        

        <paper>
            <title>PGFed: Personalize Each Client&apos;s Global Objective for Federated Learning</title>
            <abstract>Personalized federated learning has received an upsurge of attention due to the mediocre performance of conventional federated learning (FL) over heterogeneous data. Unlike conventional FL which trains a single global consensus model, personalized FL allows different models for different clients. However, existing personalized FL algorithms only implicitly transfer the collaborative knowledge across the federation by embedding the knowledge into the aggregated model or regularization. We observed that this implicit knowledge transfer fails to maximize the potential of each client&apos;s empirical risk toward other clients. Based on our observation, in this work, we propose Personalized Global Federated Learning (PGFed), a novel personalized FL framework that enables each client to personalize its own global objective by explicitly and adaptively aggregating the empirical risks of itself and other clients. To avoid massive (O(N^2)) communication overhead and potential privacy leakage while achieving this, each client&apos;s risk is estimated through a first-order approximation for other clients&apos; adaptive risk aggregation. On top of PGFed, we develop a momentum upgrade, dubbed PGFedMo, to more efficiently utilize clients&apos; empirical risks. Our extensive experiments on four datasets under different federated settings show consistent improvements of PGFed over previous state-of-the-art methods. The code is publicly available at https://github.com/ljaiverson/pgfed.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Luo_PGFed_Personalize_Each_Clients_Global_Objective_for_Federated_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Luo_PGFed_Personalize_Each_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jun Luo</author><author>Matias Mendieta</author><author>Chen Chen</author><author>Shandong Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Instance and Category Supervision are Alternate Learners for Continual Learning</title>
            <abstract>Continual Learning (CL) is the constant development of complex behaviors by building upon previously acquired skills. Yet, current CL algorithms tend to incur class-level forgetting as the label information is often quickly overwritten by new knowledge. This motivates attempts to mine instance-level discrimination by resorting to recent self-supervised learning (SSL) techniques. However, previous works have pointed that the self-supervised learning objective is essentially a trade-off between invariance to distortion and preserving sample information, which seriously hinders the unleashing of instance-level discrimination. In this work, we reformulate SSL from the information-theoretic perspective by disentangling the goal of instance-level discrimination, and tackle the trade-off to promote compact representations with maximally preserved invariance to distortion. On this basis, we develop a novel alternate learning paradigm to enjoy the complementary merits of instance-level and category-level supervision, which yields improved robustness against forgetting and better adaptation to each task. To verify the proposed method, we conduct extensive experiments on four different benchmarks using both class-incremental and task-incremental settings, where the leap in performance and thorough ablation studies demonstrate the efficacy and efficiency of our modeling strategy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tian_Instance_and_Category_Supervision_are_Alternate_Learners_for_Continual_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tian_Instance_and_Category_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xudong Tian</author><author>Zhizhong Zhang</author><author>Xin Tan</author><author>Jun Liu</author><author>Chengjie Wang</author><author>Yanyun Qu</author><author>Guannan Jiang</author><author>Yuan Xie</author>
            </authors>
        </paper>
        

        <paper>
            <title>Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning</title>
            <abstract>Benefiting from prompt tuning, recent years have witnessed the promising performance of pre-trained vision-language models, e.g., CLIP, on versatile downstream tasks. In this paper, we focus on a particular setting of learning adaptive prompts on the fly for each test sample from an unseen new domain, which is known as test-time prompt tuning (TPT). Existing TPT methods typically rely on data augmentation and confidence selection. However, conventional data augmentation techniques, e.g., random resized crops, suffers from the lack of data diversity, while entropy-based confidence selection alone is not sufficient to guarantee prediction fidelity. To address these issues, we propose a novel TPT method, named DiffTPT, which leverages pre-trained diffusion models to generate diverse and informative new data. Specifically, we incorporate augmented data by both conventional method and pre-trained stable diffusion to exploit their respective merits, improving the model&apos;s ability to adapt to unknown new test data. Moreover, to ensure the prediction fidelity of generated data, we introduce a cosine similarity-based filtration technique to select the generated data with higher similarity to the single test sample. Our experiments on test datasets with distribution shifts and unseen categories demonstrate that DiffTPT improves the zero-shot accuracy by an average of 5.13% compared to the state-of-the-art TPT method.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Feng_Diverse_Data_Augmentation_with_Diffusions_for_Effective_Test-time_Prompt_Tuning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Chun-Mei Feng</author><author>Kai Yu</author><author>Yong Liu</author><author>Salman Khan</author><author>Wangmeng Zuo</author>
            </authors>
        </paper>
        

        <paper>
            <title>GePSAn: Generative Procedure Step Anticipation in Cooking Videos</title>
            <abstract>We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our experiments, both in textual and video domains, show that our model captures diversity in the next step prediction and generates multiple plausible future predictions. Moreover, our model establishes new state-of-the-art results on YouCookII, where it outperforms existing baselines on the next step anticipation. Finally, we also show that our model can successfully transfer from text to the video domain zero-shot, ie, without fine-tuning or adaptation, and produces good-quality future step predictions from video.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Abdelsalam_GePSAn_Generative_Procedure_Step_Anticipation_in_Cooking_Videos_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Abdelsalam_GePSAn_Generative_Procedure_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Mohamed A. Abdelsalam</author><author>Samrudhdhi B. Rangrej</author><author>Isma Hadji</author><author>Nikita Dvornik</author><author>Konstantinos G. Derpanis</author><author>Afsaneh Fazly</author>
            </authors>
        </paper>
        

        <paper>
            <title>AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration</title>
            <abstract>Diffusion models are emerging expressive generative models, in which a large number of time steps (inference steps) are required for a single image generation. To accelerate such tedious process, reducing steps uniformly is considered as an undisputed principle of diffusion models. We consider that such a uniform assumption is not the optimal solution in practice; i.e., we can find different optimal time steps for different models. Therefore, we propose to search the optimal time steps sequence and compressed model architecture in a unified framework to achieve effective image generation for diffusion models without any further training. Specifically, we first design a unified search space that consists of all possible time steps and various architectures. Then, a two stage evolutionary algorithm is introduced to find the optimal solution in the designed search space. To further accelerate the search process, we employ FID score between generated and real samples to estimate the performance of the sampled examples. As a result, the proposed method is (i).training-free, obtaining the optimal time steps and model architecture without any training process; (ii). orthogonal to most advanced diffusion samplers and can be integrated to gain better sample quality. (iii). generalized, where the searched time steps and architectures can be directly applied on different diffusion models with the same guidance scale. Experimental results show that our method achieves excellent performance by using only a few time steps, e.g. 17.86 FID score on ImageNet 64 x 64 with only four steps, compared to 138.66 with DDIM.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_AutoDiffusion_Training-Free_Optimization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lijiang Li</author><author>Huixia Li</author><author>Xiawu Zheng</author><author>Jie Wu</author><author>Xuefeng Xiao</author><author>Rui Wang</author><author>Min Zheng</author><author>Xin Pan</author><author>Fei Chao</author><author>Rongrong Ji</author>
            </authors>
        </paper>
        

        <paper>
            <title>DPS-Net: Deep Polarimetric Stereo Depth Estimation</title>
            <abstract>Stereo depth estimation usually struggles to deal with textureless scenes for both traditional and learning-based methods due to the inherent dependence on image correspondence matching. In this paper, we propose a novel neural network, i.e., DPS-Net, to exploit both the prior geometric knowledge and polarimetric information for depth estimation with two polarimetric stereo images. Specifically, we construct both RGB and polarization correlation volumes to fully leverage the multi-domain similarity between polarimetric stereo images. Since inherent ambiguities exist in the polarization images, we introduce the iso-depth cost explicitly into the network to solve these ambiguities. Moreover, we design a cascaded dual-GRU architecture to recurrently update the disparity and effectively fuse both the multi-domain correlation features and the iso-depth cost. Besides, we present new synthetic and real polarimetric stereo datasets for evaluation. Experimental results demonstrate that our method outperforms the state-of-the-art stereo depth estimation methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tian_DPS-Net_Deep_Polarimetric_Stereo_Depth_Estimation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tian_DPS-Net_Deep_Polarimetric_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chaoran Tian</author><author>Weihong Pan</author><author>Zimo Wang</author><author>Mao Mao</author><author>Guofeng Zhang</author><author>Hujun Bao</author><author>Ping Tan</author><author>Zhaopeng Cui</author>
            </authors>
        </paper>
        

        <paper>
            <title>SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference</title>
            <abstract>The combination of Neural Architecture Search (NAS) and quantization has proven successful in automatically designing low-FLOPs INT8 quantized neural networks (QNN). However, directly applying NAS to design accurate QNN models that achieve low latency on real-world devices leads to inferior performance. In this work, we identify that the poor INT8 latency is due to the quantization-unfriendly issue: the operator and configuration (e.g., channel width) choices in prior art search spaces lead to diverse quantization efficiency and can slow down the INT8 inference speed. To address this challenge, we propose SpaceEvo, an automatic method for designing a dedicated, quantization-friendly search space for each target hardware. The key idea of SpaceEvo is to automatically search hardware-preferred operators and configurations to construct the search space, guided by a metric called Q-T score to quantify how quantization-friendly a candidate search space is. We further train a quantized-for-all supernet over our discovered search space, enabling the searched models to be directly deployed without extra retraining or quantization. Our discovered models, SEQnet, establish new SOTA INT8 quantized accuracy under various latency constraints, achieving up to 10.1% accuracy improvement on ImageNet than prior art CNNs under the same latency. Extensive experiments on real devices show that SpaceEvo consistently outperforms manually-designed search spaces with up to 2.5x faster speed while achieving the same accuracy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_SpaceEvo_Hardware-Friendly_Search_Space_Design_for_Efficient_INT8_Inference_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_SpaceEvo_Hardware-Friendly_Search_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xudong Wang</author><author>Li Lyna Zhang</author><author>Jiahang Xu</author><author>Quanlu Zhang</author><author>Yujing Wang</author><author>Yuqing Yang</author><author>Ningxin Zheng</author><author>Ting Cao</author><author>Mao Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>How Far Pre-trained Models Are from Neural Collapse on the Target Dataset Informs their Transferability</title>
            <abstract>This paper focuses on model transferability estimation, i.e., assessing the performance of pre-trained models on a downstream task without performing fine-tuning. Motivated by the neural collapse (NC) that reveals the feature geometry at the terminal stage of training, our method considers the model transferability as how far the target activations obtained by pre-trained models are from their hypothetical state in the terminal phase of the fine-tuned model. We propose a metric that computes this proximity based on three phenomena of NC: within-class variability collapse, simplex encoded label interpolation geometry structure is formed, and the nearest center classifier becomes optimal on training data. Extensive experiments on 11 benchmark datasets demonstrate the effectiveness and efficiency of the proposed method over the existing SOTA approaches. Particularly, our method achieves SOTA transferability estimation accuracy with approximately 10xwall-clock time speed up compared to the existing approaches</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zijian Wang</author><author>Yadan Luo</author><author>Liang Zheng</author><author>Zi Huang</author><author>Mahsa Baktashmotlagh</author>
            </authors>
        </paper>
        

        <paper>
            <title>Convolutional Networks with Oriented 1D Kernels</title>
            <abstract>In computer vision, 2D convolution is arguably the most important operation performed by a ConvNet. Unsurprisingly, it has been the focus of intense software and hardware optimization and enjoys highly efficient implementations. In this work, we ask an intriguing question: can we make a ConvNet work without 2D convolutions? Surprisingly, we find that the answer is yes --- we show that a ConvNet consisting entirely of 1D convolutions can do just as well as 2D on ImageNet classification. Specifically, we find that one key ingredient to a high-performing 1D ConvNet is oriented 1D kernels: 1D kernels that are oriented not just horizontally or vertically, but also at other angles. Our experiments show that oriented 1D convolutions can not only replace 2D convolutions but also augment existing architectures with large kernels, leading to improved accuracy with minimal FLOPs increase. A key contribution of this work is a highly-optimized custom CUDA implementation of oriented 1D kernels, specialized to the depthwise convolution setting. Our benchmarks demonstrate that our custom CUDA implementation almost perfectly realizes the theoretical advantage of 1D convolution: it is faster than a native horizontal convolution for any arbitrary angle. Code is available at https://github.com/princeton-vl/Oriented1D.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kirchmeyer_Convolutional_Networks_with_Oriented_1D_Kernels_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kirchmeyer_Convolutional_Networks_with_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Alexandre Kirchmeyer</author><author>Jia Deng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Improving Pixel-based MIM by Reducing Wasted Modeling Capability</title>
            <abstract>There has been significant progress in Masked Image Modeling (MIM). Existing MIM methods can be broadly categorized into two groups based on the reconstruction target: pixel-based and tokenizer-based approaches. The former offers a simpler pipeline and lower computational cost, but it is known to be biased toward high-frequency details. In this paper, we provide a set of empirical studies to confirm this limitation of pixel-based MIM and propose a new method that explicitly utilizes low-level features from shallow layers to aid pixel reconstruction. By incorporating this design into our base method, MAE, we reduce the wasted modeling capability of pixel-based MIM, improving its convergence and achieving non-trivial improvements across various downstream tasks. To the best of our knowledge, we are the first to systematically investigate multi-level feature fusion for isotropic architectures like the standard Vision Transformer (ViT). Notably, when applied to a smaller model (e.g., ViT-S), our method yields significant performance gains, such as 1.2% on fine-tuning, 2.8% on linear probing, and 2.6% on semantic segmentation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Improving_Pixel-based_MIM_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuan Liu</author><author>Songyang Zhang</author><author>Jiacheng Chen</author><author>Zhaohui Yu</author><author>Kai Chen</author><author>Dahua Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks</title>
            <abstract>Spiking Neural Networks (SNNs) are promising energy-efficient models for neuromorphic computing. For training the non-differentiable SNN models, the backpropagation through time (BPTT) with surrogate gradients (SG) method has achieved high performance. However, this method suffers from considerable memory cost and training time during training. In this paper, we propose the Spatial Learning Through Time (SLTT) method that can achieve high performance while greatly improving training efficiency compared with BPTT. First, we show that the backpropagation of SNNs through the temporal domain contributes just a little to the final calculated gradients. Thus, we propose to ignore the unimportant routes in the computational graph during backpropagation. The proposed method reduces the number of scalar multiplications and achieves a small memory occupation that is independent of the total time steps. Furthermore, we propose a variant of SLTT, called SLTT-K, that allows backpropagation only at K time steps, then the required number of scalar multiplications is further reduced and is independent of the total time steps. Experiments on both static and neuromorphic datasets demonstrate superior training efficiency and performance of our SLTT. In particular, our method achieves state-of-the-art accuracy on ImageNet, while the memory cost and training time are reduced by more than 70% and 50%, respectively, compared with BPTT.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Meng_Towards_Memory-_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qingyan Meng</author><author>Mingqing Xiao</author><author>Shen Yan</author><author>Yisen Wang</author><author>Zhouchen Lin</author><author>Zhi-Quan Luo</author>
            </authors>
        </paper>
        

        <paper>
            <title>When to Learn What: Model-Adaptive Data Augmentation Curriculum</title>
            <abstract>Data augmentation (DA) is widely used to improve the generalization of neural networks by enforcing the invariances and symmetries to pre-defined transformations applied to input data. However, a fixed augmentation policy may have different effects on each sample in different training stages but existing approaches cannot adjust the policy to be adaptive to each sample and the training model. In this paper, we propose &quot;Model-Adaptive Data Augmentation (MADAug)&quot; that jointly trains an augmentation policy network to teach the model &quot;when to learn what&quot;. Unlike previous work, MADAug selects augmentation operators for each input image by a model-adaptive policy varying between training stages, producing a data augmentation curriculum optimized for better generalization. In MADAug, we train the policy through a bi-level optimization scheme, which aims to minimize a validation set loss of a model trained using the policy-produced data augmentations. We conduct an extensive evaluation of MADAug on multiple image classification tasks and network architectures with thorough comparisons to existing DA approaches. MADAug outperforms or is on par with other baselines and exhibits better fairness: it brings improvement to all classes and more to the difficult ones. Moreover, MADAug learned policy shows better performance when transferred to fine-grained datasets. In addition, the auto-optimized policy in MADAug gradually introduces increasing perturbations and naturally forms an easy-to-hard curriculum.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hou_When_to_Learn_What_Model-Adaptive_Data_Augmentation_Curriculum_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hou_When_to_Learn_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chengkai Hou</author><author>Jieyu Zhang</author><author>Tianyi Zhou</author>
            </authors>
        </paper>
        

        <paper>
            <title>COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos</title>
            <abstract>The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance in applications such as VR, AR, and wearable assistive robotics. In this work, we introduce the challenging problem of predicting collisions in diverse environments from multi-view egocentric videos captured from body-mounted cameras. Solving this problem requires a generalizable perception system that can classify which human body joints will collide and estimate a collision region heatmap to localize collisions in the environment. To achieve this, we propose a transformer-based model called COPILOT to perform collision prediction and localization simultaneously, which accumulates information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and enable future research on this task, we develop a synthetic data generation framework that produces egocentric videos of virtual humans moving and colliding within diverse 3D environments. This framework is then used to establish a large-scale dataset consisting of 8.6M egocentric RGBD frames. Extensive experiments show that COPILOT generalizes to unseen synthetic as well as real-world scenes. We further demonstrate COPILOT outputs are useful for downstream collision avoidance through simple closed-loop control. Please visit our project webpage at https://sites.google.com/stanford.edu/copilot.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Boxiao Pan</author><author>Bokui Shen</author><author>Davis Rempe</author><author>Despoina Paschalidou</author><author>Kaichun Mo</author><author>Yanchao Yang</author><author>Leonidas J. Guibas</author>
            </authors>
        </paper>
        

        <paper>
            <title>EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation</title>
            <abstract>Estimating the depths of equirectangular (i.e., 360) images (EIs) is challenging given the distorted 180 x 360 field-of-view, which is hard to be addressed via convolutional neural network (CNN). Although a transformer with global attention achieves significant improvements over CNN for EI depth estimation task, it is computationally inefficient, which raises the need for transformer with local attention. However, to apply local attention successfully for EIs, a specific strategy, which addresses distorted equirectangular geometry and limited receptive field simultaneously, is required. Prior works have only cared either of them, resulting in unsatisfactory depths occasionally. In this paper, we propose an equirectangular geometry-biased transformer termed EGformer. While limiting the computational cost and the number of network parameters, EGformer enables the extraction of the equirectangular geometry-aware local attention with a large receptive field. To achieve this, we actively utilize the equirectangular geometry as the bias for the local attention instead of struggling to reduce the distortion of EIs. As compared to the most recent EI depth estimation studies, the proposed approach yields the best depth outcomes overall with the lowest computational cost and the fewest parameters, demonstrating the effectiveness of the proposed methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yun_EGformer_Equirectangular_Geometry-biased_Transformer_for_360_Depth_Estimation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yun_EGformer_Equirectangular_Geometry-biased_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ilwi Yun</author><author>Chanyong Shin</author><author>Hyunku Lee</author><author>Hyuk-Jae Lee</author><author>Chae Eun Rhee</author>
            </authors>
        </paper>
        

        <paper>
            <title>Size Does Matter: Size-aware Virtual Try-on via Clothing-oriented Transformation Try-on Network</title>
            <abstract>Virtual try-on tasks aim at synthesizing realistic try-on results by trying target clothes on humans. Most previous works relied on the Thin Plate Spline or appearance flows to warp clothes to fit human body shapes. However, both approaches cannot handle complex warping, leading to over distortion or misalignment. Furthermore, there is a critical unaddressed challenge of adjusting clothing sizes for try-on. To tackle these issues, we propose a Clothing-Oriented Transformation Try-On Network (COTTON). COTTON leverages clothing structure with landmarks and segmentation to design a novel landmark-guided transformation for precisely deforming clothes, allowing for size adjustment during try-on. Additionally, to properly remove the clothing region from the human image without losing significant human characteristics, we propose a clothing elimination policy based on both transformed clothes and human segmentation. This method enables users to try on clothes tucked-in or untucked while retaining more human characteristics. Both qualitative and quantitative results show that COTTON outperforms the state-of-the-art high-resolution virtual try-on approaches. All the code is available at https://github.com/cotton6/COTTON-size-does-matter.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Size_Does_Matter_Size-aware_Virtual_Try-on_via_Clothing-oriented_Transformation_Try-on_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Size_Does_Matter_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chieh-Yun Chen</author><author>Yi-Chung Chen</author><author>Hong-Han Shuai</author><author>Wen-Huang Cheng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generating Realistic Images from In-the-wild Sounds</title>
            <abstract>Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and image data and the significant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel approach to generate images from wild sounds. First, we convert sound into text using audio captioning. Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Generating_Realistic_Images_from_In-the-wild_Sounds_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lee_Generating_Realistic_Images_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Taegyeong Lee</author><author>Jeonghun Kang</author><author>Hyeonyu Kim</author><author>Taehwan Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Candidate-aware Selective Disambiguation Based On Normalized Entropy for Instance-dependent Partial-label Learning</title>
            <abstract>In partial-label learning (PLL), each training example has a set of candidate labels, among which only one is the true label. Most existing PLL studies focus on the instance-independent (II) case, where the generation of candidate labels is only dependent on the true label. However, this II-PLL paradigm could be unrealistic, since candidate labels are usually generated according to the specific features of the instance. Therefore, instance-dependent PLL (ID-PLL) has attracted increasing attention recently. Unfortunately, existing ID-PLL studies lack an insightful perception of the intrinsic challenge in ID-PLL. In this paper, we start with an empirical study of the dynamics of label disambiguation in both II-PLL and ID-PLL. We found that the performance degradation of ID-PLL stems from the inaccurate supervision caused by massive under-disambiguated (UD) examples that do not achieve complete disambiguation. To solve this problem, we propose a novel two-stage PLL framework including selective disambiguation and candidate-aware thresholding. Specifically, we first choose a part of well-disambiguated (WD) examples based on the magnitude of normalized entropy (NE) and integrate harmless complementary supervision from the remaining ones to train two networks. Next, the remaining examples whose NE is lower than the specific class-wise WD-NE threshold are selected as additional WD ones. Meanwhile, the remaining UD examples, whose NE is lower than the self-adaptive UD-NE threshold and whose predictions from two networks are agreed, are also regarded as WD ones for model training. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art PLL methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/He_Candidate-aware_Selective_Disambiguation_Based_On_Normalized_Entropy_for_Instance-dependent_Partial-label_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Shuo He</author><author>Guowu Yang</author><author>Lei Feng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models</title>
            <abstract>Video Question Answering (VideoQA) is a challenging task that entails complex multi-modal reasoning. In contrast to multiple-choice VideoQA which aims to predict the answer given several options, the goal of open-ended VideoQA is to answer questions without restricting candidate answers. However, the majority of previous VideoQA models formulate open-ended VideoQA as a classification task to classify the video-question pairs into a fixed answer set, i.e., closed-vocabulary, which contains only frequent answers (e.g., top-1000 answers). This leads the model to be biased toward only frequent answers and fail to generalize on out-of-vocabulary answers. We hence propose a new benchmark, Open-vocabulary Video Question Answering (OVQA), to measure the generalizability of VideoQA models by considering rare and unseen answers. In addition, in order to improve the model&apos;s generalization power, we introduce a novel GNN-based soft verbalizer that enhances the prediction on rare and unseen answers by aggregating the information from their similar words. For evaluation, we introduce new baselines by modifying the existing (closed-vocabulary) open-ended VideoQA models and improve their performances by further taking into account rare and unseen answers. Our ablation studies and qualitative analyses demonstrate that our GNN-based soft verbalizer further improves the model performance, especially on rare and unseen answers. We hope that our benchmark OVQA can serve as a guide for evaluating the generalizability of VideoQA models and inspire future research. Code is available at https://github.com/mlvlab/OVQA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ko_Open-vocabulary_Video_Question_Answering_A_New_Benchmark_for_Evaluating_the_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ko_Open-vocabulary_Video_Question_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dohwan Ko</author><author>Ji Soo Lee</author><author>Miso Choi</author><author>Jaewon Chu</author><author>Jihwan Park</author><author>Hyunwoo J. Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation</title>
            <abstract>Semantic segmentation of point clouds in autonomous driving datasets requires techniques that can process large numbers of points efficiently. Sparse 3D convolutions have become the de-facto tools to construct deep neural networks for this task: they exploit point cloud sparsity to reduce the memory and computational loads and are at the core of today&apos;s best methods. In this paper, we propose an alternative method that reaches the level of state-of-the-art methods without requiring sparse convolutions. We actually show that such level of performance is achievable by relying on tools a priori unfit for large scale and high-performing 3D perception. In particular, we propose a novel 3D backbone, WaffleIron, made almost exclusively of MLPs and dense 2D convolutions and present how to train it to reach high performance on SemanticKITTI and nuScenes. We believe that WaffleIron is a compelling alternative to backbones using sparse 3D convolutions, especially in frameworks and on hardware where those convolutions are not readily available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Puy_Using_a_Waffle_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Gilles Puy</author><author>Alexandre Boulch</author><author>Renaud Marlet</author>
            </authors>
        </paper>
        

        <paper>
            <title>AutoReP: Automatic ReLU Replacement for Fast Private Network Inference</title>
            <abstract>The growth of the Machine-Learning-As-A-Service (MLaaS) market has highlighted clients&apos; data privacy and security issues. Private inference (PI) techniques using cryptographic primitives offer a solution but often have high computation and communication costs, particularly with non-linear operators like ReLU. Many attempts to reduce ReLU operations exist, but they may need heuristic threshold selection or cause substantial accuracy loss. This work introduces AutoReP, a gradient-based approach to lessen non-linear operators and alleviate these issues. It automates the selection of ReLU and polynomial functions to speed up PI applications and introduces distribution-aware polynomial approximation (DaPa) to maintain model expressivity while accurately approximating ReLUs. Our experimental results demonstrate significant accuracy improvements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%, 12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget, Tiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever, AutoReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55% accuracy with 176.1 xReLU budget reduction.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Peng_AutoReP_Automatic_ReLU_Replacement_for_Fast_Private_Network_Inference_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Hongwu Peng</author><author>Shaoyi Huang</author><author>Tong Zhou</author><author>Yukui Luo</author><author>Chenghong Wang</author><author>Zigeng Wang</author><author>Jiahui Zhao</author><author>Xi Xie</author><author>Ang Li</author><author>Tony Geng</author><author>Kaleel Mahmood</author><author>Wujie Wen</author><author>Xiaolin Xu</author><author>Caiwen Ding</author>
            </authors>
        </paper>
        

        <paper>
            <title>Center-Based Decoupled Point-cloud Registration for 6D Object Pose Estimation</title>
            <abstract>In this paper, we propose a novel center-based decoupled point cloud registration framework for robust 6D object pose estimation in real-world scenarios. Our method decouples the translation from the entire transformation by predicting the object center and estimating the rotation in a center-aware manner. This center offset-based translation estimation is correspondence-free, freeing us from the difficulty of constructing correspondences in challenging scenarios, thus improving robustness. To obtain reliable center predictions, we use a multi-view (bird&apos;s eye view and front view) object shape description of the source-point features, with both views jointly voting for the object center. Additionally, we propose an effective shape embedding module to augment the source features, largely completing the missing shape information due to partial scanning, thus facilitating the center prediction. With the center-aligned source and model point clouds, the rotation predictor utilizes feature similarity to establish putative correspondences for SVD-based rotation estimation. In particular, we introduce a center-aware hybrid feature descriptor with a normal correction technique to extract discriminative, part-aware features for high-quality correspondence construction. Our experiments show that our method outperforms the state-of-the-art methods by a large margin on real-world datasets such as TUD-L, LINEMOD, and Occluded-LINEMOD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Haobo Jiang</author><author>Zheng Dang</author><author>Shuo Gu</author><author>Jin Xie</author><author>Mathieu Salzmann</author><author>Jian Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>GAIT: Generating Aesthetic Indoor Tours with Deep Reinforcement Learning</title>
            <abstract>Placing and orienting a camera to compose aesthetically meaningful shots of a scene is not only a key objective in real-world photography and cinematography but also for virtual content creation. The framing of a camera often significantly contributes to the story telling in movies, games, and mixed reality applications. Generating single camera poses or even contiguous trajectories either requires a significant amount of manual labor or requires solving high-dimensional optimization problems, which can be computationally demanding and error-prone. In this paper, we introduce GAIT, a framework for training a Deep Reinforcement Learning (DRL) agent, that learns to automatically control a camera to generate a sequence of aesthetically meaningful views for synthetic 3D indoor scenes. To generate sequences of frames with high aesthetic value, GAIT relies on a neural aesthetics estimator, which is trained on a crowed-sourced dataset. Additionally, we introduce regularization techniques for diversity and smoothness to generate visually interesting trajectories for a 3D environment, and to constrain agent acceleration in the reward function to generate a smooth sequence of camera frames. We validated our method by comparing it to baseline algorithms, based on a perceptual user study, and through ablation studies. Code and visual results are available on the project website: https://desaixie.github.io/gait-rl</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xie_GAIT_Generating_Aesthetic_Indoor_Tours_with_Deep_Reinforcement_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xie_GAIT_Generating_Aesthetic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Desai Xie</author><author>Ping Hu</author><author>Xin Sun</author><author>Soren Pirk</author><author>Jianming Zhang</author><author>Radomir Mech</author><author>Arie E. Kaufman</author>
            </authors>
        </paper>
        

        <paper>
            <title>Rethinking Mobile Block for Efficient Attention-based Models</title>
            <abstract>This paper focuses on developing modern, efficient, lightweight models for dense predictions while trading off parameters, FLOPs, and performance. Inverted Residual Block (IRB) serves as the infrastructure for lightweight CNNs, but no counterpart has been recognized by attention-based studies. This work rethinks lightweight infrastructure from efficient IRB and effective components of Transformer from a unified perspective, extending CNN-based IRB to attention-based models and abstracting a one-residual Meta Mobile Block (MMB) for lightweight model design. Following simple but effective design criterion, we deduce a modern Inverted Residual Mobile Block (iRMB) and build a ResNet-like Efficient MOdel (EMO) with only iRMB for down-stream tasks. Extensive experiments on ImageNet-1K, COCO2017, and ADE20K benchmarks demonstrate the superiority of our EMO over state-of-the-art methods, e.g., EMO-1M/2M/5M achieve 71.5, 75.1, and 78.4 Top-1 that surpass equal-order CNN-/Attention-based models, while trading-off the parameter, efficiency, and accuracy well: running 2.8-4.0x faster than EdgeNeXt on iPhone14.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Rethinking_Mobile_Block_for_Efficient_Attention-based_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jiangning Zhang</author><author>Xiangtai Li</author><author>Jian Li</author><author>Liang Liu</author><author>Zhucun Xue</author><author>Boshen Zhang</author><author>Zhengkai Jiang</author><author>Tianxin Huang</author><author>Yabiao Wang</author><author>Chengjie Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>REAP: A Large-Scale Realistic Adversarial Patch Benchmark</title>
            <abstract>Machine learning models are known to be susceptible to adversarial perturbation. One famous attack is the adversarial patch, a particularly crafted sticker that makes the model mispredict the object it is placed on. This attack presents a critical threat to cyber-physical systems that rely on cameras such as autonomous cars. Despite the significance of the problem, conducting research in this setting has been difficult; evaluating attacks and defenses in the real world is exceptionally costly while synthetic data are unrealistic. In this work, we propose the REAP (REalistic Adversarial Patch) benchmark, a digital benchmark that enables the evaluations on real images under real-world conditions. Built on top of the Mapillary Vistas dataset, our benchmark contains over 14,000 traffic signs. Each sign is augmented with geometric and lighting transformations for applying a digitally generated patch realistically onto the sign. Using our benchmark, we perform the first large-scale assessments of adversarial patch attacks under realistic conditions. Our experiments suggest that patch attacks may present a smaller threat than previously believed and that the success rate of an attack on simpler digital simulations is not predictive of its actual effectiveness in practice. Our benchmark is released publicly at https://github.com/wagner-group/reap-benchmark.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Nabeel Hingun</author><author>Chawin Sitawarin</author><author>Jerry Li</author><author>David Wagner</author>
            </authors>
        </paper>
        

        <paper>
            <title>StegaNeRF: Embedding Invisible Information within Neural Radiance Fields</title>
            <abstract>Recent advancements in neural rendering have paved the way for a future marked by the widespread distribution of visual data through the sharing of Neural Radiance Field (NeRF) model weights. However, while established techniques exist for embedding ownership or copyright information within conventional visual data such as images and videos, the challenges posed by the emerging NeRF format have remained unaddressed. In this paper, we introduce StegaNeRF, an innovative approach for steganographic information embedding within NeRF renderings. We have meticulously developed an optimization framework that enables precise retrieval of hidden information from images generated by NeRF, while ensuring the original visual quality of the rendered images to remain intact. Through rigorous experimentation, we assess the efficacy of our methodology across various potential deployment scenarios. Furthermore, we delve into the insights gleaned from our analysis. StegaNeRF represents an initial foray into the intriguing realm of infusing NeRF renderings with customizable, imperceptible, and recoverable information, all while minimizing any discernible impact on the rendered images. For more details, please visit our project page: https://xggnet.github.io/StegaNeRF/</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_StegaNeRF_Embedding_Invisible_Information_within_Neural_Radiance_Fields_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_StegaNeRF_Embedding_Invisible_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Chenxin Li</author><author>Brandon Y. Feng</author><author>Zhiwen Fan</author><author>Panwang Pan</author><author>Zhangyang Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Robust Evaluation of Diffusion-Based Adversarial Purification</title>
            <abstract>We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy improving robustness compared to the current diffusion-based purification methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lee_Robust_Evaluation_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Minjong Lee</author><author>Dongwoo Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Hyperbolic Audio-visual Zero-shot Learning</title>
            <abstract>Audio-visual zero-shot learning aims to classify samples consisting of a pair of corresponding audio and video sequences from classes that are not present during training. An analysis of the audio-visual data reveals a large degree of hyperbolicity, indicating the potential benefit of using a hyperbolic transformation to achieve curvature-aware geometric learning, with the aim of exploring more complex hierarchical data structures for this task. The proposed approach employs a novel loss function that incorporates cross-modality alignment between video and audio features in the hyperbolic space. Additionally, we explore the use of multiple adaptive curvatures for hyperbolic projections. The experimental results on this very challenging task demonstrate that our proposed hyperbolic approach for zero-shot learning outperforms the SOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSL achieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hong_Hyperbolic_Audio-visual_Zero-shot_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hong_Hyperbolic_Audio-visual_Zero-shot_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jie Hong</author><author>Zeeshan Hayder</author><author>Junlin Han</author><author>Pengfei Fang</author><author>Mehrtash Harandi</author><author>Lars Petersson</author>
            </authors>
        </paper>
        

        <paper>
            <title>ModelGiF: Gradient Fields for Model Functional Distance</title>
            <abstract>The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures and tasks. Inspired by the concept of &quot;field&quot; in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We provide theoretical insights into the proposed ModelGiFs for model functional distance, and validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate the versatility of the proposed ModelGiF on these tasks, with significantly superiority performance to state-of-the-art competitors. Codes are available at https://github.com/zju-vipa/modelgif.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Song_ModelGiF_Gradient_Fields_for_Model_Functional_Distance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jie Song</author><author>Zhengqi Xu</author><author>Sai Wu</author><author>Gang Chen</author><author>Mingli Song</author>
            </authors>
        </paper>
        

        <paper>
            <title>SIGMA: Scale-Invariant Global Sparse Shape Matching</title>
            <abstract>We propose a novel mixed-integer programming (MIP) formulation for generating precise sparse correspondences for highly non-rigid shapes. To this end, we introduce a projected Laplace-Beltrami operator (PLBO) which combines intrinsic and extrinsic geometric information to measure the deformation quality induced by predicted correspondences. We integrate the PLBO, together with an orientation-aware regulariser, into a novel MIP formulation that can be solved to global optimality for many practical problems. In contrast to previous methods, our approach is provably invariant to rigid transformations and global scaling, initialisation-free, has optimality guarantees, and scales to high resolution meshes with (empirically observed) linear time. We show state-of-the-art results for sparse non-rigid matching on several challenging 3D datasets, including data with inconsistent meshing, as well as applications in mesh-to-point-cloud matching.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gao_SIGMA_Scale-Invariant_Global_Sparse_Shape_Matching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gao_SIGMA_Scale-Invariant_Global_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Maolin Gao</author><author>Paul Roetzer</author><author>Marvin Eisenberger</author><author>Zorah L hner</author><author>Michael Moeller</author><author>Daniel Cremers</author><author>Florian Bernard</author>
            </authors>
        </paper>
        

        <paper>
            <title>VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs</title>
            <abstract>We propose VidStyleODE, a spatiotemporally continuous disentangled video representation based upon StyleGAN and Neural-ODEs. Effective traversal of the latent space learned by Generative Adversarial Networks (GANs) has been the basis for recent breakthroughs in image editing. However, the applicability of such advancements to the video domain has been hindered by the difficulty of representing and controlling videos in the latent space of GANs. In particular, videos are composed of content (i.e., appearance) and complex motion components that require a special mechanism to disentangle and control. To achieve this, VidStyleODE encodes the video content in a pre-trained StyleGAN W+ space and benefits from a latent ODE component to summarize the spatiotemporal dynamics of the input video. Our novel continuous video generation process then combines the two to generate high-quality and temporally consistent videos with varying frame rates. We show that our proposed method enables a variety of applications on real videos: text-guided appearance manipulation, motion manipulation, image animation, and video interpolation and extrapolation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ali_VidStyleODE_Disentangled_Video_Editing_via_StyleGAN_and_NeuralODEs_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ali_VidStyleODE_Disentangled_Video_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Moayed Haji Ali</author><author>Andrew Bond</author><author>Tolga Birdal</author><author>Duygu Ceylan</author><author>Levent Karacan</author><author>Erkut Erdem</author><author>Aykut Erdem</author>
            </authors>
        </paper>
        

        <paper>
            <title>LeaF: Learning Frames for 4D Point Cloud Sequence Understanding</title>
            <abstract>We focus on learning descriptive geometry and motion features from 4D point cloud sequences in this work. Existing works usually develop generic 4D learning tools without leveraging the prior that a 4D sequence comes from a single 3D scene with local dynamics. Based on this observation, we propose to learn region-wise coordinate frames that transform together with the underlying geometry. With such frames, we can factorize geometry and motion to facilitate a feature-space geometric reconstruction for more effective 4D learning. To learn such region frames, we develop a rotation equivariant network with a frame stabilization strategy. To leverage such frames for better spatial-temporal feature learning, we develop a frame-guided 4D learning scheme. Experiments show that this approach significantly outperforms previous state-of-the-art methods on a wide range of 4D understanding benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yunze Liu</author><author>Junyu Chen</author><author>Zekai Zhang</author><author>Jingwei Huang</author><author>Li Yi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Improved Input Masking for Convolutional Neural Networks</title>
            <abstract>The ability to remove features from the input of machine learning models is very important to understand and interpret model predictions. However, this is non-trivial for vision models since masking out parts of the input image typically causes large distribution shifts. This is because the baseline color used for masking (typically grey or black) is out of distribution. Furthermore, the shape of the mask itself can contain unwanted signals which can be used by the model for its predictions. Recently, there has been some progress in mitigating this issue (called missingness bias) in image masking for vision transformers. In this work, we propose a new masking method for CNNs we call layer masking in which the missingness bias caused by masking is reduced to a large extent. Intuitively, layer masking applies a mask to intermediate activation maps so that the model only processes the unmasked input. We show that our method (i) is able to eliminate or minimize the influence of the mask shape or color on the output of the model, and (ii) is much better than replacing the masked region by black or grey for input perturbation based interpretability techniques like LIME. Thus, layer masking is much less affected by missingness bias than other masking strategies. We also demonstrate how the shape of the mask may leak information about the class, thus affecting estimates of model reliance on class-relevant features derived from input masking. Furthermore, we discuss the role of data augmentation techniques for tackling this problem, and argue that they are not sufficient for preventing model reliance on mask shape.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Balasubramanian_Towards_Improved_Input_Masking_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Balasubramanian_Towards_Improved_Input_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Sriram Balasubramanian</author><author>Soheil Feizi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Gramian Attention Heads are Strong yet Efficient Vision Learners</title>
            <abstract>We introduce a novel architecture design that enhances expressiveness by incorporating multiple head classifiers (i.e., classification heads) instead of relying on channel expansion or additional building blocks. Our approach employs attention-based aggregation, utilizing pairwise feature similarity to enhance multiple lightweight heads with minimal resource overhead. We compute the Gramian matrices to reinforce class tokens in an attention layer for each head. This enables the heads to learn more discriminative representations, enhancing their aggregation capabilities. Furthermore, we propose a learning algorithm that encourages heads to complement each other by reducing correlation for aggregation. Our models eventually surpass state-of-the-art CNNs and ViTs regarding the accuracy-throughput trade-off on ImageNet-1K and deliver remarkable performance across various downstream tasks, such as COCO object instance segmentation, ADE20k semantic segmentation, and fine-grained visual classification datasets. The effectiveness of our framework is substantiated by practical experimental results and further underpinned by generalization error bound. We release the code publicly at: https://github.com/Lab-LVM/imagenet-models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ryu_Gramian_Attention_Heads_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jongbin Ryu</author><author>Dongyoon Han</author><author>Jongwoo Lim</author>
            </authors>
        </paper>
        

        <paper>
            <title>MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices</title>
            <abstract>In recent years, many deep learning based image inpainting methods have been developed by the research community. Some of those methods have shown impressive image completion abilities. Yet, to the best of our knowledge, there is no image inpainting model designed to run on mobile devices. In this paper we present a simple image inpainting baseline, Mobile Inpainting GAN (MI-GAN), which is approximately one order of magnitude computationally cheaper and smaller than existing state-of-the-art inpainting models, and can be efficiently deployed on mobile devices. Excessive quantitative and qualitative evaluations show that MI-GAN performs comparable or, in some cases, better than recent state-of-the-art approaches. Moreover, we perform a user study comparing MI-GAN results with results from several commercial mobile inpainting applications, which clearly shows the advantage of MI-GAN in comparison to existing apps. With the purpose of high quality and efficient inpainting, we utilize an effective combination of adversarial training, model re-parametrization, and knowledge distillation. Our models and code are publicly available at https://github.com/Picsart-AI-Research/MI-GAN.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sargsyan_MI-GAN_A_Simple_Baseline_for_Image_Inpainting_on_Mobile_Devices_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sargsyan_MI-GAN_A_Simple_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Andranik Sargsyan</author><author>Shant Navasardyan</author><author>Xingqian Xu</author><author>Humphrey Shi</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction</title>
            <abstract>Neural Radiance Fields (NeRF) has achieved impressive results in single object scene reconstruction and novel view synthesis, as demonstrated on many single modality and single object focused indoor scene datasets like DTU, BMVS, and NeRF Synthetic. However, the study of NeRF on large-scale outdoor scene reconstruction is still limited, as there is no unified outdoor scene dataset for large-scale NeRF evaluation due to expensive data acquisition and calibration costs. In this work, we propose a large-scale outdoor multi-modal dataset, OMMO dataset, containing complex objects and scenes with calibrated images, point clouds and prompt annotations. A new benchmark for several outdoor NeRF-based tasks is established, such as novel view synthesis,diverse 3D representation, and multi-modal NeRF. To create the dataset, we capture and collect a large number of real fly-view videos and select high-quality and high-resolution clips from them. Then we design a quality review module to refine images, remove low-quality frames and fail-to-calibrate scenes through a learning-based automatic evaluation plus manual review. Finally, volunteers are employed to label and review the prompt annotation for each scene and keyframe.Compared with existing NeRF datasets, our dataset contains abundant real-world urban and natural scenes with various scales, camera trajectories, and lighting conditions. Experiments show that our dataset can benchmark most state-of-the-art NeRF methods on different tasks.The dataset can be found at the following link: https://ommo.luchongshan.com/ .</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lu_A_Large-Scale_Outdoor_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Chongshan Lu</author><author>Fukun Yin</author><author>Xin Chen</author><author>Wen Liu</author><author>Tao Chen</author><author>Gang Yu</author><author>Jiayuan Fan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection</title>
            <abstract>We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT encoder can work surprisingly well in the challenging object-level recognition scenario even with randomly sampled partial observations, e.g., only 25% 50% of the input embeddings. (ii) In order to construct multi-scale representations for object detection from single-scale ViT, a randomly initialized compact convolutional stem supplants the pre-trained patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid network without further upsampling or other manipulations. While the pre-trained ViT is only regarded as the third-stage of our detector&apos;s backbone instead of the whole feature extractor. This naturally results in a ConvNet-ViT hybrid architecture. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform leading hierarchical architectures such as Swin Transformer, MViTv2 and ConvNeXt on COCO object detection &amp; instance segmentation, and achieves better results compared with the previous best adapted vanilla ViT detector using a more modest fine-tuning recipe while converging 2.8x faster. Code and pre-trained models are available at https://github.com/hustvl/MIMDet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fang_Unleashing_Vanilla_Vision_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuxin Fang</author><author>Shusheng Yang</author><author>Shijie Wang</author><author>Yixiao Ge</author><author>Ying Shan</author><author>Xinggang Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Spatio-Temporal Crop Aggregation for Video Representation Learning</title>
            <abstract>We propose Spatio-temporal Crop Aggregation for video representation LEarning (SCALE), a novel method that enjoys high scalability at both training and inference time. Our model builds long-range video features by learning from sets of video clip-level features extracted with a pre-trained backbone. To train the model, we propose a self-supervised objective consisting of masked clip feature predictions. We apply sparsity to both the input, by extracting a random set of video clips, and to the loss function, by only reconstructing the sparse inputs. Moreover, we use dimensionality reduction by working in the latent space of a pre-trained backbone applied to single video clips. These techniques make our method not only extremely efficient to train but also highly effective in transfer learning. We demonstrate that our video representation yields state-of-the-art performance with linear, nonlinear, and k-NN probing on common action classification and video understanding datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sameni_Spatio-Temporal_Crop_Aggregation_for_Video_Representation_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Sepehr Sameni</author><author>Simon Jenni</author><author>Paolo Favaro</author>
            </authors>
        </paper>
        

        <paper>
            <title>Zero-guidance Segmentation Using Zero Segment Labels</title>
            <abstract>The joint visual-language model CLIP has enabled new and exciting applications, such as open-vocabulary segmentation, which can locate any segment given an arbitrary text query. In our research, we ask whether it is possible to discover semantic segments without any user guidance in the form of text queries or predefined classes, and label them using natural language automatically? We propose a novel problem zero-guidance segmentation and the first baseline that leverages two pre-trained generalist models, DINO and CLIP, to solve this problem without any fine-tuning or segmentation dataset. The general idea is to first segment an image into small over-segments, encode them into CLIP&apos;s visual-language space, translate them into text labels, and merge semantically similar segments together. The key challenge, however, is how to encode a visual segment into a segment-specific embedding that balances global and local context information, both useful for recognition. Our main contribution is a novel attention-masking technique that balances the two contexts by analyzing the attention layers inside CLIP. We also introduce several metrics for the evaluation of this new task. With CLIP&apos;s innate knowledge, our method can precisely locate the Mona Lisa painting among a museum crowd.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Rewatbowornwong_Zero-guidance_Segmentation_Using_Zero_Segment_Labels_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Rewatbowornwong_Zero-guidance_Segmentation_Using_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Pitchaporn Rewatbowornwong</author><author>Nattanat Chatthee</author><author>Ekapol Chuangsuwanich</author><author>Supasorn Suwajanakorn</author>
            </authors>
        </paper>
        

        <paper>
            <title>Communication-efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence</title>
            <abstract>Reducing communication overhead in federated learning (FL) is challenging but crucial for large-scale distributed privacy-preserving machine learning. While methods utilizing sparsification or other techniques can largely reduce the communication overhead, the convergence rate is also greatly compromised. In this paper, we propose a novel method named Single-Step Synthetic Features Compressor (3SFC) to achieve communication-efficient FL by directly constructing a tiny synthetic dataset containing synthetic features based on raw gradients. Therefore, 3SFC can achieve an extremely low compression rate when the constructed synthetic dataset contains only one data sample. Additionally, the compressing phase of 3SFC utilizes a similarity-based objective function so that it can be optimized with just one step, considerably improving its performance and robustness. To minimize the compressing error, error feedback (EF) is also incorporated into 3SFC. Experiments on multiple datasets and models suggest that 3SFC has significantly better convergence rates compared to competing methods with lower compression rates (i.e., up to 0.02%). Furthermore, ablation studies and visualizations show that 3SFC can carry more information than competing methods for every communication round, further validating its effectiveness.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_Communication-efficient_Federated_Learning_with_Single-Step_Synthetic_Features_Compressor_for_Faster_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yuhao Zhou</author><author>Mingjia Shi</author><author>Yuanxi Li</author><author>Yanan Sun</author><author>Qing Ye</author><author>Jiancheng Lv</author>
            </authors>
        </paper>
        

        <paper>
            <title>CTVIS: Consistent Training for Online Video Instance Segmentation</title>
            <abstract>The discrimination of instance embeddings plays a vital role in associating instances across time for online video instance segmentation (VIS). Instance embedding learning is directly supervised by the contrastive loss computed upon the contrastive items (CIs), which are sets of anchor/positive/negative embeddings. Recent online VIS methods leverage CIs sourced from one reference frame only, which we argue is insufficient for learning highly discriminative embeddings. Intuitively, a possible strategy to enhance CIs is replicating the inference phase during training. To this end, we propose a simple yet effective training strategy, called Consistent Training for Online VIS(CTVIS), which devotes to aligning the training and inference pipelines in terms of building CIs. Specifically, CTVIS constructs CIs by referring inference the momentum-averaged embedding and the memory bank storage mechanisms, and adding noise to the relevant embeddings. Such an extension allows a reliable comparison between embeddings of current instances and the stable representations of historical instances, thereby conferring an advantage in modeling VIS challenges such as occlusion, re-identification, and deformation. Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on three VIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS (35.5% AP). Furthermore, we find that pseudo-videos transformed from images can train robust models surpassing fully-supervised ones.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ying_CTVIS_Consistent_Training_for_Online_Video_Instance_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ying_CTVIS_Consistent_Training_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Kaining Ying</author><author>Qing Zhong</author><author>Weian Mao</author><author>Zhenhua Wang</author><author>Hao Chen</author><author>Lin Yuanbo Wu</author><author>Yifan Liu</author><author>Chengxiang Fan</author><author>Yunzhi Zhuge</author><author>Chunhua Shen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Video Object Segmentation with Online Adversarial Self-Tuning</title>
            <abstract>The existing unsupervised video object segmentation methods depend heavily on the segmentation model trained offline on a labeled training video set, and cannot well generalize to the test videos from a different domain with possible distribution shifts. We propose to perform online fine-tuning on the pre-trained segmentation model to adapt to any ad-hoc videos at the test time. To achieve this, we design an offline semi-supervised adversarial training process, which leverages the unlabeled video frames to improve the model generalizability while aligning the features of the labeled video frames with the features of the unlabeled video frames. With the trained segmentation model, we further conduct an online self-supervised adversarial finetuning, in which a teacher model and a student model are first initialized with the pre-trained segmentation model weights, and the pseudo label produced by the teacher model is used to supervise the student model in an adversarial learning framework. Through online finetuning, the student model is progressively updated according to the emerging patterns in each test video, which significantly reduces the test-time domain gap. We integrate our offline training and online fine-tuning in a unified framework for unsupervised video object segmentation and dub our method Online Adversarial Self-Tuning (OAST). The experiments show that our method out-performs the state-of-the-arts with significant gains on the popular video object segmentation datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Su_Unsupervised_Video_Object_Segmentation_with_Online_Adversarial_Self-Tuning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Tiankang Su</author><author>Huihui Song</author><author>Dong Liu</author><author>Bo Liu</author><author>Qingshan Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>GlobalMapper: Arbitrary-Shaped Urban Layout Generation</title>
            <abstract>Modeling and designing urban building layouts is of significant interest in computer vision, computer graphics, and urban applications. A building layout consists of a set of buildings in city blocks defined by a network of roads. We observe that building layouts are discrete structures, consisting of multiple rows of buildings of various shapes, and are amenable to skeletonization for mapping arbitrary city block shapes to a canonical form. Hence, we propose a fully automatic approach to building layout generation using a graph attention networks. Our method generates realistic urban layouts given arbitrary road networks, and enables conditional generation based on learned priors. Our results, including user study, demonstrate superior performance as compared to prior layout generation networks, support arbitrary city block and varying building shapes as demonstrated by generating layouts for 28 large cities.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/He_GlobalMapper_Arbitrary-Shaped_Urban_Layout_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/He_GlobalMapper_Arbitrary-Shaped_Urban_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Liu He</author><author>Daniel Aliaga</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unified Coarse-to-Fine Alignment for Video-Text Retrieval</title>
            <abstract>The canonical approach to video-text retrieval leverages a coarse-grained or fine-grained alignment between visual and textual information. However, retrieving the correct video according to the text query is often challenging as it requires the ability to reason about both high-level (scene) and low-level (object) visual clues and how they relate to the text query. To this end, we propose a Unified Coarse-to-fine Alignment model, dubbed UCOFIA. Specifically, our model captures the cross-modal similarity information at different granularity levels. To alleviate the effect of irrelevant visual clues, we also apply an Interactive Similarity Aggregation module (ISA) to consider the importance of different visual features while aggregating the cross-modal similarity to obtain a similarity score for each granularity. Finally, we apply the Sinkhorn-Knopp algorithm to normalize the similarities of each level before summing them, alleviating over- and under-representation issues at different levels. By jointly considering the cross-modal similarity of different granularity, UCOFIA allows the effective unification of multi-grained alignments. Empirically, UCOFIA outperforms previous state-of-the-art CLIP-based methods on multiple video-text retrieval benchmarks, achieving 2.4%, 1.4% and 1.3% improvements in text-to-video retrieval R@1 on MSR-VTT, Activity-Net, and DiDeMo, respectively. Our code is publicly available at https://github.com/Ziyang412/UCoFiA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Unified_Coarse-to-Fine_Alignment_for_Video-Text_Retrieval_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Unified_Coarse-to-Fine_Alignment_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ziyang Wang</author><author>Yi-Lin Sung</author><author>Feng Cheng</author><author>Gedas Bertasius</author><author>Mohit Bansal</author>
            </authors>
        </paper>
        

        <paper>
            <title>Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models</title>
            <abstract>Prompt tuning, a recently emerging paradigm, enables the powerful vision-language pre-training models to adapt to downstream tasks in a parameter- and data- efficient way, by learning the &quot;soft prompts&quot; to condition frozen pre-training models. Though effective, it is particularly problematic in the few-shot scenario, where prompt tuning performance is sensitive to the initialization and requires a time-consuming process to find a good initialization, thus restricting the fast adaptation ability of the pre-training models. In addition, prompt tuning could undermine the generalizability of the pre-training models, because the learnable prompt tokens are easy to overfit to the limited training samples. To address these issues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization for better adaptation and a lightweight gradient regulating function for strong cross-domain generalizability in a meta-learning paradigm using only the unlabeled image-text pre-training data. Rather than designing a specific prompt tuning method, our GRAM can be easily incorporated into various prompt tuning methods in a model-agnostic way, and comprehensive experiments show that GRAM brings about consistent improvement for them in several settings (i.e., few-shot learning, cross-domain generalization, cross-dataset generalization, etc.) over 11 datasets. Further, experiments show that GRAM enables the orthogonal methods of textual and visual prompt tuning to work in a mutually-enhanced way, offering better generalizability beyond the uni-modal prompt tuning methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Gradient-Regulated_Meta-Prompt_Learning_for_Generalizable_Vision-Language_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Juncheng Li</author><author>Minghe Gao</author><author>Longhui Wei</author><author>Siliang Tang</author><author>Wenqiao Zhang</author><author>Mengze Li</author><author>Wei Ji</author><author>Qi Tian</author><author>Tat-Seng Chua</author><author>Yueting Zhuang</author>
            </authors>
        </paper>
        

        <paper>
            <title>MUter: Machine Unlearning on Adversarially Trained Models</title>
            <abstract>Machine unlearning is an emerging task of removing the influence of selected training datapoints from a trained model upon data deletion requests, which echoes the widely enforced data regulations mandating the Right to be Forgotten. Many unlearning methods have been proposed recently, achieving significant efficiency gains over the naive baseline of retraining from scratch. However, existing methods focus exclusively on unlearning from standard training models and do not apply to adversarial training models (ATMs) despite their popularity as effective defenses against adversarial examples. During adversarial training, the training data are involved in not only an outer loop for minimizing the training loss, but also an inner loop for generating the adversarial perturbation. Such bi-level optimization greatly complicates the influence measure for the data to be deleted and renders the unlearning more challenging than standard model training with single-level optimization. This paper proposes a new approach called MUter for unlearning from ATMs. We derive a closed-form unlearning step underpinned by a total Hessian-related data influence measure, while existing methods can mis-capture the data influence associated with the indirect Hessian part. We further alleviate the computational cost by introducing a series of approximations and conversions to avoid the most computationally demanding parts of Hessian inversions. The efficiency and effectiveness of MUter have been validated through experiments on four datasets using both linear and neural network models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_MUter_Machine_Unlearning_on_Adversarially_Trained_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_MUter_Machine_Unlearning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junxu Liu</author><author>Mingsheng Xue</author><author>Jian Lou</author><author>Xiaoyu Zhang</author><author>Li Xiong</author><author>Zhan Qin</author>
            </authors>
        </paper>
        

        <paper>
            <title>ParCNetV2: Oversized Kernel with Enhanced Attention</title>
            <abstract>Transformers have shown great potential in various computer vision tasks. By borrowing design concepts from transformers, many studies revolutionized CNNs and showed remarkable results. This paper falls in this line of studies. Specifically, we propose a new convolutional neural network, ParCNetV2, that extends the research line of ParCNetV1 by bridging the gap between CNN and ViT. It introduces two key designs: 1) Oversized Convolution (OC) with twice the size of the input, and 2) Bifurcate Gate Unit (BGU) to ensure that the model is input adaptive. Fusing OC and BGU in a unified CNN, ParCNetV2 is capable of flexibly extracting global features like ViT, while maintaining lower latency and better accuracy. Extensive experiments demonstrate the superiority of our method over other convolutional neural networks and hybrid models that combine CNNs and transformers. The code is publicly available at https://github.com/XuRuihan/ParCNetV2.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_ParCNetV2_Oversized_Kernel_with_Enhanced_Attention_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_ParCNetV2_Oversized_Kernel_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ruihan Xu</author><author>Haokui Zhang</author><author>Wenze Hu</author><author>Shiliang Zhang</author><author>Xiaoyu Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>RealGraph: A Multiview Dataset for 4D Real-world Context Graph Generation</title>
            <abstract>In this paper, we propose a brand new scene understanding paradigm called &quot;Context Graph Generation (CGG)&quot;, aiming at abstracting holistic semantic information in the complicated 4D world. The CGG task capitalizes on the calibrated multiview videos of a dynamic scene, and targets at recovering semantic information (coordination, trajectories and relationships) of the presented objects in the form of spatio-temporal context graph in 4D space. We also present a benchmark 4D video dataset &quot;RealGraph&quot;, the first dataset tailored for the proposed CGG task. The raw data of RealGraph is composed of calibrated and synchronized multiview videos. We exclusively provide manual annotations including object 2D&amp;3D bounding boxes, category labels and semantic relationships. We also make sure the annotated ID for every single object is temporally and spatially consistent. We propose the first CGG baseline algorithm, Multiview-based Context Graph Generation Network (MCGNet), to empirically investigate the legitimacy of CGG task on RealGraph dataset. We nevertheless reveal the great challenges behind this task and encourage the community to explore beyond our solution.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_RealGraph_A_Multiview_Dataset_for_4D_Real-world_Context_Graph_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lin_RealGraph_A_Multiview_Dataset_for_4D_Real-world_Context_Graph_Generation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Haozhe Lin</author><author>Zequn Chen</author><author>Jinzhi Zhang</author><author>Bing Bai</author><author>Yu Wang</author><author>Ruqi Huang</author><author>Lu Fang</author>
            </authors>
        </paper>
        

        <paper>
            <title>PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction</title>
            <abstract>Vectorized high-definition map online construction has garnered considerable attention in the field of autonomous driving research. Most existing approaches model changeable map elements using a fixed number of points, or predict local maps in a two-stage autoregressive manner, which may miss essential details and lead to error accumulation. Towards precise map element learning, we propose a simple yet effective architecture named PivotNet, which adopts unified pivot-based map representations and is formulated as a direct set prediction paradigm. Concretely, we first propose a novel Point-to-Line Mask module to encode both the subordinate and geometrical point-line priors in the network. Then, a well-designed Pivot Dynamic Matching module is proposed to model the topology in dynamic point sequences by introducing the concept of sequence matching. Furthermore, to supervise the position and topology of the vectorized point predictions, we propose a Dynamic Vectorized Sequence loss. Extensive experiments and ablations show that PivotNet is remarkably superior to other SOTAs by 5.9 mAP at least. The code will be available soon.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ding_PivotNet_Vectorized_Pivot_Learning_for_End-to-end_HD_Map_Construction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ding_PivotNet_Vectorized_Pivot_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenjie Ding</author><author>Limeng Qiao</author><author>Xi Qiu</author><author>Chi Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Universal Domain Adaptation via Compressive Attention Matching</title>
            <abstract>Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain without any prior knowledge about the label set. The challenge lies in how to determine whether the target samples belong to common categories. The mainstream methods make judgments based on the sample features, which overemphasizes global information while ignoring the most crucial local objects in the image, resulting in limited accuracy. To address this issue, we propose a Universal Attention Matching (UniAM) framework by exploiting the self-attention mechanism in vision transformer to capture the crucial object information. The proposed framework introduces a novel Compressive Attention Matching (CAM) approach to explore the core information by compressively representing attentions. Furthermore, CAM incorporates a residual-based measurement to determine the sample commonness. By utilizing the measurement, UniAM achieves domain-wise and category-wise Common Feature Alignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first method utilizing the attention in vision transformer directly to perform classification tasks. Extensive experiments show that UniAM outperforms the current state-of-the-art methods on various benchmark datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Didi Zhu</author><author>Yinchuan Li</author><author>Junkun Yuan</author><author>Zexi Li</author><author>Kun Kuang</author><author>Chao Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport</title>
            <abstract>Weakly-supervised image segmentation has recently attracted increasing research attentions, aiming to avoid the expensive pixel-wise labeling. In this paper, we present an effective method, namely Point2Mask, to achieve high-quality panoptic prediction using only a single random point annotation per target for training. Specifically, we formulate the panoptic pseudo-mask generation as an Optimal Transport (OT) problem, where each ground-truth (gt) point label and pixel sample are defined as the label supplier and consumer, respectively. The transportation cost is calculated by the introduced task-oriented maps, which focus on the category-wise and instance-wise differences among the various thing and stuff targets. Furthermore, a centroid-based scheme is proposed to set the accurate unit number for each gt point supplier. Hence, the pseudo-mask generation is converted into finding the optimal transport plan at a globally minimal transportation cost, which can be solved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC and COCO demonstrate the promising performance of our proposed Point2Mask approach to point-supervised panoptic segmentation. Source code is available at: https://github.com/LiWentomng/Point2Mask.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Point2Mask_Point-supervised_Panoptic_Segmentation_via_Optimal_Transport_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Point2Mask_Point-supervised_Panoptic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wentong Li</author><author>Yuqian Yuan</author><author>Song Wang</author><author>Jianke Zhu</author><author>Jianshu Li</author><author>Jian Liu</author><author>Lei Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World</title>
            <abstract>Physical adversarial attacks against deep neural networks (DNNs) have recently gained increasing attention. The current mainstream physical attacks use printed adversarial patches or camouflage to alter the appearance of the target object. However, these approaches generate conspicuous adversarial patterns that show poor stealthiness. Another physical deployable attack is the optical attack, featuring stealthiness while exhibiting weakly in the daytime with sunlight. In this paper, we propose a novel Reflected Light Attack (RFLA), featuring effective and stealthy in both the digital and physical world, which is implemented by placing the color transparent plastic sheet and a paper cut of a specific shape in front of the mirror to create different colored geometries on the target object. To achieve these goals, we devise a general framework based on the circle to model the reflected light on the target object. Specifically, we optimize a circle (composed of a coordinate and radius) to carry various geometrical shapes determined by the optimized angle. The fill color of the geometry shape and its corresponding transparency are also optimized. We extensively evaluate the effectiveness of RFLA on different datasets and models. Experiment results suggest that the proposed method achieves over 99% success rate on different datasets and models in the digital world. Additionally, we verify the effectiveness of the proposed method in different physical environments by using sunlight or a flashlight.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_RFLA_A_Stealthy_Reflected_Light_Adversarial_Attack_in_the_Physical_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_RFLA_A_Stealthy_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Donghua Wang</author><author>Wen Yao</author><author>Tingsong Jiang</author><author>Chao Li</author><author>Xiaoqian Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Nearest Neighbor Guidance for Out-of-Distribution Detection</title>
            <abstract>Detecting out-of-distribution (OOD) samples are crucial for machine learning models deployed in open-world environments. Classifier-based scores are a standard approach for OOD detection due to their fine-grained detection capability. However, these scores often suffer from overconfidence issues, misclassifying OOD samples distant from the in-distribution region. To address this challenge, we propose a method called Nearest Neighbor Guidance (NNGuide) that guides the classifier-based score to respect the boundary geometry of the data manifold. NNGuide reduces the overconfidence of OOD samples while preserving the fine-grained capability of the classifier-based score. We conduct extensive experiments on ImageNet OOD detection benchmarks under diverse settings, including a scenario where the ID data undergoes natural distribution shift. Our results demonstrate that NNGuide provides a significant performance improvement on the base detection scores, achieving state-of-the-art results on both AUROC, FPR95, and AUPR metrics.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Park_Nearest_Neighbor_Guidance_for_Out-of-Distribution_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Park_Nearest_Neighbor_Guidance_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jaewoo Park</author><author>Yoon Gyo Jung</author><author>Andrew Beng Jin Teoh</author>
            </authors>
        </paper>
        

        <paper>
            <title>Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions</title>
            <abstract>Probabilistic diffusion models have achieved state-of-the-art results for image synthesis, inpainting, and text-to-image tasks. However, they are still in the early stages of generating complex 3D shapes. This work proposes Diffusion-SDF, a generative model for shape completion, single-view reconstruction, and reconstruction of real-scanned point clouds. We use neural signed distance functions (SDFs) as our 3D representation to parameterize the geometry of various signals (e.g., point clouds, 2D images) through neural networks. Neural SDFs are implicit functions and diffusing them amounts to learning the reversal of their neural network weights, which we solve using a custom modulation module. Extensive experiments show that our method is capable of both realistic unconditional generation and conditional generation from partial inputs. This work expands the domain of diffusion models from learning 2D, explicit representations, to 3D, implicit representations. Code is released at https://github.com/princeton-computational-imaging/Diffusion-SDF.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chou_Diffusion-SDF_Conditional_Generative_Modeling_of_Signed_Distance_Functions_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Gene Chou</author><author>Yuval Bahat</author><author>Felix Heide</author>
            </authors>
        </paper>
        

        <paper>
            <title>Open-Vocabulary Object Detection With an Open Corpus</title>
            <abstract>Existing open vocabulary object detection (OVD) works expand the object detector toward open categories by replacing the classifier with the category text embeddings and optimizing the region-text alignment on data of the base categories. However, both the class-agnostic proposal generator and the classifier are biased to the seen classes as demonstrated by the gaps of objectness and accuracy assessment between base and novel classes. In this paper, an open corpus, composed of a set of external object concepts and clustered to several centroids, is introduced to improve the generalization ability in the detector. We propose the generalized objectness assessment (GOAT) in the proposal generator based on the visual-text alignment, where the similarities of visual feature to the cluster centroids are summarized as the objectness. This simple heuristic evaluates objectness with concepts in open corpus and is thus generalized to open categories. We further propose category expanding (CE) with open corpus in two training tasks, which enables the detector to perceive more categories in the feature space and get more reasonable optimization direction. For the classification task, we introduce an open corpus classifier by reconstructing original classifier with similar words in text space. For the image-caption alignment task, the open corpus centroids are incorporated to enlarge the negative samples in the contrastive loss. Extensive experiments demonstrate the effectiveness of GOAT and CE, which greatly improve the performance on novel classes and get new state-of-the-art on the OVD benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Open-Vocabulary_Object_Detection_With_an_Open_Corpus_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Open-Vocabulary_Object_Detection_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiong Wang</author><author>Huiming Zhang</author><author>Haiwen Hong</author><author>Xuan Jin</author><author>Yuan He</author><author>Hui Xue</author><author>Zhou Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Spectrum-guided Multi-granularity Referring Video Object Segmentation</title>
            <abstract>Current referring video object segmentation (R-VOS) techniques extract conditional kernels from encoded (low-resolution) vision-language features to segment the decoded high-resolution features. We discovered that this causes significant feature drift, which the segmentation kernels struggle to perceive during the forward computation. This negatively affects the ability of segmentation kernels. To address the drift problem, we propose a Spectrum-guided Multi-granularity (SgMg) approach, which performs direct segmentation on the encoded features and employs visual details to further optimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion (SCF) to perform intra-frame global interactions in the spectral domain for effective multimodal representation. Finally, we extend SgMg to perform multi-object R-VOS, a new paradigm that enables simultaneous segmentation of multiple referred objects in a video. This not only makes R-VOS faster, but also more practical. Extensive experiments show that SgMg achieves state-of-the-art performance on four video benchmark datasets, outperforming the nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMg enables multi-object R-VOS, runs about 3 times faster while maintaining satisfactory performance. Code is available at https://github.com/bo-miao/SgMg.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Miao_Spectrum-guided_Multi-granularity_Referring_Video_Object_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Miao_Spectrum-guided_Multi-granularity_Referring_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Bo Miao</author><author>Mohammed Bennamoun</author><author>Yongsheng Gao</author><author>Ajmal Mian</author>
            </authors>
        </paper>
        

        <paper>
            <title>Sound Source Localization is All about Cross-Modal Alignment</title>
            <abstract>Humans can easily perceive the direction of sound sources in a visual scene, termed sound source localization. Recent studies on learning-based sound source localization have mainly explored the problem from a localization perspective. However, prior arts and existing benchmarks do not account for a more important aspect of the problem, cross-modal semantic understanding, which is essential for genuine sound source localization. Cross-modal semantic understanding is important in understanding semantically mismatched audio-visual events, e.g., silent objects, or off-screen sounds. To account for this, we propose a cross-modal alignment task as a joint task with sound source localization to better learn the interaction between audio and visual modalities. Thereby, we achieve high localization performance with strong cross-modal semantic understanding. Our method outperforms the state-of-the-art approaches in both sound source localization and cross-modal retrieval. Our work suggests that jointly tackling both tasks is necessary to conquer genuine sound source localization.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Senocak_Sound_Source_Localization_is_All_about_Cross-Modal_Alignment_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Arda Senocak</author><author>Hyeonggon Ryu</author><author>Junsik Kim</author><author>Tae-Hyun Oh</author><author>Hanspeter Pfister</author><author>Joon Son Chung</author>
            </authors>
        </paper>
        

        <paper>
            <title>BlendFace: Re-designing Identity Encoders for Face-Swapping</title>
            <abstract>The great advancements of generative adversarial networks and face recognition models in computer vision have made it possible to swap identities on images from single sources. Although a lot of studies seems to have proposed almost satisfactory solutions, we notice previous methods still suffer from an identity-attribute entanglement that causes undesired attributes swapping because widely used identity encoders, e.g., ArcFace, have some crucial attribute biases owing to their pretraining on face recognition tasks. To address this issue, we design BlendFace, a novel identity encoder for face-swapping. The key idea behind BlendFace is training face recognition models on blended images whose attributes are replaced with those of another mitigates inter-personal biases such as hairsyles and head shapes. BlendFace feeds disentangled identity features into generators and guides generators properly as an identity loss function. Extensive experiments demonstrate that BlendFace improves the identity-attribute disentanglement in face-swapping models, maintaining a comparable quantitative performance to previous methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shiohara_BlendFace_Re-designing_Identity_Encoders_for_Face-Swapping_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shiohara_BlendFace_Re-designing_Identity_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kaede Shiohara</author><author>Xingchao Yang</author><author>Takafumi Taketomi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Test-time Personalizable Forecasting of 3D Human Poses</title>
            <abstract>Current motion forecasting approaches typically train a deep end-to-end model from the source domain data, and then apply it directly to target subjects. Despite promising results, they remain non-optimal, due to privacy considerations, the test person and his/her natural properties (e.g., stature, behavioral trait) are typically unseen/absent in training. In this case, the source pre-trained model has a low ability to adapt to these out-of-source characteristics, resulting in an unreliable prediction. To tackle this issue, we propose a novel helper-predictor test-time personalization approach (H/P-TTP), which allows for a generalizable representation of out-of-source subjects to gain more realistic predictions. Concretely, the helper is preceded by explicit and implicit augmenters, where the former yields noisy sequences to improve robustness, while the latter is to generate novel-domain data with an adversarial learning paradigm. Then, the domain-generalizable learning is achieved where the helper can extract cross-subject invariant-knowledge to update the predictor. At test time, given a new person, the predictor is able to be further optimized to empower personalized capabilities to the specific properties. Under several benchmarks, extensive experiments show that with H/P-TTP, the existing predictive models are significantly improved for various unseen subjects.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cui_Test-time_Personalizable_Forecasting_of_3D_Human_Poses_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cui_Test-time_Personalizable_Forecasting_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiongjie Cui</author><author>Huaijiang Sun</author><author>Jianfeng Lu</author><author>Weiqing Li</author><author>Bin Li</author><author>Hongwei Yi</author><author>Haofan Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DreamBooth3D: Subject-Driven Text-to-3D Generation</title>
            <abstract>We present DreamBooth3D, an approach to personalize text-to-3D generative models from as few as 3-6 casually captured images of a subject. Our approach combines recent advances in personalizing text-to-image models (DreamBooth) with text-to-3D generation (DreamFusion). We find that naively combining these methods fails to yield satisfactory subject-specific 3D assets due to personalized text-to-image models overfitting to the input viewpoints of the subject. We overcome this through a 3-stage optimization strategy where we jointly leverage the 3D consistency of neural radiance fields together with the personalization capability of text-to-image models. Our method can produce high-quality, subject-specific 3D assets with text-driven modifications such as novel poses, colors and attributes that are not seen in any of the input images of the subject.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Amit Raj</author><author>Srinivas Kaza</author><author>Ben Poole</author><author>Michael Niemeyer</author><author>Nataniel Ruiz</author><author>Ben Mildenhall</author><author>Shiran Zada</author><author>Kfir Aberman</author><author>Michael Rubinstein</author><author>Jonathan Barron</author><author>Yuanzhen Li</author><author>Varun Jampani</author>
            </authors>
        </paper>
        

        <paper>
            <title>Dynamic Snake Convolution Based on Topological Geometric Constraints for Tubular Structure Segmentation</title>
            <abstract>Accurate segmentation of topological tubular structures, such as blood vessels and roads, is crucial in various fields, ensuring accuracy and efficiency in downstream tasks. However, many factors complicate the task, including thin local structures and variable global morphologies. In this work, we note the specificity of tubular structures and use this knowledge to guide our DSCNet to simultaneously enhance perception in three stages: feature extraction, feature fusion, and loss constraint. First, we propose a dynamic snake convolution to accurately capture the features of tubular structures by adaptively focusing on slender and tortuous local structures. Subsequently, we propose a multi-view feature fusion strategy to complement the attention to features from multiple perspectives during feature fusion, ensuring the retention of important information from different global morphologies. Finally, a continuity constraint loss function, based on persistent homology, is proposed to constrain the topological continuity of the segmentation better. Experiments on 2D and 3D datasets show that our DSCNet provides better accuracy and continuity on the tubular structure segmentation task compared with several methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qi_Dynamic_Snake_Convolution_Based_on_Topological_Geometric_Constraints_for_Tubular_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Qi_Dynamic_Snake_Convolution_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yaolei Qi</author><author>Yuting He</author><author>Xiaoming Qi</author><author>Yuan Zhang</author><author>Guanyu Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning to Upsample by Learning to Sample</title>
            <abstract>We present DySample, an ultra-lightweight and effective dynamic upsampler. While impressive performance gains have been witnessed from recent kernel-based dynamic upsamplers such as CARAFE, FADE, and SAPA, they introduce much workload, mostly due to the time-consuming dynamic convolution and the additional sub-network used to generate dynamic kernels. Further, the need for high-res feature guidance of FADE and SAPA somehow limits their application scenarios. To address these concerns, we bypass dynamic convolution and formulate upsampling from the perspective of point sampling, which is more resource-efficient and can be easily implemented with the standard built-in function in PyTorch. We first showcase a naive design, and then demonstrate how to strengthen its upsampling behavior step by step towards our new upsampler, DySample. Compared with former kernel-based dynamic upsamplers, DySample requires no customized CUDA package and has much fewer parameters, FLOPs, GPU memory, and latency. Besides the light-weight characteristics, DySample outperforms other upsamplers across five dense prediction tasks, including semantic segmentation, object detection, instance segmentation, panoptic segmentation, and monocular depth estimation. Code is available at https://github.com/tiny-smart/dysample.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Learning_to_Upsample_by_Learning_to_Sample_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Wenze Liu</author><author>Hao Lu</author><author>Hongtao Fu</author><author>Zhiguo Cao</author>
            </authors>
        </paper>
        

        <paper>
            <title>LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models</title>
            <abstract>Creating graphic layouts is a fundamental step in graphic designs. In this work, we present a novel generative model named LayoutDiffusion for automatic layout generation. As layout is typically represented as a sequence of discrete tokens, LayoutDiffusion models layout generation as a discrete denoising diffusion process. It learns to reverse a mild forward process, in which layouts become increasingly chaotic with the growth of forward steps and layouts in the neighboring steps do not differ too much. Designing such a mild forward process is however very challenging as layout has both categorical attributes and ordinal attributes. To tackle the challenge, we summarize three critical factors for achieving a mild forward process for the layout, i.e., legality, coordinate proximity and type disruption. Based on the factors, we propose a block-wise transition matrix coupled with a piece-wise linear noise schedule. Experiments on RICO and PubLayNet datasets show that LayoutDiffusion outperforms state-of-the-art approaches significantly. Moreover, it enables two conditional layout generation tasks in a plug-and-play manner without re-training and achieves better performance than existing methods. Project page: https://layoutdiffusion.github.io.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_LayoutDiffusion_Improving_Graphic_Layout_Generation_by_Discrete_Diffusion_Probabilistic_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_LayoutDiffusion_Improving_Graphic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junyi Zhang</author><author>Jiaqi Guo</author><author>Shizhao Sun</author><author>Jian-Guang Lou</author><author>Dongmei Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Efficiently Robustify Pre-Trained Models</title>
            <abstract>A recent trend in deep learning algorithms has been towards training large scale models, having high parameter count and trained on big dataset. However, robustness of such large scale models towards real-world settings is still a less-explored topic. In this work, we first benchmark the performance of these models under different perturbations and datasets thereby representing real-world shifts, and highlight their degrading performance under these shifts. We then discuss on how complete model fine-tuning based existing robustification schemes might not be a scalable option given very large scale networks and can also lead them to forget some of the desired characterstics. Finally, we propose a simple and cost-effective method to solve this problem, inspired by knowledge transfer literature. It involves robustifying smaller models, at a lower computation cost, and then use them as teachers to tune a fraction of these large scale networks, reducing the overall computational overhead. We evaluate our proposed method under various vision perturbations including ImageNet-C,R,S,A datasets and also for transfer learning, zero-shot evaluation setups on different datasets. Benchmark results show that our method is able to induce robustness to these large scale models efficiently, requiring significantly lower time and also preserves the transfer learning, zero-shot properties of the original model which none of the existing methods are able to achieve.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jain_Efficiently_Robustify_Pre-Trained_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jain_Efficiently_Robustify_Pre-Trained_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nishant Jain</author><author>Harkirat Behl</author><author>Yogesh Singh Rawat</author><author>Vibhav Vineet</author>
            </authors>
        </paper>
        

        <paper>
            <title>XMem++: Production-level Video Segmentation From Few Annotated Frames</title>
            <abstract>Despite advancements in user-guided video segmentation, extracting complex objects consistently for highly complex scenes is still a labor-intensive task, especially for production. It is not uncommon that a majority of frames need to be annotated. We introduce a novel semi-supervised video object segmentation (SSVOS) model, XMem++, that improves existing memory-based models, with a permanent memory module. Most existing methods focus on single frame annotations, while our approach can effectively handle multiple user-selected frames with varying appearances of the same object or region. Our method can extract highly consistent results while keeping the required number of frame annotations low. We further introduce an iterative and attention-based frame suggestion mechanism, which computes the next best frame for annotation. Our method is real-time and does not require retraining after each user input. We also introduce a new dataset, PUMaVOS, which covers new challenging use cases not found in previous benchmarks. We demonstrate SOTA performance on challenging (partial and multi-class) segmentation scenarios as well as long videos, while ensuring significantly fewer frame annotations than any existing method. Project page: https://max810.github.io/xmem2-project-page/</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bekuzarov_XMem_Production-level_Video_Segmentation_From_Few_Annotated_Frames_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bekuzarov_XMem_Production-level_Video_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Maksym Bekuzarov</author><author>Ariana Bermudez</author><author>Joon-Young Lee</author><author>Hao Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>End-to-End Diffusion Latent Optimization Improves Classifier Guidance</title>
            <abstract>Classifier guidance---using the gradients of an image classifier to steer the generations of a diffusion model---has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation&apos;s shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidance: using CLIP guidance to improve generations of complex prompts from DrawBench, using fine-grained visual classifiers to expand the vocabulary of Stable Diffusion, enabling image-conditioned generation with a CLIP visual encoder, and improving image aesthetics using an aesthetic scoring network.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wallace_End-to-End_Diffusion_Latent_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Bram Wallace</author><author>Akash Gokul</author><author>Stefano Ermon</author><author>Nikhil Naik</author>
            </authors>
        </paper>
        

        <paper>
            <title>TRM-UAP: Enhancing the Transferability of Data-Free Universal Adversarial Perturbation via Truncated Ratio Maximization</title>
            <abstract>Aiming at crafting a single universal adversarial perturbation (UAP) to fool CNN models for various data samples, universal attack enables a more efficient and accurate evaluation for the robustness of CNN models. Early universal attacks craft UAPs depending on data priors. For more practical applications, the data-free universal attacks that make UAPs from random noises have aroused much attention recently. However, existing data-free UAP methods perturb all the CNN feature layers equally via the maximization of the CNN activation, leading to poor transferability. In this paper, we propose a novel data-free universal attack without depending on any real data samples through truncated ratio maximization, which we term as TRM-UAP. Specifically, different from the maximization of the positive activation in convolution layers, we propose to optimize the UAP generation from the ratio of positive and negative activations. To further enhance the transferability of universal attack, TRM-UAP not only performs the ratio maximization merely on low-level generic features via the truncation strategy, but also incorporates a curriculum optimization algorithm that can effectively learn the diversity of artificial images. Extensive experiments on the ImageNet dataset verify that TRM-UAP achieves a state-of-the-art average fooling rate and excellent transferability on different CNN models as compared to other data-free UAP methods. Code is available at https://github.com/RandolphCarter0/TRMUAP.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_TRM-UAP_Enhancing_the_Transferability_of_Data-Free_Universal_Adversarial_Perturbation_via_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_TRM-UAP_Enhancing_the_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yiran Liu</author><author>Xin Feng</author><author>Yunlong Wang</author><author>Wu Yang</author><author>Di Ming</author>
            </authors>
        </paper>
        

        <paper>
            <title>Scratching Visual Transformer&apos;s Back with Uniform Attention</title>
            <abstract>The favorable performance of Vision Transformers (ViTs) is often attributed to the multi-head self-attention (MSA), which enables global interactions at each layer of a ViT model. Previous works acknowledge the property of long-range dependency for the effectiveness in MSA. In this work, we study the role of MSA in terms of the different axis, density. Our preliminary analyses suggest that the spatial interactions of learned attention maps are close to dense interactions rather than sparse ones. This is a curious phenomenon because dense attention maps are harder for the model to learn due to softmax. We interpret this opposite behavior against softmax as a strong preference for the ViT models to include dense interaction. We thus manually insert the dense uniform attention to each layer of the ViT models to supply the much-needed dense interactions. We call this method Context Broadcasting, CB. Our study demonstrates the inclusion of CB takes the role of dense attention, and thereby reduces the degree of density in the original attention maps by complying softmax in MSA. We also show that, with negligible costs of CB (1 line in your model code and no additional parameters), both the capacity and generalizability of the ViT models are increased.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hyeon-Woo_Scratching_Visual_Transformers_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nam Hyeon-Woo</author><author>Kim Yu-Ji</author><author>Byeongho Heo</author><author>Dongyoon Han</author><author>Seong Joon Oh</author><author>Tae-Hyun Oh</author>
            </authors>
        </paper>
        

        <paper>
            <title>Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</title>
            <abstract>To replicate the success of text-to-image (T2I) generation, recent works employ large-scale video datasets to train a text-to-video (T2V) generator. Despite their promising results, such paradigm is computationally expensive. In this work, we propose a new T2V generation setting--One-Shot Video Tuning, where only one text-video pair is presented. Our model is built on state-of-the-art T2I diffusion models pre-trained on massive image data. We make two key observations: 1) T2I models can generate still images that represent verb terms; 2) extending T2I models to generate multiple images concurrently exhibits surprisingly good content consistency. To further learn continuous motion, we introduce Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy. At inference, we employ DDIM inversion to provide structure guidance for sampling. Extensive qualitative and numerical experiments demonstrate the remarkable ability of our method across various applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jay Zhangjie Wu</author><author>Yixiao Ge</author><author>Xintao Wang</author><author>Stan Weixian Lei</author><author>Yuchao Gu</author><author>Yufei Shi</author><author>Wynne Hsu</author><author>Ying Shan</author><author>Xiaohu Qie</author><author>Mike Zheng Shou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection</title>
            <abstract>Anchor-based detectors have been continuously developed for object detection. However, the individual anchor box makes it difficult to predict the boundary&apos;s offset accurately. Instead of taking each bounding box as a closed individual, we consider using multiple boxes together to get prediction boxes. To this end, this paper proposes the Box Decouple-Couple(BDC) strategy in the inference, which no longer discards the overlapping boxes, but decouples the corner points of these boxes. Then, according to each corner&apos;s score, we couple the corner points to select the most accurate corner pairs. To meet the BDC strategy, a simple but novel model is designed named the Anchor-Intermediate Detector(AID), which contains two head networks, i.e., an anchor-based head and an anchor-free Corner-aware head. The corner-aware head is able to score the corners of each bounding box to facilitate the coupling between corner points. Extensive experiments on MS COCO show that the proposed anchor-intermediate detector respectively outperforms their baseline RetinaNet and GFL method by 2.4 and 1.2 AP on the MS COCO test-dev dataset without any bells and whistles.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lv_Anchor-Intermediate_Detector_Decoupling_and_Coupling_Bounding_Boxes_for_Accurate_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yilong Lv</author><author>Min Li</author><author>Yujie He</author><author>Shaopeng Li</author><author>Zhuzhen He</author><author>Aitao Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Extensible and Efficient Proxy for Neural Architecture Search</title>
            <abstract>Efficient or near-zero-cost proxies were proposed recently to address the demanding computational issues of Neural Architecture Search (NAS) in designing deep neural networks (DNNs), where each candidate architecture network only requires one iteration of backpropagation. The values obtained from proxies are used as predictions of architecture performance for downstream tasks. However, two significant drawbacks hinder the wide adoption of these efficient proxies: 1. they are not adaptive to various NAS search spaces and 2. they are not extensible to multi-modality downstream tasks. To address these two issues, we first propose an Extensible proxy (Eproxy) that utilizes self-supervised, few-shot training to achieve near-zero costs. A key component to our Eproxy&apos;s efficiency is the introduction of a barrier layer with randomly initialized frozen convolution parameters, which adds non-linearities to the optimization spaces so that Eproxy can discriminate the performance of architectures at an early stage. We further propose a Discrete Proxy Search (DPS) method to find the optimized training settings for Eproxy with only a handful of benchmarked architectures on the target tasks. Our extensive experiments confirm the effectiveness of both Eproxy and DPS. On the NDS-ImageNet search spaces, Eproxy+DPS achieves a higher average ranking correlation (Spearman r = 0.73) than the previous efficient proxy (Spearman r = 0.56). On the NAS-Bench-Trans-Micro search spaces with seven tasks, Eproxy+DPS delivers comparable performance with the early stopping method (146x faster). For the end-to-end task such as DARTS-ImageNet-1k, our method delivers better results than NAS performed on CIFAR-10 while only requiring one GPU hour with a single batch of CIFAR-10 images.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Extensible_and_Efficient_Proxy_for_Neural_Architecture_Search_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Extensible_and_Efficient_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuhong Li</author><author>Jiajie Li</author><author>Cong Hao</author><author>Pan Li</author><author>Jinjun Xiong</author><author>Deming Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>MAAL: Multimodality-Aware Autoencoder-Based Affordance Learning for 3D Articulated Objects</title>
            <abstract>Inferring affordance for 3D articulated objects is a challenging and practical problem. It is a primary problem for applying robots to real-world scenarios. The exploration can be summarized as figuring out where to act and how to act. Correspondingly, the task mainly requires producing actionability scores, action proposals, and success likelihood scores according to the given 3D object information and robotic information. Current works usually directly process multi-modal inputs with early fusion and apply critic networks to produce scores, which leads to insufficient multi-modal learning ability and inefficiently iterative training in multiple stages. This paper proposes a novel Multimodality-Aware Autoencoder-based affordance Learning (MAAL) for the 3D object affordance problem. It is an efficient pipeline, trained in one go, and only requires a few positive samples in training data. More importantly, MAAL contains a MultiModal Energized Encoder (MME) for better multi-modal learning. It comprehensively models all multi-modal inputs from 3D objects and robotic actions. Jointly considering information from multiple modalities, the encoder further learns interactions between robots and objects. MME empowers the better multi-modal learning ability for understanding object affordance. Experimental results and visualizations, based on a large-scale dataset PartNet-Mobility, show the effectiveness of MAAL in learning multi-modal data and solving the 3D articulated object affordance problem.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_Affordance_Learning_for_3D_Articulated_Objects_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liang_MAAL_Multimodality-Aware_Autoencoder-Based_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuanzhi Liang</author><author>Xiaohan Wang</author><author>Linchao Zhu</author><author>Yi Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples</title>
            <abstract>Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are hard to counter unseen point cloud adversarial examples. In this paper, we first establish a comprehensive, and rigorous point cloud adversarial robustness benchmark to evaluate adversarial robustness, which can provide a detailed understanding of the effects of the defense and attack methods. We then collect existing defense tricks in point cloud adversarial defenses and then perform extensive and systematic experiments to identify an effective combination of these tricks. Furthermore, we propose a hybrid training augmentation methods that consider various types of point cloud adversarial examples to adversarial training, significantly improving the adversarial robustness. By combining these tricks, we construct a more robust defense framework achieving an average accuracy of 83.45% against various attacks, demonstrating its capability to enabling robust learners. Our codebase are open-sourced on https://github.com/qiufan319/benchmark_pc_attack.git</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ji_Benchmarking_and_Analyzing_Robust_Point_Cloud_Recognition_Bag_of_Tricks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ji_Benchmarking_and_Analyzing_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiufan Ji</author><author>Lin Wang</author><author>Cong Shi</author><author>Shengshan Hu</author><author>Yingying Chen</author><author>Lichao Sun</author>
            </authors>
        </paper>
        

        <paper>
            <title>Poincare ResNet</title>
            <abstract>This paper introduces an end-to-end residual network that operates entirely on the Poincare ball model of hyperbolic space. Hyperbolic learning has recently shown great potential for visual understanding, but is currently only performed in the penultimate layer(s) of deep networks. All visual representations are still learned through standard Euclidean networks. In this paper we investigate how to learn hyperbolic representations of visual data directly from the pixel-level. We propose Poincare ResNet, a hyperbolic counterpart of the celebrated residual network, starting from Poincare 2D convolutions up to Poincare residual connections. We identify three roadblocks for training convolutional networks entirely in hyperbolic space and propose a solution for each: (i) Current hyperbolic network initializations collapse to the origin, limiting their applicability in deeper networks. We provide an identity-based initialization that preserves norms over many layers. (ii) Residual networks rely heavily on batch normalization, which comes with expensive Frechet mean calculations in hyperbolic space. We introduce Poincare midpoint batch normalization as a faster and equally effective alternative. (iii) Due to the many intermediate operations in Poincare layers, the computation graphs of deep learning libraries blow up, limiting our ability to train on deep hyperbolic networks. We provide manual backward derivations of core hyperbolic operations to maintain manageable computation graphs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/van_Spengler_Poincare_ResNet_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/van_Spengler_Poincare_ResNet_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Max van Spengler</author><author>Erwin Berkhout</author><author>Pascal Mettes</author>
            </authors>
        </paper>
        

        <paper>
            <title>Subclass-balancing Contrastive Learning for Long-tailed Recognition</title>
            <abstract>Long-tailed recognition with imbalanced class distribution naturally emerges in practical machine learning applications. Existing methods such as data reweighing, resampling, and supervised contrastive learning enforce the class balance with a price of introducing imbalance between instances of head class and tail class, which may ignore the underlying rich semantic substructures of the former and exaggerate the biases in the latter. We overcome these drawbacks by a novel &quot;subclass-balancing contrastive learning (SBCL)&quot; approach that clusters each head class into multiple subclasses of similar sizes as the tail classes and enforce representations to capture the two-layer class hierarchy between the original classes and their subclasses. Since the clustering is conducted in the representation space and updated during the course of training, the subclass labels preserve the semantic substructures of head classes. Meanwhile, it does not overemphasize tail class samples, so each individual instance contribute to the representation learning equally. Hence, our method achieves both the instance- and subclass-balance, while the original class labels are also learned through contrastive learning among subclasses from different classes. We evaluate SBCL over a list of long-tailed benchmark datasets and it achieves the state-of-the-art performance. In addition, we present extensive analyses and ablation studies of SBCL to verify its advantages.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hou_Subclass-balancing_Contrastive_Learning_for_Long-tailed_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hou_Subclass-balancing_Contrastive_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chengkai Hou</author><author>Jieyu Zhang</author><author>Haonan Wang</author><author>Tianyi Zhou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Dynamic Mesh-Aware Radiance Fields</title>
            <abstract>Embedding polygonal mesh assets within photorealistic Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the traditional graphics pipeline. This paper designs a two-way coupling between mesh and NeRF during rendering and simulation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrepancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) images. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance physics simulator that supports cloth, rigid and soft bodies. The full rendering and simulation system can be run on a GPU at interactive rates. We show that a hybrid system approach outperforms alternatives in visual realism for mesh insertion, because it allows realistic light transport from volumetric NeRF media onto surfaces, which affects the appearance of reflective/refractive surfaces and illumination of diffuse surfaces informed by the dynamic scene.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qiao_Dynamic_Mesh-Aware_Radiance_Fields_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Qiao_Dynamic_Mesh-Aware_Radiance_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yi-Ling Qiao</author><author>Alexander Gao</author><author>Yiran Xu</author><author>Yue Feng</author><author>Jia-Bin Huang</author><author>Ming C. Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Support and Trivial Prototypes for Interpretable Image Classification</title>
            <abstract>Prototypical part network (ProtoPNet) methods have been designed to achieve interpretable classification by associating predictions with a set of training prototypes, which we refer to as trivial prototypes because they are trained to lie far from the classification boundary in the feature space. Note that it is possible to make an analogy between ProtoPNet and support vector machine (SVM) given that the classification from both methods relies on computing similarity with a set of training points (i.e., trivial prototypes in ProtoPNet, and support vectors in SVM). However, while trivial prototypes are located far from the classification boundary, support vectors are located close to this boundary, and we argue that this discrepancy with the well-established SVM theory can result in ProtoPNet models with inferior classification accuracy. In this paper, we aim to improve the classification of ProtoPNet with a new method to learn support prototypes that lie near the classification boundary in the feature space, as suggested by the SVM theory. In addition, we target the improvement of classification results with a new model, named ST-ProtoPNet, which exploits our support prototypes and the trivial prototypes to provide more effective classification. Experimental results on CUB-200-2011, Stanford Cars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achieves state-of-the-art classification accuracy and interpretability results. We also show that the proposed support prototypes tend to be better localised in the object of interest rather than in the background region. Code is available at https://github.com/cwangrun/ST-ProtoPNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Learning_Support_and_Trivial_Prototypes_for_Interpretable_Image_Classification_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Learning_Support_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chong Wang</author><author>Yuyuan Liu</author><author>Yuanhong Chen</author><author>Fengbei Liu</author><author>Yu Tian</author><author>Davis McCarthy</author><author>Helen Frazer</author><author>Gustavo Carneiro</author>
            </authors>
        </paper>
        

        <paper>
            <title>Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection</title>
            <abstract>The introduction of DETR represents a new paradigm for object detection. However, its decoder conducts classification and box localization using shared queries and cross-attention layers, leading to suboptimal results. We observe that different regions of interest in the visual feature map are suitable for performing query classification and box localization tasks, even for the same object. Salient regions provide vital information for classification, while the boundaries around them are more favorable for box regression. Unfortunately, such spatial misalignment between these two tasks greatly hinders DETR&apos;s training. Therefore, in this work, we focus on decoupling localization and classification tasks in DETR. To achieve this, we introduce a new design scheme called spatially decoupled DETR (SD-DETR), which includes a task-aware query generation module and a disentangled feature learning process. We elaborately design the task-aware query initialization process and divide the cross-attention block in the decoder to allow the task-aware queries to match different visual regions. Meanwhile, we also observe that the prediction misalignment problem for high classification confidence and precise localization exists, so we propose an alignment loss to further guide the spatially decoupled DETR training. Through extensive experiments, we demonstrate that our approach achieves a significant improvement in MSCOCO datasets compared to previous work. For instance, we improve the performance of Conditional DETR by 4.5%. By spatially disentangling the two tasks, our method overcomes the misalignment problem and greatly improves the performance of DETR for object detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Decoupled_DETR_Spatially_Disentangling_Localization_and_Classification_for_Improved_End-to-End_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Manyuan Zhang</author><author>Guanglu Song</author><author>Yu Liu</author><author>Hongsheng Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization</title>
            <abstract>Federated Learning (FL) has recently emerged as a promising distributed machine learning framework to preserve clients&apos; privacy, by allowing multiple clients to upload the gradients calculated from their local data to a central server. Recent studies find that the exchanged gradients also take the risk of privacy leakage, e.g., an attacker can invert the shared gradients and recover sensitive data against an FL system by leveraging pre-trained generative adversarial networks (GAN) as prior knowledge. However, performing gradient inversion attacks in the latent space of the GAN model limits their expression ability and generalizability. To tackle these challenges, we propose Gradient Inversion over Feature Domains (GIFD), which disassembles the GAN model and searches the feature domains of the intermediate layers. Instead of optimizing only over the initial latent code, we progressively change the optimized layer, from the initial latent space to intermediate layers closer to the output images. In addition, we design a regularizer to avoid unreal image generation by adding a small l1 ball constraint to the searching range. We also extend GIFD to the out-of-distribution (OOD) setting, which weakens the assumption that the training sets of GANs and FL tasks obey the same data distribution. Extensive experiments demonstrate that our method can achieve pixel-level reconstruction and is superior to the existing methods. Notably, GIFD also shows great generalizability under different defense strategy settings and batch sizes.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fang_GIFD_A_Generative_Gradient_Inversion_Method_with_Feature_Domain_Optimization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fang_GIFD_A_Generative_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hao Fang</author><author>Bin Chen</author><author>Xuan Wang</author><author>Zhi Wang</author><author>Shu-Tao Xia</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generalized Sum Pooling for Metric Learning</title>
            <abstract>A common architectural choice for deep metric learning is a convolutional neural network followed by global average pooling (GAP). Albeit simple, GAP is a highly effective way to aggregate information. One possible explanation for the effectiveness of GAP is considering each feature vector as representing a different semantic entity and GAP as a convex combination of them. Following this perspective, we generalize GAP and propose a learnable generalized sum pooling method (GSP). GSP improves GAP with two distinct abilities: i) the ability to choose a subset of semantic entities, effectively learning to ignore nuisance information, and ii) learning the weights corresponding to the importance of each entity. Formally, we propose an entropy-smoothed optimal transport problem and show that it is a strict generalization of GAP, i.e., a specific realization of the problem gives back GAP. We show that this optimization problem enjoys analytical gradients enabling us to use it as a direct learnable replacement for GAP. We further propose a zero-shot loss to ease the learning of GSP. We show the effectiveness of our method with extensive evaluations on 4 popular metric learning benchmarks. Code is available at: GSP-DML Framework</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gurbuz_Generalized_Sum_Pooling_for_Metric_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gurbuz_Generalized_Sum_Pooling_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yeti Z. G rb z</author><author>Ozan Sener</author><author>A. Aydin Alatan</author>
            </authors>
        </paper>
        

        <paper>
            <title>AlignDet: Aligning Pre-training and Fine-tuning in Object Detection</title>
            <abstract>The paradigm of large-scale pre-training followed by downstream fine-tuning has been widely employed in various object detection algorithms. In this paper, we reveal discrepancies in data, model, and task between the pre-training and fine-tuning procedure in existing practices, which implicitly limit the detector&apos;s performance, generalization ability, and convergence speed. To this end, we propose AlignDet, a unified pre-training framework that can be adapted to various existing detectors to alleviate the discrepancies. AlignDet decouples the pre-training process into two stages, i.e., image-domain and box-domain pre-training. The image-domain pre-training optimizes the detection backbone to capture holistic visual abstraction, and box-domain pre-training learns instance-level semantics and task-aware concepts to initialize the parts out of the backbone. By incorporating the self-supervised pre-trained backbones, we can pre-train all modules for various detectors in an unsupervised paradigm. As depicted in Figure 1, extensive experiments demonstrate that AlignDet can achieve significant improvements across diverse protocols, such as detection algorithm, model backbone, data setting, and training schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by 2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_AlignDet_Aligning_Pre-training_and_Fine-tuning_in_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_AlignDet_Aligning_Pre-training_and_Fine-tuning_in_Object_Detection_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ming Li</author><author>Jie Wu</author><author>Xionghui Wang</author><author>Chen Chen</author><author>Jie Qin</author><author>Xuefeng Xiao</author><author>Rui Wang</author><author>Min Zheng</author><author>Xin Pan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Dense Text-to-Image Generation with Attention Modulation</title>
            <abstract>Existing text-to-image diffusion models struggle to synthesize realistic images given dense captions, where each text prompt provides a detailed description for a specific image region. To address this, we propose DenseDiffusion, a training-free method that adapts a pre-trained text-to-image model to handle such dense captions while offering control over the scene layout. We first analyze the relationship between generated images&apos; layouts and the pre-trained model&apos;s intermediate attention maps. Next, we develop an attention modulation method that guides objects to appear in specific regions according to layout guidance. Without requiring additional fine-tuning or datasets, we improve image generation performance given dense captions regarding both automatic and human evaluation scores. In addition, we achieve similar-quality visual results with models specifically trained with layout conditions.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yunji Kim</author><author>Jiyoung Lee</author><author>Jin-Hwa Kim</author><author>Jung-Woo Ha</author><author>Jun-Yan Zhu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Sentence Attention Blocks for Answer Grounding</title>
            <abstract>Answer grounding is the task of locating relevant visual evidence for the Visual Question Answering task. While a wide variety of attention methods have been introduced for this task, they suffer from the following three problems: designs that do not allow the usage of pre-trained networks and do not benefit from large data pre-training, custom designs that are not based on well-grounded previous designs, therefore limiting the learning power of the network, or complicated designs that make it challenging to re-implement or improve them. In this paper, we propose a novel architectural block, which we term Sentence Attention Block, to solve these problems. The proposed block re-calibrates channel-wise image feature-maps by explicitly modeling inter-dependencies between the image feature-maps and sentence embedding. We visually demonstrate how this block filters out irrelevant feature-maps channels based on sentence embedding. We start our design with a well-known attention method, and by making minor modifications, we improve the results to achieve state-of-the-art accuracy. The flexibility of our method makes it easy to use different pre-trained backbone networks, and its simplicity makes it easy to understand and be re-implemented. We demonstrate the effectiveness of our method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We perform multiple ablation studies to show the effectiveness of our design choices.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Khoshsirat_Sentence_Attention_Blocks_for_Answer_Grounding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Seyedalireza Khoshsirat</author><author>Chandra Kambhamettu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Fairness-aware Adversarial Network Pruning</title>
            <abstract>Network pruning aims to compress models while minimizing loss in accuracy. With the increasing focus on bias in AI systems, the bias inheriting or even magnification nature of traditional network pruning methods has raised a new perspective towards fairness-aware network pruning. Straightforward pruning plus debias methods and recent designs for monitoring disparities of demographic attributes during pruning have endeavored to enhance fairness in pruning. However, neither simple assembling of two tasks nor specifically designed pruning strategies could achieve the optimal trade-off among pruning ratio, accuracy, and fairness. This paper proposes an end-to-end learnable framework for fairness-aware network pruning, which optimizes both pruning and debias tasks jointly by adversarial training against those final evaluation metrics like accuracy for pruning, and disparate impact (DI) and equalized odds (DEO) for fairness. In other words, our fairness-aware adversarial pruning method would learn to prune without any handcraft rules. Therefore, our approach could flexibly adapt to variate network structures. Exhaustive experimentation demonstrates the generalization capacity of our approach, as well as superior performance on pruning and debias simultaneously. To highlight, the proposed method could preserve the SOTA pruning performance while significantly improving fairness by around 50% as compared to traditional pruning methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Towards_Fairness-aware_Adversarial_Network_Pruning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Lei Zhang</author><author>Zhibo Wang</author><author>Xiaowei Dong</author><author>Yunhe Feng</author><author>Xiaoyi Pang</author><author>Zhifei Zhang</author><author>Kui Ren</author>
            </authors>
        </paper>
        

        <paper>
            <title>Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations Using Image Models</title>
            <abstract>As video analysis using deep learning models becomes more widespread, the vulnerability of such models to adversarial attacks is becoming a pressing concern. In particular, Universal Adversarial Perturbation (UAP) poses a significant threat, as a single perturbation can mislead deep learning models on entire datasets. We propose a novel video UAP using image data and image model. This enables us to take advantage of the rich image data and image model-based studies available for video applications. However, there is a challenge that image models are limited in their ability to analyze the temporal aspects of videos, which is crucial for a successful video attack. To address this challenge, we introduce the Breaking Temporal Consistancy (BTC) method, which is the first attempt to incorporate temporal information into video attacks using image models. We aim to generate adversarial videos that have opposite patterns to the original. Specifically, BTC-UAP minimizes the feature similarity between neighboring frames in videos. Our approach is simple but effective at attacking unseen video models. Additionally, it is applicable to videos of varying lengths and invariant to temporal shifts. Our approach surpasses existing methods in terms of effectiveness on various datasets, including ImageNet, UCF-101, and Kinetics-400.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kim_Breaking_Temporal_Consistency_Generating_Video_Universal_Adversarial_Perturbations_Using_Image_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kim_Breaking_Temporal_Consistency_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hee-Seon Kim</author><author>Minji Son</author><author>Minbeom Kim</author><author>Myung-Joon Kwon</author><author>Changick Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Smoothness Similarity Regularization for Few-Shot GAN Adaptation</title>
            <abstract>The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model to a small dataset with very few training images. While existing methods perform well when the dataset for pre-training is structurally similar to the target dataset, the approaches suffer from training instabilities or memorization issues when the objects in the two domains have a very different structure. To mitigate this limitation, we propose a new smoothness similarity regularization that transfers the inherently learned smoothness of the pre-trained GAN to the few-shot target domain even if the two domains are very different. We evaluate our approach by adapting an unconditional and a class-conditional GAN to diverse few-shot target domains. Our proposed method significantly outperforms prior few-shot GAN adaptation methods in the challenging case of structurally dissimilar source-target domains, while performing on par with the state of the art for similar source-target domains.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sushko_Smoothness_Similarity_Regularization_for_Few-Shot_GAN_Adaptation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sushko_Smoothness_Similarity_Regularization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Vadim Sushko</author><author>Ruyu Wang</author><author>Juergen Gall</author>
            </authors>
        </paper>
        

        <paper>
            <title>Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding</title>
            <abstract>3D visual grounding involves finding a target object in a 3D scene that corresponds to a given sentence query. Although many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the problem that fine-grained annotated data is difficult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coarse scene-sentence correspondences are used to learn object-sentence links. To accomplish this, we design a novel semantic matching model that analyzes the semantic similarity between object proposals and sentences in a coarse-to-fine manner. Specifically, we first extract object proposals and coarsely select the top-K candidates based on feature and class similarity matrices. Next, we reconstruct the masked keywords of the sentence using each candidate one by one, and the reconstructed accuracy finely reflects the semantic similarity of each candidate to the query. Additionally, we distill the coarse-to-fine semantic matching knowledge into a typical two-stage 3D visual grounding model, which reduces inference costs and improves performance by taking full advantage of the well-studied structure of the existing architectures. We conduct extensive experiments on ScanRefer, Nr3D, and Sr3D, which demonstrate the effectiveness of our proposed method.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Distilling_Coarse-to-Fine_Semantic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zehan Wang</author><author>Haifeng Huang</author><author>Yang Zhao</author><author>Linjun Li</author><author>Xize Cheng</author><author>Yichen Zhu</author><author>Aoxiong Yin</author><author>Zhou Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>zPROBE: Zero Peek Robustness Checks for Federated Learning</title>
            <abstract>Privacy-preserving federated learning allows multiple users to jointly train a model with coordination of a central server. The server only learns the final aggregation result, thereby preventing leakage of the users&apos; (private) training data from the individual model updates. However, keeping the individual updates private allows malicious users to degrade the model accuracy without being detected, also known as Byzantine attacks. Best existing defenses against Byzantine workers rely on robust rank-based statistics, e.g., setting robust bounds via the median of updates, to find malicious updates. However, implementing privacy-preserving rank-based statistics, especially median-based, is nontrivial and unscalable in the secure domain, as it requires sorting of all individual updates. We establish the first private robustness check that uses high break point rank-based statistics on aggregated model updates. By exploiting randomized clustering, we significantly improve the scalability of our defense without compromising privacy. We leverage the derived statistical bounds in zero-knowledge proofs to detect and remove malicious updates without revealing the private user updates. Our novel framework, zPROBE, enables Byzantine resilient and secure federated learning. We show the effectiveness of zPROBE on several computer vision benchmarks. Empirical evaluations demonstrate that zPROBE provides a low overhead solution to defend against state-of-the-art Byzantine attacks while preserving privacy.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ghodsi_zPROBE_Zero_Peek_Robustness_Checks_for_Federated_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ghodsi_zPROBE_Zero_Peek_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zahra Ghodsi</author><author>Mojan Javaheripi</author><author>Nojan Sheybani</author><author>Xinqiao Zhang</author><author>Ke Huang</author><author>Farinaz Koushanfar</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generative Prompt Model for Weakly Supervised Object Localization</title>
            <abstract>Weakly supervised object localization (WSOL) remains challenging when learning object localization models from image category labels. Conventional methods that discriminatively train activation models ignore representative yet less discriminative object parts. In this study, we propose a generative prompt model (GenPromp), defining the first generative pipeline to localize less discriminative object parts by formulating WSOL as a conditional image denoising procedure. During training, GenPromp converts image category labels to learnable prompt embeddings which are fed to a generative model to conditionally recover the input image with noise and learn representative embeddings. During inference, GenPromp combines the representative embeddings with discriminative embeddings (queried from an off-the-shelf vision-language model) for both representative and discriminative capacity. The combined embeddings are finally used to generate multi-scale high-quality attention maps, which facilitate localizing full object extent. Experiments on CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline for WSOL with the generative model. Code is available at https://github.com/callsys/GenPromp.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhao_Generative_Prompt_Model_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuzhong Zhao</author><author>Qixiang Ye</author><author>Weijia Wu</author><author>Chunhua Shen</author><author>Fang Wan</author>
            </authors>
        </paper>
        

        <paper>
            <title>ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation</title>
            <abstract>We present a GAN-based Transformer for general action-conditioned 3D human motion generation, including not only single-person actions but also multi-person interactive actions. Our approach consists of a powerful Action-conditioned motion TransFormer (ActFormer) under a GAN training scheme, equipped with a Gaussian Process latent prior. Such a design combines the strong spatio-temporal representation capacity of Transformer, superiority in generative modeling of GAN, and inherent temporal correlations from the latent prior. Furthermore, ActFormer can be naturally extended to multi-person motions by alternately modeling temporal correlations and human interactions with Transformer encoders. To further facilitate research on multi-person motion generation, we introduce a new synthetic dataset of complex multi-person combat behaviors. Extensive experiments on NTU-13, NTU RGB+D 120, BABEL and the proposed combat dataset show that our method can adapt to various human motion representations and achieve superior performance over the state-of-the-art methods on both single-person and multi-person motion generation tasks, demonstrating a promising step towards a general human motion generator.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_ActFormer_A_GAN-based_Transformer_towards_General_Action-Conditioned_3D_Human_Motion_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_ActFormer_A_GAN-based_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Liang Xu</author><author>Ziyang Song</author><author>Dongliang Wang</author><author>Jing Su</author><author>Zhicheng Fang</author><author>Chenjing Ding</author><author>Weihao Gan</author><author>Yichao Yan</author><author>Xin Jin</author><author>Xiaokang Yang</author><author>Wenjun Zeng</author><author>Wei Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Hiding Visual Information via Obfuscating Adversarial Perturbations</title>
            <abstract>Growing leakage and misuse of visual information raise security and privacy concerns, which promotes the development of information protection. Existing adversarial perturbations-based methods mainly focus on the de-identification against deep learning models. However, the inherent visual information of the data has not been well protected. In this work, inspired by the Type-I adversarial attack, we propose an Adversarial Visual Information Hiding (AVIH) method to protect the visual privacy of data. Specifically, the method generates obfuscating adversarial perturbations to obscure the visual information of the data. Meanwhile, it maintains the hidden objectives to be correctly predicted by models. In addition, our method does not modify the parameters of the applied model, which makes it flexible for different scenarios. Experimental results on the recognition and classification tasks demonstrate that the proposed method can effectively hide visual information and hardly affect the performances of models. The code is available at https://github.com/suzhigangssz/AVIH.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Su_Hiding_Visual_Information_via_Obfuscating_Adversarial_Perturbations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Su_Hiding_Visual_Information_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhigang Su</author><author>Dawei Zhou</author><author>Nannan Wang</author><author>Decheng Liu</author><author>Zhen Wang</author><author>Xinbo Gao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Category-aware Allocation Transformer for Weakly Supervised Object Localization</title>
            <abstract>Weakly supervised object localization (WSOL) aims to localize objects based on only image-level labels as supervision. Recently, transformers have been introduced into WSOL, yielding impressive results. The self-attention mechanism and multilayer perceptron structure in transformers preserve long-range feature dependency, facilitating complete localization of the full object extent. However, current transformer-based methods predict bounding boxes using category-agnostic attention maps, which may lead to confused and noisy object localization. To address this issue, we propose a novel Category-aware Allocation TRansformer (CATR) that learns category-aware representations for specific objects and produces corresponding category-aware attention maps for object localization. First, we introduce a Category-aware Stimulation Module (CSM) to induce learnable category biases for self-attention maps, providing auxiliary supervision to guide the learning of more effective transformer representations. Second, we design an Object Constraint Module (OCM) to refine the object regions for the category-aware attention maps in a self-supervised manner. Extensive experiments on the CUB-200-2011 and ILSVRC datasets demonstrate that the proposed CATR achieves significant and consistent performance improvements over competing approaches.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Category-aware_Allocation_Transformer_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Category-aware_Allocation_Transformer_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhiwei Chen</author><author>Jinren Ding</author><author>Liujuan Cao</author><author>Yunhang Shen</author><author>Shengchuan Zhang</author><author>Guannan Jiang</author><author>Rongrong Ji</author>
            </authors>
        </paper>
        

        <paper>
            <title>Domain Specified Optimization for Deployment Authorization</title>
            <abstract>This paper explores Deployment Authorization (DPA) as a means of restricting the generalization capabilities of vision models on certain domains to protect intellectual property. Nevertheless, the current advancements in DPA are predominantly confined to fully supervised settings. Such settings require the accessibility of annotated images from any unauthorized domain, rendering the DPA approach impractical for real-world applications due to its exorbitant costs. To address this issue, we propose Source-Only Deployment Authorization (SDPA), which assumes that only authorized domains are accessible during training phases, and the model&apos;s performance on unauthorized domains must be suppressed in inference stages. Drawing inspiration from distributional robust statistics, we present a lightweight method called Domain-Specified Optimization (DSO) for SDPA that degrades the model&apos;s generalization over a divergence ball. DSO comes with theoretical guarantees on the convergence property and its authorization performance. As a complementary of SDPA, we also propose Target-Combined Deployment Authorization (TPDA), where unauthorized domains are partially accessible, and simplify the DSO method to a perturbation operation on the pseudo predictions, referred to as Target-Dependent Domain-Specified Optimization (TDSO). We demonstrate the effectiveness of our proposed DSO and TDSO methods through extensive experiments on six image benchmarks, achieving dominant performance on both SDPA and TDPA settings.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Domain_Specified_Optimization_for_Deployment_Authorization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Domain_Specified_Optimization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Haotian Wang</author><author>Haoang Chi</author><author>Wenjing Yang</author><author>Zhipeng Lin</author><author>Mingyang Geng</author><author>Long Lan</author><author>Jing Zhang</author><author>Dacheng Tao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Locally Stylized Neural Radiance Fields</title>
            <abstract>In recent years, there has been increasing interest in applying stylization on 3D scenes from a reference style image, in particular onto neural radiance fields (NeRF). While performing stylization directly on NeRF guarantees appearance consistency over arbitrary novel views, it is a challenging problem to guide the transfer of patterns from the style image onto different parts of the NeRF scene. In this work, we propose a stylization framework for NeRF based on local style transfer. In particular, we use a hash-grid encoding to learn the embedding of the appearance and geometry components, and show that the mapping defined by the hash table allows us to control the stylization to a certain extent. Stylization is then achieved by optimizing the appearance branch while keeping the geometry branch fixed. To support local style transfer, we propose a new loss function that utilizes a segmentation network and bipartite matching to establish region correspondences between the style image and the content images obtained from volume rendering. Our experiments show that our method yields plausible stylization results with novel view synthesis while having flexible controllability via manipulating and customizing the region correspondences.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pang_Locally_Stylized_Neural_Radiance_Fields_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pang_Locally_Stylized_Neural_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hong-Wing Pang</author><author>Binh-Son Hua</author><author>Sai-Kit Yeung</author>
            </authors>
        </paper>
        

        <paper>
            <title>Confidence-aware Pseudo-label Learning for Weakly Supervised Visual Grounding</title>
            <abstract>Visual grounding aims at localizing the target object in image which is most related to the given free-form natural language query. As labeling the position of target object is labor-intensive, the weakly supervised methods, where only image-sentence annotations are required during model training have recently received increasing attention. Most of the existing weakly-supervised methods first generate region proposals via pre-trained object detectors and then employ either cross-modal similarity score or reconstruction loss as the criteria to select proposal from them. However, due to the cross-modal heterogeneous gap, these method often suffer from high confidence spurious association and model prone to error propagation. In this paper, we propose Confidence-aware Pseudo-label Learning (CPL) to overcome the above limitations. Specifically, we first adopt both the uni-modal and cross-modal pre-trained models and propose conditional prompt engineering to automatically generate multiple `descriptive, realistic and diverse&apos; pseudo language queries for each region proposal, and then establish reliable cross-modal association for model training based on the uni-modal similarity score (between pseudo and real text queries). Secondly, we propose a confidence-aware pseudo label verification module which reduces the amount of noise encountered in the training process and the risk of error propagation. Experiments on five widely used datasets validate the efficacy of our proposed components and demonstrate state-of-the-art performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Confidence-aware_Pseudo-label_Learning_for_Weakly_Supervised_Visual_Grounding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yang Liu</author><author>Jiahua Zhang</author><author>Qingchao Chen</author><author>Yuxin Peng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Luminance-aware Color Transform for Multiple Exposure Correction</title>
            <abstract>Images captured with irregular exposures inevitably present unsatisfactory visual effects, such as distorted hue and color tone. However, most recent studies mainly focus on underexposure correction, which limits their applicability to real-world scenarios where exposure levels vary. Furthermore, some works to tackle multiple exposure rely on the encoder-decoder architecture, resulting in losses of details in input images during down-sampling and up-sampling processes. With this regard, a novel correction algorithm for multiple exposure, called luminance-aware color transform (LACT), is proposed in this study. First, we reason the relative exposure condition between images to obtain luminance features based on a luminance comparison module. Next, we encode the set of transformation functions from the luminance features, which enable complex color transformations for both overexposure and underexposure images. Finally, we project the transformed representation onto RGB color space to produce exposure correction results. Extensive experiments demonstrate that the proposed LACT yields new state-of-the-arts on two multiple exposure datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Baek_Luminance-aware_Color_Transform_for_Multiple_Exposure_Correction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Baek_Luminance-aware_Color_Transform_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jong-Hyeon Baek</author><author>DaeHyun Kim</author><author>Su-Min Choi</author><author>Hyo-jun Lee</author><author>Hanul Kim</author><author>Yeong Jun Koh</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Simple Framework for Open-Vocabulary Segmentation and Detection</title>
            <abstract>In this work, we present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotation granularity, we first introduce a pretrained text encoder to encode all the visual concepts in two tasks and learn a common semantic space for them. This gives us reasonably good results compared with the counterparts trained on segmentation task only. To further reconcile them, we locate two discrepancies: i) task discrepancy -- segmentation requires extracting masks for both foreground objects and background stuff, while detection merely cares about the former; ii) data discrepancy -- box and mask annotations are with different spatial granularity, and thus not directly interchangeable. We propose a decoupled foreground/background decoding and a conditioned mask decoding to address these issues, respectively. To this end, we develop a simple encoder-decoder model encompassing all three techniques and train it jointly on COCO and Objects365. After pretraining, our model exhibits competitive or stronger zero-shot transferability for both segmentation and detection. Specifically, OpenSeeD beats the state-of-the-art method for open-vocabulary instance and panoptic segmentation across 5 datasets, and outperforms previous work for open-vocabulary detection on LVIS and ODinW under similar settings. When transferred to specific tasks, our model achieves new SoTA on panoptic segmentation on COCO and ADE20K, and instance segmentation on ADE20K and Cityscapes. Finally, we note that OpenSeed is the first to explore the potential of joint training on segmentation and detection, and hope it can be received as a strong baseline for developing a single model for open-vocabulary segmentation and detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_A_Simple_Framework_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hao Zhang</author><author>Feng Li</author><author>Xueyan Zou</author><author>Shilong Liu</author><author>Chunyuan Li</author><author>Jianwei Yang</author><author>Lei Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Alignment Before Aggregation: Trajectory Memory Retrieval Network for Video Object Segmentation</title>
            <abstract>Memory-based methods in semi-supervised video object segmentation task achieve competitive performance by performing dense matching between query and memory frames. However, most of the existing methods neglect the fact that videos carry rich temporal information yet redundant spatial information. In this case, direct pixel-level global matching will lead to ambiguous correspondences. In this work, we reconcile the inherent tension of spatial and temporal information to retrieve memory frame information along the object trajectory, and propose a novel and coherent Trajectory Memory Retrieval Network (TMRN) to equip with the trajectory information, including a spatial alignment module and a temporal aggregation module. The proposed TMRN enjoys several merits. First, TMRN is empowered to characterize the temporal correspondence which is in line with the nature of video in a data-driven manner. Second, we elegantly customize the spatial alignment module by coupling SVD initialization with agent-level correlation for representative agent construction and rectifying false matches caused by direct pairwise pixel-level correlation, respectively. Extensive experimental results on challenging benchmarks including DAVIS 2017 validation / test and Youtube-VOS 2018 / 2019 demonstrate that our TMRN, as a general plugin module, achieves consistent improvements over several leading methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sun_Alignment_Before_Aggregation_Trajectory_Memory_Retrieval_Network_for_Video_Object_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sun_Alignment_Before_Aggregation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Rui Sun</author><author>Yuan Wang</author><author>Huayu Mai</author><author>Tianzhu Zhang</author><author>Feng Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Deep Directly-Trained Spiking Neural Networks for Object Detection</title>
            <abstract>Spiking neural networks (SNNs) are brain-inspired energy-efficient models that encode information in spatiotemporal dynamics. Recently, deep SNNs trained directly have shown great success in achieving high performance on classification tasks with very few time steps. However, how to design a directly-trained SNN for the regression task of object detection still remains a challenging problem. To address this problem, we propose EMS-YOLO, a novel directly-trained SNN framework for object detection, which is the first trial to train a deep SNN with surrogate gradients for object detection rather than ANN-SNN conversion strategies. Specifically, we design a full-spike residual block, EMS-ResNet, which can effectively extend the depth of the directly-trained SNN with low power consumption. Furthermore, we theoretically analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding. The results demonstrate that our approach outperforms the state-of-the-art ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time steps (only 4 time steps). It is shown that our model could achieve comparable performance to the ANN with the same architecture while consuming 5.83x less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset. Our code is available in https://github.com/BICLab/EMS-YOLO.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Su_Deep_Directly-Trained_Spiking_Neural_Networks_for_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Su_Deep_Directly-Trained_Spiking_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiaoyi Su</author><author>Yuhong Chou</author><author>Yifan Hu</author><author>Jianing Li</author><author>Shijie Mei</author><author>Ziyang Zhang</author><author>Guoqi Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Masked Autoencoders Are Stronger Knowledge Distillers</title>
            <abstract>Knowledge distillation (KD) has shown great success in improving student&apos;s performance by mimicking the intermediate output of the high-capacity teacher in fine-grained visual tasks, e.g. object detection. This paper proposes a technique called Masked Knowledge Distillation (MKD) that enhances this process using a masked autoencoding scheme. In MKD, random patches of the input image are masked, and the corresponding missing feature is recovered by forcing it to imitate the output of the teacher. MKD is based on two core designs. First, using the student as the encoder, we develop an adaptive decoder architecture, which includes a spatial alignment module that operates on the multi-scale features in the feature pyramid network (FPN), a simple decoder, and a spatial recovery module that mimics the teacher&apos;s output from the latent representation and mask tokens. Second, we introduce the masked convolution in each convolution block to keep the masked patches unaffected by others. By coupling these two designs, we can further improve the completeness and effectiveness of teacher knowledge learning. We conduct extensive experiments on different architectures with object detection and semantic segmentation. The results show that all the students can achieve further improvements compared to the conventional KD. Notably, we establish the new state-of-the-art results by boosting RetinaNet ResNet-18, and ResNet-50 from 33.4 to 37.5 mAP, and 37.4 to 41.5 mAP, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lao_Masked_Autoencoders_Are_Stronger_Knowledge_Distillers_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lao_Masked_Autoencoders_Are_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shanshan Lao</author><author>Guanglu Song</author><author>Boxiao Liu</author><author>Yu Liu</author><author>Yujiu Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>ASIC: Aligning Sparse in-the-wild Image Collections</title>
            <abstract>We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB, SPair-71k and PF-Willow benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compared to existing self-supervised methods. Code and other material will be made available at https://kampta.github.io/asic.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gupta_ASIC_Aligning_Sparse_in-the-wild_Image_Collections_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gupta_ASIC_Aligning_Sparse_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kamal Gupta</author><author>Varun Jampani</author><author>Carlos Esteves</author><author>Abhinav Shrivastava</author><author>Ameesh Makadia</author><author>Noah Snavely</author><author>Abhishek Kar</author>
            </authors>
        </paper>
        

        <paper>
            <title>Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation</title>
            <abstract>Semantic segmentation models classify pixels into a set of known (&quot;in-distribution&quot;) visual classes. When deployed in an open world, the reliability of these models depends on their ability to not only classify in-distribution pixels but also to detect out-of-distribution (OoD) pixels. Historically, the poor OoD detection performance of these models has motivated the design of methods based on model re-training using synthetic training images that include OoD visual objects. Although successful, these re-trained methods have two issues: 1) their in-distribution segmentation accuracy may drop during re-training, and 2) their OoD detection accuracy does not generalise well to new contexts (e.g., country surroundings) outside the training set (e.g., city surroundings). In this paper, we mitigate these issues with: (i) a new residual pattern learning (RPL) module that assists the segmentation model to detect OoD pixels with minimal deterioration to the inlier segmentation performance; and (ii) a novel context-robust contrastive learning (CoroCL) that enforces RPL to robustly detect OoD pixels in various contexts. Our approach improves by around 10% FPR and 7% AuPRC the previous state-of-the-art in Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly datasets. Code will be available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Residual_Pattern_Learning_for_Pixel-Wise_Out-of-Distribution_Detection_in_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Residual_Pattern_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuyuan Liu</author><author>Choubo Ding</author><author>Yu Tian</author><author>Guansong Pang</author><author>Vasileios Belagiannis</author><author>Ian Reid</author><author>Gustavo Carneiro</author>
            </authors>
        </paper>
        

        <paper>
            <title>Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning</title>
            <abstract>Compositional zero-shot learning (CZSL) aims to recognize unseen compositions with prior knowledge of known primitives (attribute and object). Previous works for CZSL often suffer from grasping the contextuality between attribute and object, as well as the discriminability of visual features, and the long-tailed distribution of real-world compositional data. We propose a simple and scalable framework called Composition Transformer (CoT) to address these issues. CoT employs object and attribute experts in distinctive manners to generate representative embeddings, using the visual network hierarchically. The object expert extracts representative object embeddings from the final layer in a bottom-up manner, while the attribute expert makes attribute embeddings in a top-down manner with a proposed object-guided attention module that models contextuality explicitly. To remedy biased prediction caused by imbalanced data distribution, we develop a simple minority attribute augmentation (MAA) that synthesizes virtual samples by mixing two images and oversampling minority attribute classes. Our method achieves SoTA performance on several benchmarks, including MIT-States, C-GQA, and VAW-CZSL. We also demonstrate the effectiveness of CoT in improving visual discrimination and addressing the model bias from the imbalanced data distribution. The code is available at https://github.com/HanjaeKim98/CoT.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kim_Hierarchical_Visual_Primitive_Experts_for_Compositional_Zero-Shot_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kim_Hierarchical_Visual_Primitive_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hanjae Kim</author><author>Jiyoung Lee</author><author>Seongheon Park</author><author>Kwanghoon Sohn</author>
            </authors>
        </paper>
        

        <paper>
            <title>Segment Every Reference Object in Spatial and Temporal Spaces</title>
            <abstract>The reference-based object segmentation tasks, namely referring image segmentation (RIS), referring video object segmentation (RVOS), and video object segmentation (VOS), aim to segment a specific object by utilizing either language or annotated masks as references. Despite significant progress in each respective field, current methods are task-specifically designed and developed in different directions, which hinders the activation of multi-task capabilities for these tasks. In this work, we end the current fragmented situation and propose UniRef to unify the three reference-based object segmentation tasks with a single architecture. At the heart of our approach is the multiway-fusion for handling different task with respect to their specified references. And a unified Transformer architecture is then adopted for performing instance-level segmentation. With the unified designs, UniRef can be jointly trained on a broad range of benchmarks and can flexibly perform multiple tasks at runtime by specifying the corresponding references. We evaluate the jointly trained network on various benchmarks. Extensive experimental results indicate that our proposed UniRef achieves state-of-the-art performance on RIS and RVOS, and performs competitively on VOS with a single network.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Segment_Every_Reference_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiannan Wu</author><author>Yi Jiang</author><author>Bin Yan</author><author>Huchuan Lu</author><author>Zehuan Yuan</author><author>Ping Luo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unified Out-Of-Distribution Detection: A Model-Specific Perspective</title>
            <abstract>Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is &quot;model-specific&quot;. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that involves a variety of models (e.g., different architectures and training strategies), sources of OOD examples, and OOD detection approaches, and reveal several insights into improving and understanding OOD detection in uncontrolled environments.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Averly_Unified_Out-Of-Distribution_Detection_A_Model-Specific_Perspective_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Averly_Unified_Out-Of-Distribution_Detection_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Reza Averly</author><author>Wei-Lun Chao</author>
            </authors>
        </paper>
        

        <paper>
            <title>RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels</title>
            <abstract>Learning with noisy labels (LNL) is one of the most important and challenging problems in weakly-supervised learning. Recent advances adopt the sample selection strategy to mitigate the interference of noisy labels and use small-loss criteria to select clean samples. However, the one-dimensional loss is an over-simplified metric that fails to accommodate the complex feature landscape of various samples, and, hence, is prone to introduce classification errors during sample selection. In this paper, we propose RankMatch, a novel LNL framework that investigates additional dimensions of confidence and consistency in order to combat noisy labels. Confidence-wise, we propose a novel sample selection strategy based on confidence representation voting instead of the widely-used small-loss criterion. This new strategy is capable of increasing sample selection quantity without sacrificing labeling accuracy. Consistency-wise, instead of the widely adopted feature distance metric for measuring the consistency of inner-class samples, we advocate that the rank of principal features is a much more robust indicator. Based on this metric, we propose rank contrastive loss, which strengthens the consistency of similar samples regardless of their labels and facilitates feature representation learning. Experimental results on noisy versions of CIFAR-10, CIFAR-100, Clothing1M, and WebVision have validated the superiority of our approach over existing state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_RankMatch_Fostering_Confidence_and_Consistency_in_Learning_with_Noisy_Labels_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_RankMatch_Fostering_Confidence_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ziyi Zhang</author><author>Weikai Chen</author><author>Chaowei Fang</author><author>Zhen Li</author><author>Lechao Chen</author><author>Liang Lin</author><author>Guanbin Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation</title>
            <abstract>Recently, semantic segmentation models trained with image-level text supervision have shown promising results in challenging open-world scenarios. However, these models still face difficulties in learning fine-grained semantic alignment at the pixel level and predicting accurate object masks. To address this issue, we propose MixReorg, a novel and straightforward pre-training paradigm for semantic segmentation that enhances a model&apos;s ability to reorganize patches mixed across images, exploring both local visual relevance and global semantic coherence. Our approach involves generating fine-grained patch-text pairs data by mixing image patches while preserving the correspondence between patches and text. The model is then trained to minimize the segmentation loss of the mixed images and the two contrastive losses of the original and restored features. With MixReorg as a mask learner, conventional text-supervised semantic segmentation models can achieve highly generalizable pixel-semantic alignment ability, which is crucial for open-world segmentation. After training with large-scale image-text data, MixReorg models can be applied directly to segment visual objects of arbitrary categories, without the need for further fine-tuning. Our proposed framework demonstrates strong performance on popular zero-shot semantic segmentation benchmarks, outperforming GroupViT by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012, PASCAL Context, MS COCO, and ADE20K, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cai_MixReorg_Cross-Modal_Mixed_Patch_Reorganization_is_a_Good_Mask_Learner_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cai_MixReorg_Cross-Modal_Mixed_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kaixin Cai</author><author>Pengzhen Ren</author><author>Yi Zhu</author><author>Hang Xu</author><author>Jianzhuang Liu</author><author>Changlin Li</author><author>Guangrun Wang</author><author>Xiaodan Liang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis</title>
            <abstract>NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior&apos;s training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly, our method requires as few as two views of casually captured images as input at inference time.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Buhler_Preface_A_Data-driven_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Marcel C. B hler</author><author>Kripasindhu Sarkar</author><author>Tanmay Shah</author><author>Gengyan Li</author><author>Daoye Wang</author><author>Leonhard Helminger</author><author>Sergio Orts-Escolano</author><author>Dmitry Lagun</author><author>Otmar Hilliges</author><author>Thabo Beeler</author><author>Abhimitra Meka</author>
            </authors>
        </paper>
        

        <paper>
            <title>ICICLE: Interpretable Class Incremental Continual Learning</title>
            <abstract>Continual learning enables incremental learning of new tasks without forgetting those previously learned, resulting in positive knowledge transfer that can enhance performance on both new and old tasks. However, continual learning poses new challenges for interpretability, as the rationale behind model predictions may change over time, leading to interpretability concept drift. We address this problem by proposing Interpretable Class-InCremental LEarning (ICICLE), an exemplar-free approach that adopts a prototypical part-based approach. It consists of three crucial novelties: interpretability regularization that distills previously learned concepts while preserving user-friendly positive reasoning; proximity-based prototype initialization strategy dedicated to the fine-grained setting; and task-recency bias compensation devoted to prototypical parts. Our experimental results demonstrate that ICICLE reduces the interpretability concept drift and outperforms the existing exemplar-free methods of common class-incremental learning when applied to concept-based models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Rymarczyk_ICICLE_Interpretable_Class_Incremental_Continual_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Rymarczyk_ICICLE_Interpretable_Class_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Dawid Rymarczyk</author><author>Joost van de Weijer</author><author>Bartosz Zieli ski</author><author>Bartlomiej Twardowski</author>
            </authors>
        </paper>
        

        <paper>
            <title>PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</title>
            <abstract>Large-scale pre-trained models have shown promising open-world performance for both vision and language tasks. However, their transferred capacity on 3D point clouds is still limited and only constrained to the classification task. In this paper, we first collaborate CLIP and GPT to be a unified 3D open-world learner, named as PointCLIP V2, which fully unleashes their potential for zero-shot 3D classification, segmentation, and detection. To better align 3D data with the pre-trained language knowledge, PointCLIP V2 contains two key designs. For the visual end, we prompt CLIP via a shape projection module to generate more realistic depth maps, narrowing the domain gap between projected point clouds with natural images. For the textual end, we prompt the GPT model to generate 3D-specific text as the input of CLIP&apos;s textual encoder. Without any training in 3D domains, our approach significantly surpasses PointCLIP by +42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D classification. On top of that, V2 can be extended to few-shot 3D classification, zero-shot 3D part segmentation, and 3D object detection in a simple manner, demonstrating our generalization ability for unified 3D open-world learning.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_PointCLIP_V2_Prompting_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiangyang Zhu</author><author>Renrui Zhang</author><author>Bowei He</author><author>Ziyu Guo</author><author>Ziyao Zeng</author><author>Zipeng Qin</author><author>Shanghang Zhang</author><author>Peng Gao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Identification of Systematic Errors of Image Classifiers on Rare Subgroups</title>
            <abstract>Despite excellent average-case performance of many image classifiers, their performance can substantially deteriorate on semantically coherent subgroups of the data that were under-represented in the training data. These systematic errors can impact both fairness for demographic minority groups as well as robustness and safety under domain shift. A major challenge is to identify such subgroups with subpar performance when the subgroups are not annotated and their occurrence is very rare. We leverage recent advances in text-to-image models and search in the space of textual descriptions of subgroups (&quot;prompts&quot;) for subgroups where the target model has low performance on the prompt-conditioned synthesized data. To tackle the exponentially growing number of subgroups, we employ combinatorial testing. We denote this procedure as PromptAttack as it can be interpreted as an adversarial attack in a prompt space. We study subgroup coverage and identifiability with PromptAttack in a controlled setting and find that it identifies systematic errors with high accuracy. Thereupon, we apply PromptAttack to ImageNet classifiers and identify novel systematic errors on rare subgroups.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Metzen_Identification_of_Systematic_Errors_of_Image_Classifiers_on_Rare_Subgroups_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Metzen_Identification_of_Systematic_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jan Hendrik Metzen</author><author>Robin Hutmacher</author><author>N. Grace Hua</author><author>Valentyn Boreiko</author><author>Dan Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Clusterformer: Cluster-based Transformer for 3D Object Detection in Point Clouds</title>
            <abstract>Attributed to the unstructured and sparse nature of point clouds, the transformer shows greater potential in point clouds data processing. However, the recent query-based 3D detectors usually project the features acquired from a sparse backbone into the structured and compact Bird&apos;s Eye View(BEV) plane before adopting the transformer, which destroys the sparsity of features, introducing empty tokens and additional resource consumption for the transformer. To this end, in this paper, we propose a novel query-based 3D detector called Clusterformer, our Clusterformer regards each object as a cluster of 3D space which mainly consists of the non-empty voxels belonging to the same object, and leverages the cluster to conduct the transformer decoder to generate the proposals from the sparse voxel features directly. Such cluster-based transformer structure can effectively improve the performance and convergence speed of query-based detectors by making use of the object prior information contained in the clusters. Additionally, we introduce a Query2Key strategy to enhance the key and value features with the object-level information iteratively in our cluster-based transformer structure. Experimental results show that the proposed Clusterformer outperforms the previous query-based detectors with a lower latency and memory usage, which achieves state-of-the-art performance on the Waymo Open Datasets and KITTI Datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pei_Clusterformer_Cluster-based_Transformer_for_3D_Object_Detection_in_Point_Clouds_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yu Pei</author><author>Xian Zhao</author><author>Hao Li</author><author>Jingyuan Ma</author><author>Jingwei Zhang</author><author>Shiliang Pu</author>
            </authors>
        </paper>
        

        <paper>
            <title>CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification</title>
            <abstract>This paper presents a CLIP-based unsupervised learning method for annotation-free multi-label image classification, including three stages: initialization, training, and inference. At the initialization stage, we take full advantage of the powerful CLIP model and propose a novel approach to extend CLIP for multi-label predictions based on global-local image-text similarity aggregation. To be more specific, we split each image into snippets and leverage CLIP to generate the similarity vector for the whole image (global) as well as each snippet (local). Then a similarity aggregator is introduced to leverage the global and local similarity vectors. Using the aggregated similarity scores as the initial pseudo labels at the training stage, we propose an optimization framework to train the parameters of the classification network and refine pseudo labels for unobserved labels. During inference, only the classification network is used to predict the labels of the input image. Extensive experiments show that our method outperforms state-of-the-art unsupervised methods on MS-COCO, PASCAL VOC 2007, PASCAL VOC 2012, and NUS datasets and even achieves comparable results to weakly supervised classification methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Abdelfattah_CDUL_CLIP-Driven_Unsupervised_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Rabab Abdelfattah</author><author>Qing Guo</author><author>Xiaoguang Li</author><author>Xiaofeng Wang</author><author>Song Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Your Diffusion Model is Secretly a Zero-Shot Classifier</title>
            <abstract>The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification, which we call Diffusion Classifier, attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. Although a gap remains between generative and discriminative approaches on zero-shot recognition tasks, our diffusion-based approach has stronger multimodal compositional reasoning abilities than competing discriminative approaches. Finally, we use Diffusion Classifier to extract standard classifiers from class-conditional diffusion models trained on ImageNet. These models approach the performance of SOTA discriminative classifiers and exhibit strong &quot;effective robustness&quot; to distribution shift. Overall, our results are a step toward using generative over discriminative models for downstream tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Your_Diffusion_Model_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Alexander C. Li</author><author>Mihir Prabhudesai</author><author>Shivam Duggal</author><author>Ellis Brown</author><author>Deepak Pathak</author>
            </authors>
        </paper>
        

        <paper>
            <title>Backpropagation Path Search On Adversarial Transferability</title>
            <abstract>Deep neural networks are vulnerable to adversarial examples, dictating the imperativeness to test the model&apos;s robustness before deployment. Transfer-based attackers craft adversarial examples against surrogate models and transfer them to victim models deployed in the black-box situation. To enhance the adversarial transferability, structure-based attackers adjust the backpropagation path to avoid the attack from overfitting the surrogate model. However, existing structure-based attackers fail to explore the convolution module in CNNs and modify the backpropagation graph heuristically, leading to limited effectiveness. In this paper, we propose backPropagation pAth Search (PAS), solving the aforementioned two problems. We first propose SkipConv to adjust the backpropagation path of convolution by structural reparameterization. To overcome the drawback of heuristically designed backpropagation paths, we further construct a DAG-based search space, utilize one-step approximation for path evaluation and employ Bayesian Optimization to search for the optimal path. We conduct comprehensive experiments in a wide range of transfer settings, showing that PAS improves the attack success rate by a huge margin for both normally trained and defense models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_Backpropagation_Path_Search_On_Adversarial_Transferability_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_Backpropagation_Path_Search_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Zhuoer Xu</author><author>Zhangxuan Gu</author><author>Jianping Zhang</author><author>Shiwen Cui</author><author>Changhua Meng</author><author>Weiqiang Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Boosting Adversarial Transferability via Gradient Relevance Attack</title>
            <abstract>Plentiful adversarial attack researches have revealed the fragility of deep neural networks (DNNs), where the imperceptible perturbations can cause drastic changes in the output. Among the diverse types of attack methods, gradient-based attacks are powerful and easy to implement, arousing wide concern for the security problem of DNNs. However, under the black-box setting, the existing gradient-based attacks have much trouble in breaking through DNN models with defense technologies, especially those adversarially trained models. To make adversarial examples more transferable, in this paper, we explore the fluctuation phenomenon on the plus-minus sign of the adversarial perturbations&apos; pixels during the generation of adversarial examples, and propose an ingenious Gradient Relevance Attack (GRA). Specifically, two gradient relevance frameworks are presented to better utilize the information in the neighborhood of the input, which can correct the update direction adaptively. Then we adjust the update step at each iteration with a decay indicator to counter the fluctuation. Experiment results on a subset of the ILSVRC 2012 validation set forcefully verify the effectiveness of GRA. Furthermore, the attack success rates of 68.7% and 64.8% on Tencent Cloud and Baidu AI Cloud further indicate that GRA can craft adversarial examples with the ability to transfer across both datasets and model architectures. Code is released at https://github.com/RYC-98/GRA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Boosting_Adversarial_Transferability_via_Gradient_Relevance_Attack_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Hegui Zhu</author><author>Yuchen Ren</author><author>Xiaoyan Sui</author><author>Lianping Yang</author><author>Wuming Jiang</author>
            </authors>
        </paper>
        

        <paper>
            <title>CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No</title>
            <abstract>Out-of-distribution (OOD) detection refers to training the model on in-distribution (ID) dataset to classify if the input images come from unknown classes. Considerable efforts have been invested in designing various OOD detection methods based on either convolutional neural networks or transformers. However, Zero-shot OOD detection methods driven by CLIP, which require only class names for ID, have received less attention. This paper presents a novel method, namely CLIP saying no (CLIPN), which empowers &quot;no&quot; logic within CLIP. Our key motivation is to equip CLIP with the capability of distinguishing OOD and ID samples via positive-semantic prompts and negation-semantic prompts. To be specific, we design a novel learnable &quot;no&quot; prompt and a &quot;no&quot; text encoder to capture the negation-semantic with images. Subsequently, we introduce two loss functions: the image-text binary-opposite loss and the text semantic-opposite loss, which we use to teach CLIPN to associate images with &quot;no&quot; prompts, thereby enabling it to identify unknown samples. Furthermore, we propose two threshold-free inference algorithms to perform OOD detection via using negation semantics from &quot;no&quot; prompts and text encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6 OOD datasets) for the OOD detection task demonstrate that CLIPN outperforms 7 well-used algorithms by at least 1.1% and 7.37% on AUROC and FPR95 on zero-shot OOD detection of ImageNet-1K. Our CLIPN can serve as a solid foundation for leveraging CLIP effectively in downstream OOD tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_CLIPN_for_Zero-Shot_OOD_Detection_Teaching_CLIP_to_Say_No_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Hualiang Wang</author><author>Yi Li</author><author>Huifeng Yao</author><author>Xiaomeng Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>CO-Net: Learning Multiple Point Cloud Tasks at Once with A Cohesive Network</title>
            <abstract>We present CO-Net, a cohesive framework that optimizes multiple point cloud tasks collectively across heterogeneous dataset domains. CO-Net maintains the characteristics of high storage efficiency since models with the preponderance of shared parameters can be assembled into a single model. Specifically, we leverage residual MLP (Res-MLP) block for effective feature extraction and scale it gracefully along the depth and width of the network to meet the demands of different tasks. Based on the block, we propose a novel nested layer-wise processing policy, which identifies the optimal architecture for each task while provides partial sharing parameters and partial non-sharing parameters inside each layer of the block. Such policy tackles the inherent challenges of multi-task learning on point cloud, e.g., diverse model topologies resulting from task skew and conflicting gradients induced by heterogeneous dataset domains. Finally, we propose a sign-based gradient surgery to promote the training of CO-Net, thereby emphasizing the usage of task-shared parameters and guaranteeing that each task can be thoroughly optimized. Experimental results reveal that models optimized by CO-Net jointly for all point cloud tasks maintain much fewer computation cost and overall storage cost yet outpace prior methods by a significant margin. We also demonstrate that CO-Net allows incremental learning and prevents catastrophic amnesia when adapting to a new point cloud task.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xie_CO-Net_Learning_Multiple_Point_Cloud_Tasks_at_Once_with_A_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xie_CO-Net_Learning_Multiple_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Tao Xie</author><author>Ke Wang</author><author>Siyi Lu</author><author>Yukun Zhang</author><author>Kun Dai</author><author>Xiaoyu Li</author><author>Jie Xu</author><author>Li Wang</author><author>Lijun Zhao</author><author>Xinyu Zhang</author><author>Ruifeng Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Quality Diversity for Visual Pre-Training</title>
            <abstract>Models pre-trained on large datasets such as ImageNet provide the de-facto standard for transfer learning, with both supervised and self-supervised approaches proving effective. However, emerging evidence suggests that any single pre-trained feature will not perform well on diverse downstream tasks. Each pre-training strategy encodes a certain inductive bias, which may suit some downstream tasks but not others. Notably, the augmentations used in both supervised and self-supervised training lead to features with high invariance to spatial and appearance transformations. This renders them sub-optimal for tasks that demand sensitivity to these factors. In this paper we develop a feature that better supports diverse downstream tasks by providing a diverse set of sensitivities and invariances. In particular, we are inspired by Quality-Diversity in evolution, to define a pre-training objective that requires high quality yet diverse features -- where diversity is defined in terms of transformation (in)variances. Our framework plugs in to both supervised and self-supervised pre-training, and produces a small ensemble of features. We further show how downstream tasks can easily and efficiently select their preferred (in)variances. Both empirical and theoretical analysis show the efficacy of our representation and transfer learning approach for diverse downstream tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chavhan_Quality_Diversity_for_Visual_Pre-Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chavhan_Quality_Diversity_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ruchika Chavhan</author><author>Henry Gouk</author><author>Da Li</author><author>Timothy Hospedales</author>
            </authors>
        </paper>
        

        <paper>
            <title>UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-Aware Curriculum and Iterative Generalist-Specialist Learning</title>
            <abstract>We propose a novel, object-agnostic method for learning a universal policy for dexterous object grasping from realistic point cloud observations and proprioceptive information under a table-top setting, namely UniDexGrasp++. To address the challenge of learning the vision-based policy across thousands of object instances, we propose Geometry-aware Curriculum Learning (GeoCurriculum) and Geometry-aware iterative Generalist-Specialist Learning (GiGSL) which leverage the geometry feature of the task and significantly improve the generalizability. With our proposed techniques, our final policy shows universal dexterous grasping on thousands of object instances with 85.4% and 78.2% success rate on the train set and test set which outperforms the state-of-the-art baseline UniDexGrasp by 11.7% and 11.3%, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wan_UniDexGrasp_Improving_Dexterous_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Weikang Wan</author><author>Haoran Geng</author><author>Yun Liu</author><author>Zikang Shan</author><author>Yaodong Yang</author><author>Li Yi</author><author>He Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>FerKD: Surgical Label Adaptation for Efficient Distillation</title>
            <abstract>We present FerKD, a novel efficient knowledge distillation framework that incorporates partial soft-hard label adaptation coupled with a region-calibration mechanism. Our approach stems from the observation and intuition that standard data augmentations, such as RandomResizedCrop, tend to transform inputs into diverse conditions: easy positives, hard positives, or hard negatives. In traditional distillation frameworks, these transformed samples are utilized equally through their predictive probabilities derived from pretrained teacher models. However, merely relying on prediction values from a pretrained teacher, a common practice in prior studies, neglects the reliability of these soft label predictions. To address this, we propose a new scheme that calibrates the less-confident regions to be the context using softened hard groundtruth labels. Our approach involves the processes of hard regions mining + calibration. We demonstrate empirically that this method can dramatically improve the convergence speed and final accuracy. Additionally, we find that a consistent mixing strategy can stabilize the distributions of soft supervision, taking advantage of the soft labels. As a result, we introduce a stabilized SelfMix augmentation that weakens the variation of the mixed images and corresponding soft labels through mixing similar regions within the same image. FerKD is an intuitive and well-designed learning system that eliminates several heuristics and hyperparameters in former FKD solution. More importantly, it achieves remarkable improvement on ImageNet-1K and downstream tasks. For instance, FerKD achieves 81.2% on ImageNet-1K with ResNet-50, outperforming FKD and FunMatch by remarkable margins. Leveraging better pre-trained weights and larger architectures, our finetuned ViT-G14 even achieves 89.9%. Our code is available at https://github.com/szq0214/FKD/tree/main/FerKD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shen_FerKD_Surgical_Label_Adaptation_for_Efficient_Distillation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shen_FerKD_Surgical_Label_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhiqiang Shen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Neural Fields for Structured Lighting</title>
            <abstract>We present an image formation model and optimization procedure that combines the advantages of neural radiance fields and structured light imaging. Existing depth-supervised neural models rely on depth sensors to accurately capture the scene&apos;s geometry. However, the depth maps recovered by these sensors can be prone to error, or even fail outright. Instead of depending on the fidelity of processed depth maps from a structured light system, a more principled approach is to explicitly model the raw structured light images themselves. Our proposed approach enables the estimation of high-fidelity depth maps, including for objects with complex material properties (e.g., partially-transparent surfaces). Besides computing depth, the raw structured light images also confer other useful radiometric cues, which enable predicting surface normals and decomposing scene appearance in terms of a direct, indirect, and ambient component. We evaluate our framework quantitatively and qualitatively on a range of real and synthetic scenes, and decompose scenes into their constituent components for novel views.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shandilya_Neural_Fields_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Aarrushi Shandilya</author><author>Benjamin Attal</author><author>Christian Richardt</author><author>James Tompkin</author><author>Matthew O&apos;toole</author>
            </authors>
        </paper>
        

        <paper>
            <title>ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via An Indirect Recording Solution</title>
            <abstract>Garments are important and pervasive in daily life. However, visual analysis on them for pose estimation is challenging because it requires recovering the complete configurations of garments, which is difficult, if not impossible, to annotate in the real world. In this work, we propose a recording system, GarmentTwin, which can track garment poses in dynamic settings such as manipulation. GarmentTwin first collects garment models and RGB-D manipulation videos from the real world and then replays the manipulation process using physics-based animation. This way, we can obtain deformed garments with poses coarsely aligned with real-world observations. Finally, we adopt an optimization-based approach to fit the pose with real-world observations. We verify the fitting results quantitatively and qualitatively. With GarmentTwin, we construct a large-scale dataset named ClothPose, which consists of 30K RGB-D frames from 2K video clips on 600 garments of 10 categories. We benchmark two tasks on the proposed ClothPose: non-rigid reconstruction and pose estimation. The experiments show that previous baseline methods struggle with highly large non-rigid deformation of manipulated garments. Therefore, we hope that the recording system and the dataset can facilitate research on pose estimation tasks on non-rigid objects. Datasets, models, and codes are made publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Wenqiang Xu</author><author>Wenxin Du</author><author>Han Xue</author><author>Yutong Li</author><author>Ruolin Ye</author><author>Yan-Feng Wang</author><author>Cewu Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unsupervised Object Localization with Representer Point Selection</title>
            <abstract>We propose a novel unsupervised object localization method that allows us to explain the predictions of the model by utilizing self-supervised pre-trained models without additional finetuning. Existing unsupervised and self-supervised object localization methods often utilize class-agnostic activation maps or self-similarity maps of a pre-trained model. Although these maps can offer valuable information for localization, their limited ability to explain how the model makes predictions remains challenging. In this paper, we propose a simple yet effective unsupervised object localization method based on representer point selection, where the predictions of the model can be represented as a linear combination of representer values of training points. By selecting representer points, which are the most important examples for the model predictions, our model can provide insights into how the model predicts the foreground object by providing relevant examples as well as their importance. Our method outperforms the state-of-the-art unsupervised and self-supervised object localization methods on various datasets with significant margins and even outperforms recent weakly supervised and few-shot methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Song_Unsupervised_Object_Localization_with_Representer_Point_Selection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Song_Unsupervised_Object_Localization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yeonghwan Song</author><author>Seokwoo Jang</author><author>Dina Katabi</author><author>Jeany Son</author>
            </authors>
        </paper>
        

        <paper>
            <title>SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</title>
            <abstract>Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image&apos;s DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ravindran_SEMPART_Self-supervised_Multi-resolution_Partitioning_of_Image_Semantics_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ravindran_SEMPART_Self-supervised_Multi-resolution_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sriram Ravindran</author><author>Debraj Basu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Flatness-Aware Minimization for Domain Generalization</title>
            <abstract>Domain generalization (DG) seeks to learn robust models that generalize well under unknown distribution shifts. As a critical aspect of DG, optimizer selection has not been explored in depth. Currently, most DG methods follow the widely used benchmark, DomainBed, and utilize Adam as the default optimizer for all datasets. However, we reveal that Adam is not necessarily the optimal choice for the majority of current DG methods and datasets. Based on the perspective of loss landscape flatness, we propose a novel approach, Flatness-Aware Minimization for Domain Generalization (FAD), which can efficiently optimize both zeroth-order and first-order flatness simultaneously for DG. We provide theoretical analyses of the FAD&apos;s out-of-distribution (OOD) generalization error and convergence. Our experimental results demonstrate the superiority of FAD on various DG datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Flatness-Aware_Minimization_for_Domain_Generalization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xingxuan Zhang</author><author>Renzhe Xu</author><author>Han Yu</author><author>Yancheng Dong</author><author>Pengfei Tian</author><author>Peng Cui</author>
            </authors>
        </paper>
        

        <paper>
            <title>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</title>
            <abstract>Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose &apos;ProtoFL&apos;, Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kim_ProtoFL_Unsupervised_Federated_Learning_via_Prototypical_Distillation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Hansol Kim</author><author>Youngjun Kwak</author><author>Minyoung Jung</author><author>Jinho Shin</author><author>Youngsung Kim</author><author>Changick Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multi-label Affordance Mapping from Egocentric Vision</title>
            <abstract>Accurate affordance detection and segmentation with pixel precision is an important piece in many complex systems based on interactions, such as robots and assitive devices. We present a new approach to affordance perception which enables accurate multi-label segmentation. Our approach can be used to automatically annotate grounded affordances from first person videos of interactions using a 3D map of the environment providing pixel level precision for the affordance location. We use this method to build the largest and most complete dataset on affordances based on the EPIC-Kitchen dataset, EPIC-Aff, which provides automatic, interaction-grounded, multi-label, metric and spatial affordance annotations. Then, we propose a new approach to affordance segmentation based on multi-label detection which enables multiple affordances to co-exists in the same space, for example if they are associated with the same object. We present several strategies of multi-label detection using several segmentation architectures. The experimental results highlights the importance of the multi-label detection. Finally, we show how our metric representation can be exploited for build a map of interaction hotspots in spatial action-centric zones and use that representation to perform a task-oriented navigation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mur-Labadia_Multi-label_Affordance_Mapping_from_Egocentric_Vision_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mur-Labadia_Multi-label_Affordance_Mapping_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lorenzo Mur-Labadia</author><author>Jose J. Guerrero</author><author>Ruben Martinez-Cantin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Unified Adversarial Patch for Cross-Modal Attacks in the Physical World</title>
            <abstract>Recently, physical adversarial attacks have been presented to evade DNNs-based object detectors. To ensure the security, many scenarios are simultaneously deployed with visible sensors and infrared sensors, leading to the failures of these single-modal physical attacks. To show the potential risks under such scenes, we propose a unified adversarial patch to perform cross-modal physical attacks, i.e., fooling visible and infrared object detectors at the same time via a single patch. Considering different imaging mechanisms of visible and infrared sensors, our work focuses on modeling the shapes of adversarial patches, which can be captured in different modalities when they change. To this end, we design a novel boundary-limited shape optimization to achieve the compact and smooth shapes, and thus they can be easily implemented in the physical world. In addition, to balance the fooling degree between visible detector and infrared detector during the optimization process, we propose a score-aware iterative evaluation, which can guide the adversarial patch to iteratively reduce the predicted scores of the multi-modal sensors. We finally test our method against the one-stage detector: YOLOv3 and the two-stage detector: Faster RCNN. Results show that our unified patch achieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More importantly, we verify the effective attacks in the physical world when visible and infrared sensors shoot the objects under various settings like different angles, distances, postures, and scenes.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wei_Unified_Adversarial_Patch_for_Cross-Modal_Attacks_in_the_Physical_World_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wei_Unified_Adversarial_Patch_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xingxing Wei</author><author>Yao Huang</author><author>Yitong Sun</author><author>Jie Yu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-training</title>
            <abstract>Contrastive Language-Image Pretraining has emerged as a prominent approach for training vision and text encoders with uncurated image-text pairs from the web. To enhance data-efficiency, recent efforts have introduced additional supervision terms that involve random-augmented views of the image. However, since the image augmentation process is unaware of its text counterpart, this procedure could cause various degrees of image-text misalignments during training. Prior methods either disregarded this discrepancy or introduced external models to mitigate the impact of misalignments during training. In contrast, we propose a novel metric learning approach that capitalizes on these misalignments as an additional training source, which we term &quot;Misalign, Contrast then Distill (MCD)&quot;. Unlike previous methods that treat augmented images and their text counterparts as simple positive pairs, MCD predicts the continuous scales of misalignment caused by the augmentation. Our extensive experimental results show that our proposed MCD achieves state-of-the-art transferability in multiple classification and retrieval downstream datasets.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kim_Misalign_Contrast_then_Distill_Rethinking_Misalignments_in_Language-Image_Pre-training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Bumsoo Kim</author><author>Yeonsik Jo</author><author>Jinhyung Kim</author><author>Seunghwan Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>MixPath: A Unified Approach for One-shot Neural Architecture Search</title>
            <abstract>Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state-of-the-art results on ImageNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chu_MixPath_A_Unified_Approach_for_One-shot_Neural_Architecture_Search_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chu_MixPath_A_Unified_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiangxiang Chu</author><author>Shun Lu</author><author>Xudong Li</author><author>Bo Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts</title>
            <abstract>Cross-scene generalizable NeRF models, which can directly synthesize novel views of unseen scenes, have become a new spotlight of the NeRF field. Several existing attempts rely on increasingly end-to-end &quot;neuralized&quot; architectures, i.e., replacing scene representation and/or rendering modules with performant neural networks such as transformers, and turning novel view synthesis into a feed-forward inference pipeline. While those feedforward &quot;neuralized&quot; architectures still do not fit diverse scenes well out of the box, we propose to bridge them with the powerful Mixture-of-Experts (MoE) idea from large language models (LLMs), which has demonstrated superior generalization ability by balancing between larger overall model capacity and flexible per-instance specialization. Starting from a recent generalizable NeRF architecture called GNT, we first demonstrate that MoE can be neatly plugged in to enhance the model. We further customize a shared permanent expert and a geometry-aware consistency loss to enforce cross-scene consistency and spatial smoothness respectively, which are essential for generalizable view synthesis. Our proposed model, dubbed GNT with Mixture-of-View-Experts (GNT-MOVE), has experimentally shown state-of-the-art results when transferring to unseen scenes, indicating remarkably better cross-scene generalization in both zero-shot and few-shot settings. Our codes are available at https://github.com/VITA-Group/GNT-MOVE.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cong_Enhancing_NeRF_akin_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenyan Cong</author><author>Hanxue Liang</author><author>Peihao Wang</author><author>Zhiwen Fan</author><author>Tianlong Chen</author><author>Mukund Varma</author><author>Yi Wang</author><author>Zhangyang Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Task-aware Adaptive Learning for Cross-domain Few-shot Learning</title>
            <abstract>Although existing few-shot learning works yield promising results for in-domain queries, they still suffer from weak cross-domain generalization. Limited support data requires effective knowledge transfer, but domain-shift makes this harder. Towards this emerging challenge, researchers improved adaptation by introducing task-specific parameters, which are directly optimized and estimated for each task. However, adding a fixed number of additional parameters fails to consider the diverse domain shifts between target tasks and the source domain, limiting efficacy. In this paper, we first observe the dependence of task-specific parameter configuration on the target task. Abundant task-specific parameters may over-fit, and insufficient task-specific parameters may result in under-adaptation -- but the optimal task-specific configuration varies for different test tasks. Based on these findings, we propose the Task-aware Adaptive Network (TA2-Net), which is trained by reinforcement learning to adaptively estimate the optimal task-specific parameter configuration for each test task. It learns, for example, that tasks with significant domain shift usually have a larger need for task-specific parameters for adaptation. We evaluate our model on Meta-dataset. Empirical results show that our model outperforms existing state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Guo_Task-aware_Adaptive_Learning_for_Cross-domain_Few-shot_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yurong Guo</author><author>Ruoyi Du</author><author>Yuan Dong</author><author>Timothy Hospedales</author><author>Yi-Zhe Song</author><author>Zhanyu Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling</title>
            <abstract>Unsupervised domain adaptation (DA) with the aid of pseudo labeling techniques has emerged as a crucial approach for domain-adaptive 3D object detection. While effective, existing DA methods suffer from a substantial drop in performance when applied to a multi-class training setting, due to the co-existence of low-quality pseudo labels and class imbalance issues. In this paper, we address this challenge by proposing a novel ReDB framework tailored for learning to detect all classes at once. Our approach produces Reliable, Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the self-training on a distributionally different target domain. To alleviate disruptions caused by the environmental discrepancy (e.g., beam numbers), the proposed cross-domain examination (CDE) assesses the correctness of pseudo labels by copy-pasting target instances into a source environment and measuring the prediction consistency. To reduce computational overhead and mitigate the object shift (e.g., scales and point densities), we design an overlapped boxes counting (OBC) metric that allows to uniformly downsample pseudo-labeled objects across different geometric characteristics. To confront the issue of inter-class imbalance, we progressively augment the target point clouds with a class-balanced set of pseudo-labeled target instances and source objects, which boosts recognition accuracies on both frequently appearing and rare classes. Experimental results on three benchmark datasets using both voxel-based (i.e., SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our proposed ReDB approach outperforms existing 3D domain adaptation methods by a large margin, improving 23.15% mAP on the nuScenes - KITTI task. The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Revisiting_Domain-Adaptive_3D_Object_Detection_by_Reliable_Diverse_and_Class-balanced_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Revisiting_Domain-Adaptive_3D_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhuoxiao Chen</author><author>Yadan Luo</author><author>Zheng Wang</author><author>Mahsa Baktashmotlagh</author><author>Zi Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Efficient Adaptive Human-Object Interaction Detection with Concept-guided Memory</title>
            <abstract>Human Object Interaction (HOI) detection aims to localize and infer the relationships between a human and an object. Arguably, training supervised models for this task from scratch presents challenges due to the performance drop over rare classes and the high computational cost and time required to handle long-tailed distributions of HOIs in complex HOI scenes in realistic settings. This observation motivates us to design an HOI detector that can be trained even with long-tailed labeled data and can leverage existing knowledge from pre-trained models. Inspired by the powerful generalization ability of the large Vision-Language Models (VLM) on classification and retrieval tasks, we propose an efficient Adaptive HOI Detector with Concept-guided Memory (ADA-CM). ADA-CM has two operating modes. The first mode makes it tunable without learning new parameters in a training-free paradigm. Its second mode incorporates an instance-aware adapter mechanism that can further efficiently boost performance if updating a lightweight set of parameters can be afforded. Our proposed method achieves competitive results with state-of-the-art on the HICO-DET and V-COCO datasets with much less training time. Code can be found at https://github.com/ltttpku/ADA-CM.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lei_Efficient_Adaptive_Human-Object_Interaction_Detection_with_Concept-guided_Memory_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lei_Efficient_Adaptive_Human-Object_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ting Lei</author><author>Fabian Caba</author><author>Qingchao Chen</author><author>Hailin Jin</author><author>Yuxin Peng</author><author>Yang Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Attentive Mask CLIP</title>
            <abstract>In vision-language modeling, image token removal is an efficient augmentation technique to reduce the cost of encoding image features. The CLIP-style models, however, have been found to be negatively impacted by this technique. We hypothesize that removing a large portion of image tokens may inadvertently destroy the semantic information associated to a given text description, resulting in misaligned paired data in CLIP training. To address this issue, we propose an attentive token removal approach, which retains a small number of tokens that have a strong semantic correlation to the corresponding text description. The correlation scores are dynamically evaluated through an EMA-updated vision encoder. Our method, termed attentive mask CLIP, outperforms original CLIP and CLIP variant with random token removal while saving the training time. In addition, our approach also enables efficient multi-view contrastive learning. Experimentally, by training ViT-B on YFCC-15M dataset, our approach achieves 43.9% top-1 accuracy on ImageNet-1K zero-shot classification, 62.7/42.1 and 38.0/23.2 I2T/T2I retrieval accuracy on Flickr30K and MS COCO, outperforming SLIP by +1.1%,+5.5/+0.9, and +4.4/+1.3, respectively, while being 2.30x faster. An efficient version of our approach runs 1.16x faster than the plain CLIP model, while achieving significant gains of +5.3%, +11.3/+8.0, and +9.5/+4.9 on these benchmarks, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_Attentive_Mask_CLIP_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yifan Yang</author><author>Weiquan Huang</author><author>Yixuan Wei</author><author>Houwen Peng</author><author>Xinyang Jiang</author><author>Huiqiang Jiang</author><author>Fangyun Wei</author><author>Yin Wang</author><author>Han Hu</author><author>Lili Qiu</author><author>Yuqing Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Motion-Guided Masking for Spatiotemporal Representation Learning</title>
            <abstract>Several recent works have directly extended the image masked autoencoder (MAE) with random masking into video domain, achieving promising results. However, unlike images, both spatial and temporal information are important for video understanding. This suggests that the random masking strategy that is inherited from the image MAE is less effective for video MAE. This motivates the design of a novel masking algorithm that can more efficiently make use of video saliency. Specifically, we propose a motion-guided masking algorithm (MGM) which leverages motion vectors to guide the position of each mask over time. Crucially, these motion-based correspondences can be directly obtained from information stored in the compressed format of the video, which makes our method efficient and scalable. On two challenging large-scale video benchmarks (Kinetics-400 and Something-Something V2), we equip video MAE with our MGM and achieve up to +1.3% improvement compared to previous state-of-the-art methods. Additionally, our MGM achieves equivalent performance to previous video MAE using up to 66% fewer training epochs. Lastly, we show that MGM generalizes better to downstream transfer learning and domain adaptation tasks on the UCF101, HMDB51, and Diving48 datasets, achieving up to +4.9% improvement compared to baseline methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fan_Motion-Guided_Masking_for_Spatiotemporal_Representation_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fan_Motion-Guided_Masking_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>David Fan</author><author>Jue Wang</author><author>Shuai Liao</author><author>Yi Zhu</author><author>Vimal Bhat</author><author>Hector Santos-Villalobos</author><author>Rohith MV</author><author>Xinyu Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>Urban Radiance Field Representation with Deformable Neural Mesh Primitives</title>
            <abstract>Neural Radiance Fields (NeRFs) have achieved great success in the past few years. However, most current methods still require intensive resources due to ray marching-based rendering. To construct urban-level radiance fields efficiently, we design Deformable Neural Mesh Primitive (DNMP), and propose to parameterize the entire scene with such primitives. The DNMP is a flexible and compact neural variant of classic mesh representation, which enjoys both the efficiency of rasterization-based rendering and the powerful neural representation capability for photo-realistic image synthesis. Specifically, a DNMP consists of a set of connected deformable mesh vertices with paired vertex features to parameterize the geometry and radiance information of a local area. To constrain the degree of freedom for optimization and lower the storage budgets, we enforce the shape of each primitive to be decoded from a relatively low-dimensional latent space. The rendering colors are decoded from the vertex features (interpolated with rasterization) by a view-dependent MLP. The DNMP provides a new paradigm for urban-level scene representation with appealing properties: (1) High-quality rendering. Our method achieves leading performance for novel view synthesis in urban scenarios. (2) Low computational costs. Our representation enables fast rendering (2.07ms/1k pixels) and low peak memory usage (110MB/1k pixels). We also present a lightweight version that can run 33xfaster than vanilla NeRFs, and comparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Urban_Radiance_Field_Representation_with_Deformable_Neural_Mesh_Primitives_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lu_Urban_Radiance_Field_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Fan Lu</author><author>Yan Xu</author><author>Guang Chen</author><author>Hongsheng Li</author><author>Kwan-Yee Lin</author><author>Changjun Jiang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Adaptive Frequency Filters As Efficient Global Token Mixers</title>
            <abstract>Recent vision transformers, large-kernel CNNs and MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope. However, their efficient deployments, especially on mobile devices, still suffer from noteworthy challenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers. In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adaptive frequency filters can serve as efficient global token mixer. With this insight, we propose Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive frequency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation. We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet. Extensive experiments demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, including visual recognition and dense prediction tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Adaptive_Frequency_Filters_As_Efficient_Global_Token_Mixers_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Adaptive_Frequency_Filters_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhipeng Huang</author><author>Zhizheng Zhang</author><author>Cuiling Lan</author><author>Zheng-Jun Zha</author><author>Yan Lu</author><author>Baining Guo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction</title>
            <abstract>As it is hard to calibrate single-view RGB images in the wild, existing 3D human mesh reconstruction (3DHMR) methods either use a constant large focal length or estimate one based on the background environment context, which can not tackle the problem of the torso, limb, hand or face distortion caused by perspective camera projection when the camera is close to the human body. The naive focal length assumptions can harm this task with the incorrectly formulated projection matrices. To solve this, we propose Zolly, the first 3DHMR method focusing on perspective-distorted images. Our approach begins with analysing the reason for perspective distortion, which we find is mainly caused by the relative location of the human body to the camera center. We propose a new camera model and a novel 2D representation, termed distortion image, which describes the 2D dense distortion scale of the human body. We then estimate the distance from distortion scale features rather than environment context features. Afterwards, We integrate the distortion feature with image features to reconstruct the body mesh. To formulate the correct projection matrix and locate the human body position, we simultaneously use perspective and weak-perspective projection loss. Since existing datasets could not handle this task, we propose the first synthetic dataset PDHuman and extend two real-world datasets tailored for this task, all containing perspective-distorted human images. Extensive experiments show that Zolly outperforms existing state-of-the-art methods on both perspective-distorted datasets and the standard benchmark (3DPW). Code and dataset will be released at https://wenjiawang0312.github.io/projects/zolly/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Zolly_Zoom_Focal_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenjia Wang</author><author>Yongtao Ge</author><author>Haiyi Mei</author><author>Zhongang Cai</author><author>Qingping Sun</author><author>Yanjun Wang</author><author>Chunhua Shen</author><author>Lei Yang</author><author>Taku Komura</author>
            </authors>
        </paper>
        

        <paper>
            <title>Beyond One-to-One: Rethinking the Referring Image Segmentation</title>
            <abstract>Referring image segmentation aims to segment the target object referred by a natural language expression. However, previous methods rely on the strong assumption that one sentence must describe one target in the image, which is often not the case in real-world applications. As a result, such methods fail when the expressions refer to either no objects or multiple objects. In this paper, we address this issue from two perspectives. First, we propose a Dual Multi-Modal Interaction (DMMI) Network, which contains two decoder branches and enables information flow in two directions. In the text-to-image decoder, text embedding is utilized to query the visual feature and localize the corresponding target. Meanwhile, the image-to-text decoder is implemented to reconstruct the erased entity-phrase conditioned on the visual feature. In this way, visual features are encouraged to contain the critical semantic information about target entity, which supports the accurate segmentation in the text-to-image decoder in turn. Secondly, we collect a new challenging but realistic dataset called Ref-ZOM, which includes image-text pairs under different settings. Extensive experiments demonstrate our method achieves state-of-the-art performance on different datasets, and the Ref-ZOM-trained model performs well on various types of text inputs. Codes and datasets are available at https://github.com/toggle1995/RIS-DMMI.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hu_Beyond_One-to-One_Rethinking_the_Referring_Image_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hu_Beyond_One-to-One_Rethinking_the_Referring_Image_Segmentation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yutao Hu</author><author>Qixiong Wang</author><author>Wenqi Shao</author><author>Enze Xie</author><author>Zhenguo Li</author><author>Jungong Han</author><author>Ping Luo</author>
            </authors>
        </paper>
        

        <paper>
            <title>MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope</title>
            <abstract>Explaining the predictions of deep neural nets has been a topic of great interest in the computer vision literature. While several gradient-based interpretation schemes have been proposed to reveal the influential variables in a neural net&apos;s prediction, standard gradient-based interpretation frameworks have been commonly observed to lack robustness to input perturbations and flexibility for incorporating prior knowledge of sparsity and group-sparsity structures. In this work, we propose MoreauGrad as an interpretation scheme based on the classifier neural net&apos;s Moreau envelope. We demonstrate that MoreauGrad results in a smooth and robust interpretation of a multi-layer neural network and can be efficiently computed through first-order optimization methods. Furthermore, we show that MoreauGrad can be naturally combined with L1-norm regularization techniques to output a sparse or group-sparse explanation which are prior conditions applicable to a wide range of deep learning applications. We empirically evaluate the proposed MoreauGrad scheme on standard computer vision datasets, showing the qualitative and quantitative success of the MoreauGrad approach in comparison to standard gradient-based interpretation methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_MoreauGrad_Sparse_and_Robust_Interpretation_of_Neural_Networks_via_Moreau_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_MoreauGrad_Sparse_and_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Jingwei Zhang</author><author>Farzan Farnia</author>
            </authors>
        </paper>
        

        <paper>
            <title>Class-Incremental Grouping Network for Continual Audio-Visual Learning</title>
            <abstract>Continual learning is a challenging problem in which models need to be trained on non-stationary data across sequential tasks for class-incremental learning. While previous methods have focused on using either regularization or rehearsal-based frameworks to alleviate catastrophic forgetting in image classification, they are limited to a single modality and cannot learn compact class-aware cross-modal representations for continual audio-visual learning. To address this gap, we propose a novel class-incremental grouping network (CIGN) that can learn category-wise semantic features to achieve continual audio-visual learning. Our CIGN leverages learnable audio-visual class tokens and audio-visual grouping to continually aggregate class-aware features. Additionally, it utilizes class tokens distillation and continual grouping to prevent forgetting parameters learned from previous tasks, thereby improving the model&apos;s ability to capture discriminative audio-visual categories. We conduct extensive experiments on VGGSound-Instruments, VGGSound-100, and VGG-Sound Sources benchmarks. Our experimental results demonstrate that the CIGN achieves state-of-the-art audio-visual class-incremental learning performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mo_Class-Incremental_Grouping_Network_for_Continual_Audio-Visual_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mo_Class-Incremental_Grouping_Network_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shentong Mo</author><author>Weiguo Pian</author><author>Yapeng Tian</author>
            </authors>
        </paper>
        

        <paper>
            <title>Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</title>
            <abstract>Denoising diffusion models (DDMs) have attracted attention for their exceptional generation quality and diversity. This success is largely attributed to the use of class- or text-conditional diffusion guidance methods, such as classifier and classifier-free guidance. In this paper, we present a more comprehensive perspective that goes beyond the traditional guidance methods. From this generalized perspective, we introduce novel condition- and training-free strategies to enhance the quality of generated images. As a simple solution, blur guidance improves the suitability of intermediate samples for their fine-scale information and structures, enabling diffusion models to generate higher quality samples with a moderate guidance scale. Improving upon this, Self-Attention Guidance (SAG) uses the intermediate self-attention maps of diffusion models to enhance their stability and efficacy. Specifically, SAG adversarially blurs only the regions that diffusion models attend to at each iteration and guides them accordingly. Our experimental results show that our SAG improves the performance of various diffusion models, including ADM, IDDPM, Stable Diffusion, and DiT. Moreover, combining SAG with conventional guidance methods leads to further improvement.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hong_Improving_Sample_Quality_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Susung Hong</author><author>Gyuseong Lee</author><author>Wooseok Jang</author><author>Seungryong Kim</author>
            </authors>
        </paper>
        

        <paper>
            <title>Evaluating Data Attribution for Text-to-Image Models</title>
            <abstract>While large text-to-image models are able to synthesize &quot;novel&quot; images, these images are necessarily a reflection of the training data. The problem of data attribution in such models -- which of the images in the training set are most responsible for the appearance of a given generated image -- is a difficult yet important one. As an initial step toward this problem, we evaluate attribution through &quot;customization&quot; methods, which tune an existing large-scale model toward a given exemplar object or style. Our key insight is that this allows us to efficiently create synthetic images that are computationally influenced by the exemplar by construction. With our new dataset of such exemplar-influenced images, we are able to evaluate various data attribution algorithms and different possible feature spaces. Furthermore, by training on our dataset, we can tune standard models, such as DINO, CLIP, and ViT, toward the attribution problem. Even though the procedure is tuned towards small exemplar sets, we show generalization to larger sets. Finally, by taking into account the inherent uncertainty of the problem, we can assign soft attribution scores over a set of training images.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Evaluating_Data_Attribution_for_Text-to-Image_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Evaluating_Data_Attribution_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Sheng-Yu Wang</author><author>Alexei A. Efros</author><author>Jun-Yan Zhu</author><author>Richard Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Delta Denoising Score</title>
            <abstract>This paper introduces Delta Denoising Score (DDS), a novel diffusion-based scoring technique that optimizes a parametric model for the task of image editing. Unlike the existing Score Distillation Sampling (SDS), which queries the generative model with a single image-text pair, DDS utilizes an additional fixed query of a reference image-text pair to generate delta scores that represent the difference between the outputs of the two queries. By estimating noisy gradient directions introduced by SDS using the source image and its text description, DDS provides cleaner gradient directions that modify the edited portions of the image while leaving others unchanged, yielding a distilled edit of the source image. The analysis presented in this paper supports the power of the new score for image-to-image translation. We further show that the new score can be used to train an effective zero-shot image translation model. The experimental results show that the proposed loss term outperforms existing methods in terms of stability and quality, highlighting its potential for real-world applications.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hertz_Delta_Denoising_Score_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hertz_Delta_Denoising_Score_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Amir Hertz</author><author>Kfir Aberman</author><author>Daniel Cohen-Or</author><author></author>
            </authors>
        </paper>
        

        <paper>
            <title>Hierarchical Prior Mining for Non-local Multi-View Stereo</title>
            <abstract>As a fundamental problem in computer vision, multi-view stereo (MVS) aims at recovering the 3D geometry of a target from a set of 2D images. Recent advances in MVS have shown that it is important to perceive non-local structured information for recovering geometry in low-textured areas. In this work, we propose a Hierarchical Prior Mining for Non-local Multi-View Stereo (HPM-MVS). The key characteristics are the following techniques that exploit non-local information to assist MVS: 1) A Non-local Extensible Sampling Pattern (NESP), which is able to adaptively change the size of sampled areas without becoming snared in locally optimal solutions. 2) A new approach to leverage non-local reliable points and construct a planar prior model based on K-Nearest Neighbor (KNN), to obtain potential hypotheses for the regions where prior construction is challenging. 3) A Hierarchical Prior Mining (HPM) framework, which is used to mine extensive non-local prior information at different scales to assist 3D model recovery, this strategy can achieve a considerable balance between the reconstruction of details and low-textured areas. Experimental results on the ETH3D and Tanks &amp; Temples have verified the superior performance and strong generalization capability of our method. Our code will be available at https://github.com/CLinvx/HPM-MVS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ren_Hierarchical_Prior_Mining_for_Non-local_Multi-View_Stereo_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ren_Hierarchical_Prior_Mining_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chunlin Ren</author><author>Qingshan Xu</author><author>Shikun Zhang</author><author>Jiaqi Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generative Multiplane Neural Radiance for 3D-Aware Image Generation</title>
            <abstract>We present a method to efficiently generate 3D-aware high-resolution images that are view-consistent across multiple target views. The proposed multiplane neural radiance model, named GMNR, consists of a novel a-guided view-dependent representation (a-VdR) module for learning view-dependent information. The a-VdR module, faciliated by an a-guided pixel sampling technique, computes the view-dependent representation efficiently by learning viewing direction and position coefficients. Moreover, we propose a view-consistency loss to enforce photometric similarity across multiple views. The GMNR model can generate 3D-aware high-resolution images that are view-consistent across multiple camera poses, while maintaining the computational efficiency in terms of both training and inference time. Experiments on three datasets demonstrate the effectiveness of the proposed modules, leading to favorable results in terms of both generation quality and inference time, compared to existing approaches. Our GMNR model generates 3D-aware images of 1024 x 1024 pixels with 17.6 FPS on a single V100. Code : https://github.com/VIROBO-15/GMNR</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Kumar_Generative_Multiplane_Neural_Radiance_for_3D-Aware_Image_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Kumar_Generative_Multiplane_Neural_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Amandeep Kumar</author><author>Ankan Kumar Bhunia</author><author>Sanath Narayan</author><author>Hisham Cholakkal</author><author>Rao Muhammad Anwer</author><author>Salman Khan</author><author>Ming-Hsuan Yang</author><author>Fahad Shahbaz Khan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings</title>
            <abstract>Semantic segmentation is a computer vision task that associates a label with each pixel in an image. Modern approaches tend to introduce class embeddings into semantic segmentation for deeply utilizing category semantics, and regard supervised class masks as final predictions. In this paper, we explore the mechanism of class embeddings and have an insight that more explicit and meaningful class embeddings can be generated based on class masks purposely. Following this observation, we propose ECENet, a new segmentation paradigm, in which class embeddings are obtained and enhanced explicitly during interacting with multi-stage image features. Based on this, we revisit the traditional decoding process and explore inverted information flow between segmentation masks and class embeddings. Furthermore, to ensure the discriminability and informativity of features from backbone, we propose a Feature Reconstruction module, which combines intrinsic and diverse branches together to ensure the concurrence of diversity and redundancy in features. Experiments show that our ECENet outperforms its counterparts on the ADE20K dataset with much less computational cost and achieves new state-of-the-art results on PASCAL-Context dataset. The code will be released at https://gitee.com/mindspore/models and https://github.com/Carol-lyh/ECENet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Boosting_Semantic_Segmentation_from_the_Perspective_of_Explicit_Class_Embeddings_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yuhe Liu</author><author>Chuanjian Liu</author><author>Kai Han</author><author>Quan Tang</author><author>Zengchang Qin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning to Identify Critical States for Reinforcement Learning from Videos</title>
            <abstract>Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-truth annotations, our new method called Deep State Identifier learns to predict returns from episodes encoded as videos. Then it uses a kind of mask-based sensitivity analysis to extract/identify important critical states. Extensive experiments showcase our method&apos;s potential for understanding and improving agent behavior. The source code and the generated datasets are available at https://github.com/AI-Initiative-KAUST/VideoRLCS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Learning_to_Identify_Critical_States_for_Reinforcement_Learning_from_Videos_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Learning_to_Identify_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Haozhe Liu</author><author>Mingchen Zhuge</author><author>Bing Li</author><author>Yuhui Wang</author><author>Francesco Faccio</author><author>Bernard Ghanem</author><author>J rgen Schmidhuber</author>
            </authors>
        </paper>
        

        <paper>
            <title>Editing Implicit Assumptions in Text-to-Image Diffusion Models</title>
            <abstract>Text-to-image diffusion models often make implicit assumptions about the world when generating images. While some assumptions are useful (e.g., the sky is blue), they can also be outdated, incorrect, or reflective of social biases present in the training data. Thus, there is a need to control these assumptions without requiring explicit user input or costly re-training. In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model. Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a &quot;source&quot; under-specified prompt for which the model makes an implicit assumption (e.g., &quot;a pack of roses&quot;), and a &quot;destination&quot; prompt that describes the same setting, but with a specified desired attribute (e.g., &quot;a pack of blue roses&quot;). TIME then updates the model&apos;s cross-attention layers, as these layers assign visual meaning to textual tokens. We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt. Our method is highly efficient, as it modifies a mere 2.2% of the model&apos;s parameters in under one second. To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains. Our experiments (using Stable Diffusion) show that TIME is successful in model editing, generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Orgad_Editing_Implicit_Assumptions_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hadas Orgad</author><author>Bahjat Kawar</author><author>Yonatan Belinkov</author>
            </authors>
        </paper>
        

        <paper>
            <title>Conceptual and Hierarchical Latent Space Decomposition for Face Editing</title>
            <abstract>Generative Adversarial Networks (GANs) can produce photo-realistic results using an unconditional image-generation pipeline. However, the images generated by GANs (e.g., StyleGAN) are entangled in feature spaces, which makes it difficult to interpret and control the contents of images. In this paper, we present an encoder-decoder model that decomposes the entangled GAN space into a conceptual and hierarchical latent space in a self-supervised manner. The outputs of 3D morphable face models are leveraged to independently control image synthesis parameters like pose, expression, and illumination. For this purpose, a novel latent space decomposition pipeline is introduced using transformer networks and generative models. Later, this new space is used to optimize a transformer-based GAN space controller for face editing. In this work, a StyleGAN2 model for faces is utilized. Since our method manipulates only GAN features, the photo-realism of StyleGAN2 is fully preserved. The results demonstrate that our method qualitatively and quantitatively outperforms baselines in terms of identity preservation and editing precision.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ozkan_Conceptual_and_Hierarchical_Latent_Space_Decomposition_for_Face_Editing_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ozkan_Conceptual_and_Hierarchical_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Savas Ozkan</author><author>Mete Ozay</author><author>Tom Robinson</author>
            </authors>
        </paper>
        

        <paper>
            <title>VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching</title>
            <abstract>Vision-Language Pretraining (VLP) has significantly improved the performance of various vision-language tasks with the matching of images and texts. In this paper, we propose VL-Match, a Vision-Language framework with Enhanced Token-level and Instance-level Matching. At the token level, a Vision-Language Replaced Token Detection task is designed to boost the substantial interaction between text tokens and images, where the text encoder of VLP works as a generator to generate a corrupted text, and the multimodal encoder of VLP works as a discriminator to predict whether each text token in the corrupted text matches the image. At the instance level, in the Image-Text Matching task that judges whether an image-text pair is matched, we propose a novel bootstrapping method to generate hard negative text samples that are different from the positive ones only at the token level. In this way, we can force the network to detect fine-grained differences between images and texts. Notably, with a smaller amount of parameters, VL-Match significantly outperforms previous SOTA on all image-text retrieval tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Junyu Bi</author><author>Daixuan Cheng</author><author>Ping Yao</author><author>Bochen Pang</author><author>Yuefeng Zhan</author><author>Chuanguang Yang</author><author>Yujing Wang</author><author>Hao Sun</author><author>Weiwei Deng</author><author>Qi Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach</title>
            <abstract>The extraction of road network is essential for the generation of high-definition maps since it enables the precise localization of road landmarks and their interconnections. However, generating road network poses a significant challenge due to the conflicting underlying combination of Euclidean (e.g., road landmarks location) and non-Euclidean (e.g., road topological connectivity) structures. Existing methods struggle to merge the two types of data domains effectively, but few of them address it properly. Instead, our work establishes a unified representation of both types of data domain by projecting both Euclidean and non-Euclidean data into an integer series called RoadNet Sequence. Further than modeling an auto-regressive sequence-to-sequence Transformer model to understand RoadNet Sequence, we decouple the dependency of RoadNet Sequence into a mixture of auto-regressive and non-autoregressive dependency. Building on this, our proposed non-autoregressive sequence-to-sequence approach leverages non-autoregressive dependencies while fixing the gap towards auto-regressive dependencies, resulting in success on both efficiency and accuracy. Extensive experiments on nuScenes dataset demonstrate the superiority of RoadNet Sequence representation and the non-autoregressive approach compared to existing state-of-the-art alternatives.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lu_Translating_Images_to_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiachen Lu</author><author>Renyuan Peng</author><author>Xinyue Cai</author><author>Hang Xu</author><author>Hongyang Li</author><author>Feng Wen</author><author>Wei Zhang</author><author>Li Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generative Novel View Synthesis with 3D-Aware Diffusion Models</title>
            <abstract>We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method&apos;s ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chan_Generative_Novel_View_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Eric R. Chan</author><author>Koki Nagano</author><author>Matthew A. Chan</author><author>Alexander W. Bergman</author><author>Jeong Joon Park</author><author>Axel Levy</author><author>Miika Aittala</author><author>Shalini De Mello</author><author>Tero Karras</author><author>Gordon Wetzstein</author>
            </authors>
        </paper>
        

        <paper>
            <title>ALWOD: Active Learning for Weakly-Supervised Object Detection</title>
            <abstract>Object detection (OD), a crucial vision task, remains challenged by the lack of large training datasets with precise object localization labels. In this work, we propose ALWOD, a new framework that addresses this problem by fusing active learning (AL) with weakly and semi-supervised object detection paradigms. Because the performance of AL critically depends on the model initialization, we propose a new auxiliary image generator strategy that utilizes an extremely small labeled set, coupled with a large weakly tagged set of images, as a warm-start for AL. We then propose a new AL acquisition function, another critical factor in AL success, that leverages the student-teacher OD pair disagreement and uncertainty to effectively propose the most informative images to annotate. Finally, to complete the AL loop, we introduce a new labeling task delegated to human annotators, based on selection and correction of model-proposed detections, which is both rapid and effective in labeling the informative images. We demonstrate, across several challenging benchmarks, that ALWOD significantly narrows the gap between the ODs trained on few partially labeled but strategically selected image instances and those that rely on the fully-labeled data. Our code is publicly available on https://github.com/seqam-lab/ALWOD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_ALWOD_Active_Learning_for_Weakly-Supervised_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_ALWOD_Active_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuting Wang</author><author>Velibor Ilic</author><author>Jiatong Li</author><author>Branislav Kisa anin</author><author>Vladimir Pavlovic</author>
            </authors>
        </paper>
        

        <paper>
            <title>S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces</title>
            <abstract>Neural rendering of implicit surfaces performs well in 3D vision applications. However, it requires dense input views as supervision. When only sparse input images are available, output quality drops significantly due to the shape-radiance ambiguity problem. We note that this ambiguity can be constrained when a 3D point is visible in multiple views, as is the case in multi-view stereo (MVS). We thus propose to regularize neural rendering optimization with an MVS solution. The use of an MVS probability volume and a generalized cross entropy loss leads to a noise-tolerant optimization process. In addition, neural rendering provides global consistency constraints that guide the MVS depth hypothesis sampling and thus improves MVS performance. Given only three sparse input views, experiments show that our method not only outperforms generic neural rendering models by a large margin but also significantly increases the reconstruction quality of MVS models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_S-VolSDF_Sparse_Multi-View_Stereo_Regularization_of_Neural_Implicit_Surfaces_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_S-VolSDF_Sparse_Multi-View_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Haoyu Wu</author><author>Alexandros Graikos</author><author>Dimitris Samaras</author>
            </authors>
        </paper>
        

        <paper>
            <title>TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation</title>
            <abstract>We propose TextManiA, a text-driven manifold augmentation method that semantically enriches visual feature spaces, regardless of class distribution. TextManiA augments visual data with intra-class semantic perturbation by exploiting easy-to-understand visually mimetic words, i.e., attributes. This work is built on an interesting hypothesis that general language models, e.g., BERT and GPT, encompass visual information to some extent, even without training on visual training data. Given the hypothesis, TextManiA transfers pre-trained text representation obtained from a well-established large language encoder to a target visual feature space being learned. Our extensive analysis hints that the language encoder indeed encompasses visual information at least useful to augment visual representation. Our experiments demonstrate that TextManiA is particularly powerful in scarce samples with class imbalance as well as even distribution. We also show compatibility with the label mix-based approaches in evenly distributed scarce data.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ye-Bin_TextManiA_Enriching_Visual_Feature_by_Text-driven_Manifold_Augmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ye-Bin_TextManiA_Enriching_Visual_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Moon Ye-Bin</author><author>Jisoo Kim</author><author>Hongyeob Kim</author><author>Kilho Son</author><author>Tae-Hyun Oh</author>
            </authors>
        </paper>
        

        <paper>
            <title>Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images</title>
            <abstract>Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2 still lag behind human performance on WHOOPS!. We hope our dataset will inspire the development of AI models with stronger visual commonsense reasoning abilities.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Bitton-Guetta_Breaking_Common_Sense_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nitzan Bitton-Guetta</author><author>Yonatan Bitton</author><author>Jack Hessel</author><author>Ludwig Schmidt</author><author>Yuval Elovici</author><author>Gabriel Stanovsky</author><author>Roy Schwartz</author>
            </authors>
        </paper>
        

        <paper>
            <title>Consistent Depth Prediction for Transparent Object Reconstruction from RGB-D Camera</title>
            <abstract>Transparent objects are commonly seen in indoor scenes but are hard to estimate. Currently, commercial depth cameras face difficulties in estimating the depth of transparent objects due to the light reflection and refraction on their surface. As a result, they tend to make a noisy and incorrect depth value for transparent objects. These incorrect depth data make the traditional RGB-D SLAM method fails in reconstructing the scenes that contain transparent objects. An exact depth value of the transparent object is required to restore in advance and it is essential that the depth value of the transparent object must keep consistent in different views, or the reconstruction result will be distorted. Previous depth prediction methods of transparent objects can restore these missing depth values but none of them can provide a good result in reconstruction due to the inconsistency prediction. In this work, we propose a real-time reconstruction method using a novel stereo-based depth prediction network to keep the consistency of depth prediction in a sequence of images. Because there is no video dataset about transparent objects currently to train our model, we construct a synthetic RGB-D video dataset with different transparent objects. Moreover, to test generalization capability, we capture video from real scenes using the RealSense D435i RGB-D camera. We compare the metrics on our dataset and SLAM reconstruction results in both synthetic scenes and real scenes with the previous methods. Experiments show our significant improvement in accuracy on depth prediction and scene reconstruction.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Cai_Consistent_Depth_Prediction_for_Transparent_Object_Reconstruction_from_RGB-D_Camera_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Cai_Consistent_Depth_Prediction_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Yuxiang Cai</author><author>Yifan Zhu</author><author>Haiwei Zhang</author><author>Bo Ren</author>
            </authors>
        </paper>
        

        <paper>
            <title>DETR Does Not Need Multi-Scale or Locality Design</title>
            <abstract>This paper presents an improved DETR detector that maintains a &quot;plain&quot; nature: using a single-scale feature map and global cross-attention calculations without specific locality constraints, in contrast to previous leading DETR-based detectors that reintroduce architectural inductive biases of multi-scale and locality into the decoder. We show that two simple technologies are surprisingly effective within a plain design to compensate for the lack of multi-scale feature maps and locality constraints. The first is a box-to-pixel relative position bias (BoxRPB) term added to the cross-attention formulation, which well guides each query to attend to the corresponding object region while also providing encoding flexibility. The second is masked image modeling (MIM)-based backbone pre-training which helps learn representation with fine-grained localization ability and proves crucial for remedying dependencies on the multi-scale feature maps. By incorporating these technologies and recent advancements in training and problem formation, the improved &quot;plain&quot; DETR showed exceptional improvements over the original DETR detector. By leveraging the Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using a Swin-L backbone, which is highly competitive with state-of-the-art detectors which all heavily rely on multi-scale feature maps and region-based feature extraction. Code will be available at https://github.com/impiga/Plain-DETR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lin_DETR_Does_Not_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yutong Lin</author><author>Yuhui Yuan</author><author>Zheng Zhang</author><author>Chen Li</author><author>Nanning Zheng</author><author>Han Hu</author>
            </authors>
        </paper>
        

        <paper>
            <title>ClusT3: Information Invariant Test-Time Training</title>
            <abstract>Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks. The code can be found at: https://github.com/dosowiechi/ClusT3.git</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hakim_ClusT3_Information_Invariant_Test-Time_Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hakim_ClusT3_Information_Invariant_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Gustavo A. Vargas Hakim</author><author>David Osowiechi</author><author>Mehrdad Noori</author><author>Milad Cheraghalikhani</author><author>Ali Bahri</author><author>Ismail Ben Ayed</author><author>Christian Desrosiers</author>
            </authors>
        </paper>
        

        <paper>
            <title>AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation</title>
            <abstract>Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efficient in practice. Inspired by this observation, we propose AssetField, a novel neural scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object editing, our ground feature plane representation offers a natural visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, deformation) on objects to configure a new scene. With the template feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on object individuals. We show that AssetField not only achieves competitive performance for novel-view synthesis but also generates realistic renderings for new scene configurations.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xiangli_AssetField_Assets_Mining_and_Reconfiguration_in_Ground_Feature_Plane_Representation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yuanbo Xiangli</author><author>Linning Xu</author><author>Xingang Pan</author><author>Nanxuan Zhao</author><author>Bo Dai</author><author>Dahua Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>SAGA: Spectral Adversarial Geometric Attack on 3D Meshes</title>
            <abstract>A triangular mesh is one of the most popular 3D data representations. As such, the deployment of deep neural networks for mesh processing is widely spread and is increasingly attracting more attention. However, neural networks are prone to adversarial attacks, where carefully crafted inputs impair the model&apos;s functionality. The need to explore these vulnerabilities is a fundamental factor in the future development of 3D-based applications. Recently, mesh attacks were studied on the semantic level, where classifiers are misled to produce wrong predictions. Nevertheless, mesh surfaces possess complex geometric attributes beyond their semantic meaning, and their analysis often includes the need to encode and reconstruct the geometry of the shape. We propose a novel framework for a geometric adversarial attack on a 3D mesh autoencoder. In this setting, an adversarial input mesh deceives the autoencoder by forcing it to reconstruct a different geometric shape at its output. The malicious input is produced by perturbing a clean shape in the spectral domain. Our method leverages the spectral decomposition of the mesh along with additional mesh-related properties to obtain visually credible results that consider the delicacy of surface distortions.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Stolik_SAGA_Spectral_Adversarial_Geometric_Attack_on_3D_Meshes_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Stolik_SAGA_Spectral_Adversarial_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Tomer Stolik</author><author>Itai Lang</author><author>Shai Avidan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Navigational Visual Representations with Semantic Map Supervision</title>
            <abstract>Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, both neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that human naturally build semantically and spatially meaningful cognitive maps in their brain during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent&apos;s egocentric views and semantic maps (Ego^2-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego^2-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent&apos;s egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperforms recent visual pre-training methods. Moreover, our representations lead to a significant improvement in vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hong_Learning_Navigational_Visual_Representations_with_Semantic_Map_Supervision_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hong_Learning_Navigational_Visual_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yicong Hong</author><author>Yang Zhou</author><author>Ruiyi Zhang</author><author>Franck Dernoncourt</author><author>Trung Bui</author><author>Stephen Gould</author><author>Hao Tan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Shortcut-V2V: Compression Framework for Video-to-Video Translation Based on Temporal Redundancy Reduction</title>
            <abstract>Video-to-video translation aims to generate video frames of a target domain from an input video. Despite its usefulness, the existing networks require enormous computations, necessitating their model compression for wide use. While there exist compression methods that improve computational efficiency in various image/video tasks, a generally applicable compression method for video-to-video translation has not been studied much. In response, we present Shortcut-V2V, a general-purpose compression framework for video-to-video translation. Shortcut-V2V avoids full inference for every neighboring video frame by approximating the intermediate features of a current frame from those of the previous frame. Moreover, in our framework, a newly-proposed block called AdaBD adaptively blends and deforms features of neighboring frames, which makes more accurate predictions of the intermediate features possible. We conduct quantitative and qualitative evaluations using well-known video-to-video translation models on various tasks to demonstrate the general applicability of our framework. The results show that Shortcut-V2V achieves comparable performance compared to the original video-to-video translation model while saving 3.2-5.7x computational cost and 7.8-44x memory at test time. Our code and videos are available at https://shortcut-v2v.github.io/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chung_Shortcut-V2V_Compression_Framework_for_Video-to-Video_Translation_Based_on_Temporal_Redundancy_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chung_Shortcut-V2V_Compression_Framework_for_Video-to-Video_Translation_Based_on_Temporal_Redundancy_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chaeyeon Chung</author><author>Yeojeong Park</author><author>Seunghwan Choi</author><author>Munkhsoyol Ganbat</author><author>Jaegul Choo</author>
            </authors>
        </paper>
        

        <paper>
            <title>SG-Former: Self-guided Transformer with Evolving Token Reallocation</title>
            <abstract>Vision Transformer has demonstrated impressive success across various vision tasks. However, its heavy computation cost, which grows quadratically with respect to the token sequence length, largely limits its power in handling large feature maps. To alleviate the computation cost, previous works rely on either fine-grained self-attentions restricted to local small regions, or global self-attentions but to shorten the sequence length resulting in coarse granularity. In this paper, we propose a novel model, termed as Self-guided Transformer (SG-Former), towards effective global self-attention with adaptive fine granularity. At the heart of our approach is to utilize a significance map, which is estimated through hybrid-scale self-attention and evolves itself during training, to reallocate tokens based on the significance of each region. Intuitively, we assign more tokens to the salient regions for achieving fine-grained attention, while allocating fewer tokens to the minor regions in exchange for efficiency and global receptive fields. The proposed SG-Former achieves performance superior to state of the art: our base size model achieves 84.7% Top-1 accuracy on ImageNet-1K, 51.2mAP bbAP on CoCo, 52.7mIoU on ADE20K surpassing the Swin Transformer by +1.3% / +2.7 mAP/ +3 mIoU, with lower computation costs and fewer parameters.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Sucheng Ren</author><author>Xingyi Yang</author><author>Songhua Liu</author><author>Xinchao Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>ProtoTransfer: Cross-Modal Prototype Transfer for Point Cloud Segmentation</title>
            <abstract>Knowledge transfer from multi-modal, i.e., LiDAR points and images, to a single LiDAR modal can take advantage of complimentary information from modal-fusion but keep a single modal inference speed, showing a promising direction for point cloud semantic segmentation in autonomous driving. Recent advances in point cloud segmentation distill knowledge from strictly aligned point-pixel fusion features while leaving a large number of unmatched image pixels unexplored and unmatched LiDAR points under-benefited. In this paper, we propose a novel approach, named ProtoTransfer, which not only fully exploits image representations but also transfers the learned multi-modal knowledge to all point cloud features. Specifically, based on the basic multi-modal learning framework, we build up a class-wise prototype bank from the strictly-aligned fusion features and encourage all the point cloud features to learn from the prototypes during model training. Moreover, to exploit the massive unmatched point and pixel features, we use a pseudo-labeling scheme and further accumulate these features into the class-wise prototype bank with a carefully designed fusion strategy. Without bells and whistles, our approach demonstrates superior performance over the published state-of-the-arts on two large-scale benchmarks, i.e., nuScenes and SemanticKITTI, and ranks 2nd on the competitive nuScenes Lidarseg challenge leaderboard.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tang_ProtoTransfer_Cross-Modal_Prototype_Transfer_for_Point_Cloud_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tang_ProtoTransfer_Cross-Modal_Prototype_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Pin Tang</author><author>Hai-Ming Xu</author><author>Chao Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation</title>
            <abstract>Given a composite image, image harmonization aims to adjust the foreground illumination to be consistent with background. Previous methods have explored transforming foreground features to achieve competitive performance. In this work, we show that using global information to guide foreground feature transformation could achieve significant improvement. Besides, we propose to transfer the foreground-background relation from real images to composite images, which can provide intermediate supervision for the transformed encoder features. Additionally, considering the drawbacks of existing harmonization datasets, we also contribute a ccHarmony dataset which simulates the natural illumination variation. Extensive experiments on iHarmony4 and our contributed dataset demonstrate the superiority of our method. Our ccHarmony dataset is released at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Niu_Deep_Image_Harmonization_with_Globally_Guided_Feature_Transformation_and_Relation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Niu_Deep_Image_Harmonization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Li Niu</author><author>Linfeng Tan</author><author>Xinhao Tao</author><author>Junyan Cao</author><author>Fengjun Guo</author><author>Teng Long</author><author>Liqing Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>VQ3D: Learning a 3D-Aware Generative Model on ImageNet</title>
            <abstract>Recent work has shown the possibility of training generative models of 3D content from 2D image collections on small datasets corresponding to a single object class, such as human faces, animal faces, or cars. However, these models struggle on larger, more complex datasets. To model diverse and unconstrained image collections such as ImageNet, we present VQ3D, which introduces a NeRF-based decoder into a two-stage vector-quantized autoencoder. Our Stage 1 allows for the reconstruction of an input image and the ability to change the camera position around the image, and our Stage 2 allows for the generation of new 3D scenes. VQ3D is capable of generating and reconstructing 3D-aware images from the 1000-class ImageNet dataset of 1.2 million training images, and achieves a competitive ImageNet generation FID score of 16.8.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sargent_VQ3D_Learning_a_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kyle Sargent</author><author>Jing Yu Koh</author><author>Han Zhang</author><author>Huiwen Chang</author><author>Charles Herrmann</author><author>Pratul Srinivasan</author><author>Jiajun Wu</author><author>Deqing Sun</author>
            </authors>
        </paper>
        

        <paper>
            <title>2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision</title>
            <abstract>We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are iteratively enriched by each other. Experiments show that it performs favorably against existing weakly supervised point cloud segmentation methods by a large margin on the S3DIS and ScanNet benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_2D-3D_Interlaced_Transformer_for_Point_Cloud_Segmentation_with_Scene-Level_Supervision_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_2D-3D_Interlaced_Transformer_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Cheng-Kun Yang</author><author>Min-Hung Chen</author><author>Yung-Yu Chuang</author><author>Yen-Yu Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures</title>
            <abstract>Human pose transfer synthesizes new view(s) of a person for a given pose. Recent work achieves this via self-reconstruction, which disentangles a person&apos;s pose and texture information by breaking down the person into several parts, then recombines them to reconstruct the person. However, this part-level disentanglement preserves some pose information that can create unwanted artifacts. In this paper, we propose Pose Transfer by Permuting Textures, a self-driven human pose transfer approach that disentangles pose from texture at the patch-level. Specifically, we remove pose from an input image by permuting image patches so only texture information remains. Then we reconstruct the input image by sampling from the permuted textures to achieve patch-level disentanglement. To reduce the noise and recover clothing shape information from the permuted patches, we employ encoders with multiple kernel sizes in a triple branch network. Extensive experiments on DeepFashion and Market-1501 show that our model improves the quality of generated images in terms of FID, LPIPS and SSIM over other self-driven methods, and even outperforming some fully-supervised methods. A user study also shows that among self-driven approaches, images generated by our method are preferred in 68% of cases over prior work. Code is available at https://github.com/NannanLi999/pt_square.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Collecting_The_Puzzle_Pieces_Disentangled_Self-Driven_Human_Pose_Transfer_by_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Collecting_The_Puzzle_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nannan Li</author><author>Kevin J Shih</author><author>Bryan A. Plummer</author>
            </authors>
        </paper>
        

        <paper>
            <title>Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation</title>
            <abstract>The images and sounds that we perceive undergo subtle but geometrically consistent changes as we rotate our heads. In this paper, we use these cues to solve a problem we call Sound Localization from Motion (SLfM): jointly estimating camera rotation and localizing sound sources. We learn to solve these tasks solely through self-supervision. A visual model predicts camera rotation from a pair of images, while an audio model predicts the direction of sound sources from binaural sounds. We train these models to generate predictions that agree with one another. At test time, the models can be deployed independently. To obtain a feature representation that is well-suited to solving this challenging problem, we also propose a method for learning an audio-visual representation through cross-view binauralization: estimating binaural sound from one view, given images and sound from another. Our model can successfully estimate accurate rotations on both real and synthetic scenes, and localize sound sources with accuracy competitive with state-of-the-art self-supervised approaches.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Sound_Localization_from_Motion_Jointly_Learning_Sound_Direction_and_Camera_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Sound_Localization_from_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ziyang Chen</author><author>Shengyi Qian</author><author>Andrew Owens</author>
            </authors>
        </paper>
        

        <paper>
            <title>Prompt Tuning Inversion for Text-driven Image Editing Using Diffusion Models</title>
            <abstract>Recently large-scale language-image models (e.g., text-guided diffusion models) have considerably improved the image generation capabilities to generate photorealistic images in various domains. Based on this success, current image editing methods use texts to achieve intuitive and versatile modification of images. To edit a real image using diffusion models, one must first invert the image to a noisy latent from which an edited image is sampled with a target text prompt. However, most methods lack one of the following: user-friendliness (e.g., additional masks or precise descriptions of the input image are required), generalization to larger domains, or high fidelity to the input image. In this paper, we design an accurate and quick inversion technique, Prompt Tuning Inversion, for text-driven image editing. Specifically, our proposed editing method consists of a reconstruction stage and an editing stage. In the first stage, we encode the information of the input image into a learnable conditional embedding via Prompt Tuning Inversion. In the second stage, we apply classifier-free guidance to sample the edited image, where the conditional embedding is calculated by linearly interpolating between the target embedding and the optimized one obtained in the first stage. This technique ensures a superior trade-off between editability and high fidelity to the input image of our method. For example, we can change the color of a specific object while preserving its original shape and background under the guidance of only a target text prompt. Extensive experiments on ImageNet demonstrate the superior editing performance of our method compared to the state-of-the-art baselines.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Dong_Prompt_Tuning_Inversion_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Wenkai Dong</author><author>Song Xue</author><author>Xiaoyue Duan</author><author>Shumin Han</author>
            </authors>
        </paper>
        

        <paper>
            <title>UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation</title>
            <abstract>Human generation has achieved significant progress. Nonetheless, existing methods still struggle to synthesize specific regions such as faces and hands. We argue that the main reason is rooted in the training data. A holistic human dataset inevitably has insufficient and low-resolution information on local parts. Therefore, we propose to use multi-source datasets with various resolution images to jointly learn a high-resolution human generative model. However, multi-source data inherently a) contains different parts that do not spatially align into a coherent human, and b) comes with different scales. To tackle these challenges, we propose an end-to-end framework, UnitedHuman, that empowers continuous GAN with the ability to effectively utilize multi-source data for high-resolution human generation. Specifically, 1) we design a Multi-Source Spatial Transformer that spatially aligns multi-source images to full-body space with a human parametric model. 2) Next, a continuous GAN is proposed with global-structural guidance and CutMix consistency. Patches from different datasets are then sampled and transformed to supervise the training of this scale-invariant generative model. Extensive experiments demonstrate that our model jointly learned from multi-source data achieves superior quality than those learned from a holistic dataset.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fu_UnitedHuman_Harnessing_Multi-Source_Data_for_High-Resolution_Human_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fu_UnitedHuman_Harnessing_Multi-Source_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Jianglin Fu</author><author>Shikai Li</author><author>Yuming Jiang</author><author>Kwan-Yee Lin</author><author>Wayne Wu</author><author>Ziwei Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Neural Microfacet Fields for Inverse Rendering</title>
            <abstract>We present Neural Microfacet Fields, a method for recovering materials, geometry (volumetric density), and environmental illumination from a collection of images of a scene. Our method applies a microfacet reflectance model within a volumetric setting by treating each sample along the ray as a surface, rather than an emitter. Using surface-based Monte Carlo rendering in a volumetric setting enables our method to perform inverse rendering efficiently and enjoy recent advances in volume rendering. Our approach obtains similar performance as state-of-the-art methods for novel view synthesis and outperforms prior work in inverse rendering, capturing high fidelity geometry and high frequency illumination details.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mai_Neural_Microfacet_Fields_for_Inverse_Rendering_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mai_Neural_Microfacet_Fields_for_Inverse_Rendering_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Alexander Mai</author><author>Dor Verbin</author><author>Falko Kuester</author><author>Sara Fridovich-Keil</author>
            </authors>
        </paper>
        

        <paper>
            <title>Understanding Self-attention Mechanism via Dynamical System Perspective</title>
            <abstract>The self-attention mechanism (SAM) is widely used in various fields of artificial intelligence and has successfully boosted the performance of different models. However, current explanations of this mechanism are mainly based on intuitions and experiences, while there still lacks direct modeling for how the SAM helps performance. To mitigate this issue, in this paper, based on the dynamical system perspective of the residual neural network, we first show that the intrinsic stiffness phenomenon (SP) in the high-precision solution of ordinary differential equations (ODEs) also widely exists in high-performance neural networks (NN). Thus the ability of NN to measure SP at the feature level is necessary to obtain high performance and is an important factor in the difficulty of training NN. Similar to the adaptive step-size method which is effective in solving stiff ODEs, we show that the SAM is also a stiffness-aware step size adaptor that can enhance the model&apos;s representational ability to measure intrinsic SP by refining the estimation of stiffness information and generating adaptive attention values, which provides a new understanding about why and how the SAM can benefit the model performance. This novel perspective can also explain the lottery ticket hypothesis in SAM, design new quantitative metrics of representational ability, and inspire a new theoretic-inspired approach, StepNet. Extensive experiments on several popular benchmarks demonstrate that StepNet can extract fine-grained stiffness information and measure SP accurately, leading to significant improvements in various visual tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Understanding_Self-attention_Mechanism_via_Dynamical_System_Perspective_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Understanding_Self-attention_Mechanism_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhongzhan Huang</author><author>Mingfu Liang</author><author>Jinghui Qin</author><author>Shanshan Zhong</author><author>Liang Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>DDG-Net: Discriminability-Driven Graph Network for Weakly-supervised Temporal Action Localization</title>
            <abstract>Weakly-supervised temporal action localization (WTAL) is a practical yet challenging task. Due to large-scale datasets, most existing methods use a network pretrained in other datasets to extract features, which are not suitable enough for WTAL. To address this problem, researchers design several modules for feature enhancement, which improve the performance of the localization module, especially modeling the temporal relationship between snippets. However, all of them omit that ambiguous snippets deliver contradictory information, which would reduce the discriminability of linked snippets. Considering this phenomenon, we propose Discriminability-Driven Graph Network (DDG-Net), which explicitly models ambiguous snippets and discriminative snippets with well-designed connections, preventing the transmission of ambiguous information and enhancing the discriminability of snippet-level representations. Additionally, we propose feature consistency loss to prevent the assimilation of features and drive the graph convolution network to generate more discriminative representations. Extensive experiments on THUMOS14 and ActivityNet1.2 benchmarks demonstrate the effectiveness of DDG-Net, establishing new state-of-the-art results on both datasets. Source code is available at https://github.com/XiaojunTang22/ICCV2023-DDGNet.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tang_DDG-Net_Discriminability-Driven_Graph_Network_for_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tang_DDG-Net_Discriminability-Driven_Graph_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiaojun Tang</author><author>Junsong Fan</author><author>Chuanchen Luo</author><author>Zhaoxiang Zhang</author><author>Man Zhang</author><author>Zongyuan Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Rethinking Data Distillation: Do Not Overlook Calibration</title>
            <abstract>Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Rethinking_Data_Distillation_Do_Not_Overlook_Calibration_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Rethinking_Data_Distillation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dongyao Zhu</author><author>Bowen Lei</author><author>Jie Zhang</author><author>Yanbo Fang</author><author>Yiqun Xie</author><author>Ruqi Zhang</author><author>Dongkuan Xu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Building Vision Transformers with Hierarchy Aware Feature Aggregation</title>
            <abstract>Thanks to the excellent global modeling capability of attention mechanisms, the Vision Transformer has achieved better results than ConvNet in many computer tasks. However, in generating hierarchical feature maps, the Transformer still adopts the ConvNet feature aggregation scheme. This leads to the problem that the semantic information of the grid area of image becomes confused after feature aggregation, making it difficult for attention to accurately model global relationships. To address this, we propose the Hierarchy Aware Feature Aggregation framework (HAFA). HAFA enhances the extraction of local features adaptively in shallow layers where semantic information is weak, while is able to aggregate patches with similar semantics in deep layers. The clear semantic information of the aggregated patches, enables the attention mechanism to more accurately model global information at the semantic level. Extensive experiments show that after using the HAFA framework, significant improvements have been achieved relative to the baseline models in image classification, object detection, and semantic segmentation tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Building_Vision_Transformers_with_Hierarchy_Aware_Feature_Aggregation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_Building_Vision_Transformers_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yongjie Chen</author><author>Hongmin Liu</author><author>Haoran Yin</author><author>Bin Fan</author>
            </authors>
        </paper>
        

        <paper>
            <title>SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation</title>
            <abstract>Recently, private inference (PI) has addressed the rising concern over data and model privacy in machine learning inference as a service. However, existing PI frameworks suffer from high computational and communication overheads due to the expensive multi-party computation (MPC) protocols, particularly for large models such as vision transformers (ViT). The majority of this overhead is due to the encrypted softmax operation in each self-attention layer. In this work, we present SAL-ViT with two novel techniques to boost PI efficiency on ViTs. Our first technique is a learnable PI-efficient approximation to softmax, namely, learnable 2Quad (L2Q), that introduces learnable scaling and shifting parameters to the prior 2Quad softmax approximation, enabling improvement in accuracy. Then, given our observation that external attention (EA) presents lower PI latency than widely-adopted self-attention (SA) at the cost of accuracy, we present a selective attention search (SAS) method to integrate the strength of EA and SA. Specifically, for a given lightweight EA ViT, we leverage a constrained optimization procedure to selectively search and replace EA modules with SA alternatives to maximize the accuracy. Our extensive experiments show that our SAL-ViT can averagely achieve 1.28x, 1.28x, 1.14x lower PI latency with 1.79%, 1.41%, and 2.08% higher accuracy compared to the existing alternatives, on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_SAL-ViT_Towards_Latency_Efficient_Private_Inference_on_ViT_using_Selective_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yuke Zhang</author><author>Dake Chen</author><author>Souvik Kundu</author><author>Chenghao Li</author><author>Peter A. Beerel</author>
            </authors>
        </paper>
        

        <paper>
            <title>TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models</title>
            <abstract>We present a Multimodal Backdoor defense technique TIJO (Trigger Inversion using Joint Optimization). Recently Walmer et al. demonstrated successful backdoor attacks on multimodal models for the Visual Question Answering task. Their dual-key backdoor trigger is split across two modalities (image and text), such that the backdoor is activated if and only if the trigger is present in both modalities. We propose TIJO that defends against dual-key attacks through a joint optimization that reverse-engineers the trigger in both the image and text modalities. This joint optimization is challenging in multimodal models due to the disconnected nature of the visual pipeline which consists of an offline feature extractor, whose output is then fused with the text using a fusion module. The key insight enabling the joint optimization in TIJO is that the trigger inversion needs to be carried out in the object detection box feature space as opposed to the pixel space. We demonstrate the effectiveness of our method on the TrojVQA benchmark, where TIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to 0.92 on multimodal dual-key backdoors. Furthermore, our method also improves upon the unimodal baselines on unimodal backdoors. We also present detailed ablation studies as well as qualitative results to provide insights into our algorithm such as the critical importance of overlaying the inverted feature triggers on all visual features during trigger inversion.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sur_TIJO_Trigger_Inversion_with_Joint_Optimization_for_Defending_Multimodal_Backdoored_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sur_TIJO_Trigger_Inversion_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Indranil Sur</author><author>Karan Sikka</author><author>Matthew Walmer</author><author>Kaushik Koneripalli</author><author>Anirban Roy</author><author>Xiao Lin</author><author>Ajay Divakaran</author><author>Susmit Jha</author>
            </authors>
        </paper>
        

        <paper>
            <title>Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting</title>
            <abstract>In this paper, we investigate the adversarial robustness of vision transformers that are equipped with BERT pretraining (e.g., BEiT, MAE). A surprising observation is that MAE has significantly worse adversarial robustness than other BERT pretraining methods. This observation drives us to rethink the basic differences between these BERT pretraining methods and how these differences affect the robustness against adversarial perturbations. Our empirical analysis reveals that the adversarial robustness of BERT pretraining is highly related to the reconstruction target, i.e., predicting the raw pixels of masked image patches will degrade more adversarial robustness of the model than predicting the semantic context, since it guides the model to concentrate more on medium-/high-frequency components of images. Based on our analysis, we provide a simple yet effective way to boost the adversarial robustness of MAE. The basic idea is using the dataset-extracted domain knowledge to occupy the medium-/high-frequency of images, thus narrowing the optimization space of adversarial perturbations. Specifically, we group the distribution of pretraining data and optimize a set of cluster-specific visual prompts on frequency domain. These prompts are incorporated with input images through prototype-based prompt selection during test period. Extensive evaluation shows that our method clearly boost MAE&apos;s adversarial robustness while maintaining its clean performance on ImageNet-1k classification. Our code is available at: https://github.com/shikiw/RobustMAE.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Improving_Adversarial_Robustness_of_Masked_Autoencoders_via_Test-time_Frequency-domain_Prompting_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Improving_Adversarial_Robustness_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qidong Huang</author><author>Xiaoyi Dong</author><author>Dongdong Chen</author><author>Yinpeng Chen</author><author>Lu Yuan</author><author>Gang Hua</author><author>Weiming Zhang</author><author>Nenghai Yu</author>
            </authors>
        </paper>
        

        <paper>
            <title>The Making and Breaking of Camouflage</title>
            <abstract>Not all camouflages are equally effective, as even a partially visible contour or a slight color difference can make the animal stand out and break its camouflage. In this paper, we address the question of what makes a camouflage successful, by proposing three scores for automatically assessing its effectiveness. In particular, we show that camouflage can be measured by the similarity between background and foreground features and boundary visibility. We use these camouflage scores to assess and compare all available camouflage datasets. We also incorporate the proposed camouflage score into a generative model as an auxiliary loss and show that effective camouflage images or videos can be synthesised in a scalable manner. The generated synthetic dataset is used to train a transformer-based model for segmenting camouflaged animals in videos. Experimentally, we demonstrate state-of-the-art camouflage breaking performance on the public MoCA-Mask benchmark.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lamdouar_The_Making_and_Breaking_of_Camouflage_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lamdouar_The_Making_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hala Lamdouar</author><author>Weidi Xie</author><author>Andrew Zisserman</author>
            </authors>
        </paper>
        

        <paper>
            <title>Object as Query: Lifting Any 2D Object Detector to 3D Detection</title>
            <abstract>3D object detection from multi-view images has drawn much attention over the past few years. Existing methods mainly establish 3D representations from multi-view images and adopt a dense detection head for object detection, or employ object queries distributed in 3D space to localize objects. In this paper, we design Multi-View 2D Objects guided 3D Object Detector (MV2D), which can lift any 2D object detector to multi-view 3D object detection. Since 2D detections can provide valuable priors for object existence, MV2D exploits 2D detectors to generate object queries conditioned on the rich image semantics. These dynamically generated queries help MV2D to recall objects in the field of view and show a strong capability of localizing 3D objects. For the generated queries, we design a sparse cross attention module to force them to focus on the features of specific objects, which suppresses interference from noises. The evaluation results on the nuScenes dataset demonstrate the dynamic object queries and sparse feature aggregation can promote 3D detection capability. MV2D also exhibits a state-of-the-art performance among existing methods. We hope MV2D can serve as a new baseline for future research.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Object_as_Query_Lifting_Any_2D_Object_Detector_to_3D_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Object_as_Query_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zitian Wang</author><author>Zehao Huang</author><author>Jiahui Fu</author><author>Naiyan Wang</author><author>Si Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Versatile Diffusion: Text, Images and Variations All in One Diffusion Model</title>
            <abstract>Recent advances in diffusion models have set an impressive milestone in many generation tasks, and trending works such as DALL-E2, Imagen, and Stable Diffusion have attracted great interest. Despite the rapid landscape changes, recent new approaches focus on extensions and performance rather than capacity, thus requiring separate models for separate tasks. In this work, we expand the existing single-flow diffusion pipeline into a multi-task multimodal network, dubbed Versatile Diffusion (VD), that handles multiple flows of text-to-image, image-to-text, and variations in one unified model. The pipeline design of VD instantiates a unified multi-flow diffusion framework, consisting of sharable and swappable layer modules that enable the crossmodal generality beyond images and text. Through extensive experiments, we demonstrate that VD successfully achieves the following: a) VD outperforms the baseline approaches and handles all its base tasks with competitive quality; b) VD enables novel extensions such as disentanglement of style and semantics, dual- and multi-context blending, etc.; c) The success of our multi-flow multimodal framework over images and text may inspire further diffusion-based universal AI research. Our code and models are open-sourced at https://github.com/SHI-Labs/Versatile-Diffusion.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xu_Versatile_Diffusion_Text_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xingqian Xu</author><author>Zhangyang Wang</author><author>Gong Zhang</author><author>Kai Wang</author><author>Humphrey Shi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs</title>
            <abstract>This paper aims to develop an accurate 3D geometry representation of satellite images using satellite-ground image pairs. Our focus is on the challenging problem of 3D-aware ground-views synthesis from a satellite image. We draw inspiration from the density field representation used in volumetric neural rendering and propose a new approach, called Sat2Density. Our method utilizes the properties of ground-view panoramas for the sky and non-sky regions to learn faithful density fields of 3D scenes in a geometric perspective. Unlike other methods that require extra depth information during training, our Sat2Density can automatically learn accurate and faithful 3D geometry via density representation without depth supervision. This advancement significantly improves the ground-view panorama synthesis task. Additionally, our study provides a new geometric perspective to understand the relationship between satellite and ground-view images in 3D space.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Qian_Sat2Density_Faithful_Density_Learning_from_Satellite-Ground_Image_Pairs_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Qian_Sat2Density_Faithful_Density_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ming Qian</author><author>Jincheng Xiong</author><author>Gui-Song Xia</author><author>Nan Xue</author>
            </authors>
        </paper>
        

        <paper>
            <title>Expressive Text-to-Image Generation with Rich Text</title>
            <abstract>Plain text has become a prevalent interface for text-to-image synthesis. However, its limited customization options hinder users from accurately describing desired outputs. For example, plain text makes it hard to specify continuous quantities, such as the precise RGB color value or importance of each word. Furthermore, creating detailed text prompts for complex scenes is tedious for humans to write and challenging for text encoders to interpret. To address these challenges, we propose using a rich-text editor supporting formats such as font style, size, color, and footnote. We extract each word&apos;s attributes from rich text to enable local style control, explicit token reweighting, precise color rendering, and detailed region synthesis. We achieve these capabilities through a region-based diffusion process. We first obtain each word&apos;s region based on attention maps of a diffusion process using plain text. For each region, we enforce its text attributes by creating region-specific detailed prompts and applying region-specific guidance, and maintain its fidelity against plain-text generation through region-based injections. We present various examples of image generation from rich text and demonstrate that our method outperforms strong baselines with quantitative evaluations.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ge_Expressive_Text-to-Image_Generation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Songwei Ge</author><author>Taesung Park</author><author>Jun-Yan Zhu</author><author>Jia-Bin Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Text-Driven Generative Domain Adaptation with Spectral Consistency Regularization</title>
            <abstract>Combined with the generative prior of pre-trained models and the flexibility of text, text-driven generative domain adaptation can generate images from a wide range of target domains. However, current methods still suffer from overfitting and the mode collapse problem. In this paper, we analyze the mode collapse from the geometric point of view and reveal its relationship to the Hessian matrix of generator. To alleviate it, we propose the spectral consistency regularization to preserve the diversity of source domain without restricting the semantic adaptation to target domain. We also design granularity adaptive regularization to flexibly control the balance between diversity and stylization for target model. We conduct experiments for broad target domains compared with state-of-the-art methods and extensive ablation studies. The experiments demonstrate the effectiveness of our method to preserve the diversity of source domain and generate high fidelity target images.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Text-Driven_Generative_Domain_Adaptation_with_Spectral_Consistency_Regularization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Zhenhuan Liu</author><author>Liang Li</author><author>Jiayu Xiao</author><author>Zheng-Jun Zha</author><author>Qingming Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Neural Reconstruction of Relightable Human Model from Monocular Video</title>
            <abstract>Creating relightable and animatable human characters from monocular video at a low cost is a critical task for digital human modeling and virtual reality applications. This task is complex due to intricate articulation motion, a wide range of ambient lighting conditions, and pose-dependent clothing deformations. In this paper, we introduce a novel self-supervised framework that takes a monocular video of a moving human as input and generates a 3D neural representation capable of being rendered with novel poses under arbitrary lighting conditions. Our framework decomposes dynamic humans under varying illumination into neural fields in canonical space, taking into account geometry and spatially varying BRDF material properties. Additionally, we introduce pose-driven deformation fields, enabling bidirectional mapping between canonical space and observation. Leveraging the proposed appearance decomposition and deformation fields, our framework learns in a self-supervised manner. Ultimately, based on pose-driven deformation, recovered appearance, and physically-based rendering, the reconstructed human figure becomes relightable and can be explicitly driven by novel poses. We demonstrate significant performance improvements over previous works and provide compelling examples of relighting from monocular videos of moving humans in challenging, uncontrolled capture scenarios.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sun_Neural_Reconstruction_of_Relightable_Human_Model_from_Monocular_Video_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Sun_Neural_Reconstruction_of_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Wenzhang Sun</author><author>Yunlong Che</author><author>Han Huang</author><author>Yandong Guo</author>
            </authors>
        </paper>
        

        <paper>
            <title>FB-BEV: BEV Representation from Forward-Backward View Transformations</title>
            <abstract>View Transformation Module (VTM), where transformations happen between multi-view image features and Bird-Eye-View (BEV) representation, is a crucial step in camera-based BEV perception systems. Currently, the two most prominent VTM paradigms are forward projection and backward projection. Forward projection, represented by Lift-Splat-Shoot, leads to sparsely projected BEV features without post-processing. Backward projection, with BEVFormer being an example, tends to generate false-positive BEV features from incorrect projections due to the lack of utilization on depth. To address the above limitations, we propose a novel forward-backward view transformation module. Our approach compensates for the deficiencies in both existing methods, allowing them to enhance each other to obtain higher quality BEV representations mutually. We instantiate the proposed module with FB-BEV, which achieves a new state-of-the-art result of 62.4% NDS on the nuScenes test set. Code and models are available at https://github.com/NVlabs/FB-BEV</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_FB-BEV_BEV_Representation_from_Forward-Backward_View_Transformations_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_FB-BEV_BEV_Representation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhiqi Li</author><author>Zhiding Yu</author><author>Wenhai Wang</author><author>Anima Anandkumar</author><author>Tong Lu</author><author>Jose M. Alvarez</author>
            </authors>
        </paper>
        

        <paper>
            <title>BoxSnake: Polygonal Instance Segmentation with Box Supervision</title>
            <abstract>Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that constrains the bounding box of predicted polygons to achieve coarse-grained segmentation; and (2) a distance-aware pairwise loss that encourages the predicted polygons to fit the object boundaries. Compared with the mask-based weakly-supervised methods, BoxSnake further reduces the performance gap between the predicted segmentation and the bounding box, and shows significant superiority on the Cityscapes dataset.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_BoxSnake_Polygonal_Instance_Segmentation_with_Box_Supervision_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_BoxSnake_Polygonal_Instance_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Rui Yang</author><author>Lin Song</author><author>Yixiao Ge</author><author>Xiu Li</author>
            </authors>
        </paper>
        

        <paper>
            <title>ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field</title>
            <abstract>Physical simulations produce excellent predictions of weather effects. Neural radiance fields produce SOTA scene models. We describe a novel NeRF-editing procedure that can fuse physical simulations with NeRF models of scenes, producing realistic movies of physical phenomena in those scenes. Our application -- Climate NeRF -- allows people to visualize what climate change outcomes will do to them. ClimateNeRF allows us to render realistic weather effects, including smog, snow, and flood. Results can be controlled with physically meaningful variables like water level. Qualitative and quantitative studies show that our simulated results are significantly more realistic than those from SOTA 2D image editing and SOTA 3D NeRF stylization.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_ClimateNeRF_Extreme_Weather_Synthesis_in_Neural_Radiance_Field_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_ClimateNeRF_Extreme_Weather_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yuan Li</author><author>Zhi-Hao Lin</author><author>David Forsyth</author><author>Jia-Bin Huang</author><author>Shenlong Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection</title>
            <abstract>Single-frame infrared small target (SIRST) detection aims at separating small targets from clutter backgrounds on infrared images. Recently, deep learning based methods have achieved promising performance on SIRST detection, but at the cost of a large amount of training data with expensive pixel-level annotations. To reduce the annotation burden, we propose the first method to achieve SIRST detection with single-point supervision. The core idea of this work is to recover the per-pixel mask of each target from the given single point label by using clustering approaches, which looks simple but is indeed challenging since targets are always insalient and accompanied with background clutters. To handle this issue, we introduce randomness to the clustering process by adding noise to the input images, and then obtain much more reliable pseudo masks by averaging the clustered results. Thanks to this &quot;Monte Carlo&quot; clustering approach, our method can accurately recover pseudo masks and thus turn arbitrary fully supervised SIRST detection networks into weakly supervised ones with only single point annotation. Experiments on four datasets demonstrate that our method can be applied to existing SIRST detection networks to achieve comparable performance with their fully-supervised counterparts, which reveals that single-point supervision is strong enough for SIRST detection.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Monte_Carlo_Linear_Clustering_with_Single-Point_Supervision_is_Enough_for_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Monte_Carlo_Linear_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Boyang Li</author><author>Yingqian Wang</author><author>Longguang Wang</author><author>Fei Zhang</author><author>Ting Liu</author><author>Zaiping Lin</author><author>Wei An</author><author>Yulan Guo</author>
            </authors>
        </paper>
        

        <paper>
            <title>Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study</title>
            <abstract>Membership inference attacks (MIAs) aim to infer whether a data point has been used to train a machine learning model. These attacks can be employed to identify potential privacy vulnerabilities and detect unauthorized use of personal data. While MIAs have been traditionally studied for simple classification models, recent advancements in multi-modal pre-training, such as CLIP, have demonstrated remarkable zero-shot performance across a range of computer vision tasks. However, the sheer scale of data and models presents significant computational challenges for performing the attacks. This paper takes a first step towards developing practical MIAs against large-scale multi-modal models. We introduce a simple baseline strategy by thresholding the cosine similarity between text and image features of a target point, and propose further enhancing the baseline by aggregating cosine similarity across transformations of the target. We also present a new weakly supervised attack method that leverages ground-truth non-members (e.g., obtained by using the publication date of a target model and the timestamps of the open data) to further enhance the attack. Our evaluation shows that CLIP models are susceptible to our attack strategies, with our simple baseline achieving over 75% membership identification accuracy. Furthermore, our enhanced attacks outperform the baseline across multiple models and datasets, with the weakly supervised attack demonstrating an average-case performance improvement of 17% and being at least 7X more effective at low false-positive rates. These findings highlight the importance of protecting the privacy of multi-modal foundational models, which were previously assumed to be less susceptible to MIAs due to less overfitting. The reach of the results presents unique challenges and insights for the broader community to address multi-modal privacy concerns.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ko_Practical_Membership_Inference_Attacks_Against_Large-Scale_Multi-Modal_Models_A_Pilot_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ko_Practical_Membership_Inference_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Myeongseob Ko</author><author>Ming Jin</author><author>Chenguang Wang</author><author>Ruoxi Jia</author>
            </authors>
        </paper>
        

        <paper>
            <title>TCOVIS: Temporally Consistent Online Video Instance Segmentation</title>
            <abstract>In recent years, significant progress has been made in video instance segmentation (VIS), with many offline and online methods achieving state-of-the-art performance. While offline methods have the advantage of producing temporally consistent predictions, they are not suitable for real-time scenarios. Conversely, online methods are more practical, but maintaining temporal consistency remains a challenging task. In this paper, we propose a novel online method for video instance segmentation, called TCOVIS, which fully exploits the temporal information in a video clip. The core of our method consists of a global instance assignment strategy and a spatio-temporal enhancement module, which improve the temporal consistency of the features from two aspects. Specifically, we perform global optimal matching between the predictions and ground truth across the whole video clip, and supervise the model with the global optimal objective. We also capture the spatial feature and aggregate it with the semantic feature between frames, thus realizing the spatio-temporal enhancement. We evaluate our method on four widely adopted VIS benchmarks, namely YouTube-VIS 2019/2021/2022 and OVIS, and achieve state-of-the-art performance on all benchmarks without bells-and-whistles. For instance, on YouTube-VIS 2021, TCOVIS achieves 49.5 AP and 61.3 AP with ResNet-50 and Swin-L backbones, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_TCOVIS_Temporally_Consistent_Online_Video_Instance_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Junlong Li</author><author>Bingyao Yu</author><author>Yongming Rao</author><author>Jie Zhou</author><author>Jiwen Lu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models</title>
            <abstract>Novel view synthesis from a single input image is a challenging task, where the goal is to generate a new view of a scene from a desired camera pose that may be separated by a large motion. The highly uncertain nature of this synthesis task due to unobserved elements within the scene (i.e. occlusion) and outside the field-of-view makes the use of generative models appealing to capture the variety of possible outputs. In this paper, we propose a novel generative model capable of producing a sequence of photorealistic images consistent with a specified camera trajectory, and a single starting image. Our approach is centred on an autoregressive conditional diffusion-based model capable of interpolating visible scene elements, and extrapolating unobserved regions in a view, in a geometrically consistent manner. Conditioning is limited to an image capturing a single camera view and the (relative) pose of the new camera view. To measure the consistency over a sequence of generated views, we introduce a new metric, the thresholded symmetric epipolar distance (TSED), to measure the number of consistent frame pairs in a sequence. While previous methods have been shown to produce high quality images and consistent semantics across pairs of views, we show empirically with our metric that they are often inconsistent with the desired camera poses. In contrast, we demonstrate that our method produces both photorealistic and view-consistent imagery. Additional material is available on our project page: https://yorkucvil.github.io/Photoconsistent-NVS/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_Long-Term_Photometric_Consistent_Novel_View_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yu_Long-Term_Photometric_Consistent_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jason J. Yu</author><author>Fereshteh Forghani</author><author>Konstantinos G. Derpanis</author><author>Marcus A. Brubaker</author>
            </authors>
        </paper>
        

        <paper>
            <title>Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation</title>
            <abstract>We propose an experimental method for measuring bias in face recognition systems. Existing methods to measure bias depend on benchmark datasets that are collected in the wild and annotated for protected (e.g., race, gender) and non-protected (e.g., pose, lighting) attributes. Such observational datasets only permit correlational conclusions, e.g., &quot;Algorithm A&apos;s accuracy is different on female and male faces in dataset X.&quot;. By contrast, experimental methods manipulate attributes individually and thus permit causal conclusions, e.g., &quot;Algorithm A&apos;s accuracy is affected by gender and skin color.&quot; Our method is based on generating synthetic faces using a neural face generator, where each attribute of interest is modified independently while leaving all other attributes constant. Human observers crucially provide the ground truth on perceptual identity similarity between synthetic image pairs. We validate our method quantitatively by evaluating race and gender biases of three research-grade face recognition models. Our synthetic pipeline reveals that for these algorithms, accuracy is lower for Black and East Asian population subgroups. Our method can also quantify how perceptual changes in attributes affect face identity distances reported by these models. Our large synthetic dataset, consisting of 48,000 synthetic face image pairs (10,200 unique synthetic faces) and 555,000 human annotations (individual attributes and pairwise identity comparisons) is available to researchers in this important area.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liang_Benchmarking_Algorithmic_Bias_in_Face_Recognition_An_Experimental_Approach_Using_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liang_Benchmarking_Algorithmic_Bias_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hao Liang</author><author>Pietro Perona</author><author>Guha Balakrishnan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Spatial-Aware Token for Weakly Supervised Object Localization</title>
            <abstract>Weakly supervised object localization (WSOL) is a challenging task aiming to localize objects with only image-level supervision. Recent works apply visual transformer to WSOL and achieve significant success by exploiting the long-range feature dependency in self-attention mechanism. However, existing transformer-based methods synthesize the classification feature maps as the localization map, which leads to optimization conflicts between classification and localization tasks. To address this problem, we propose to learn a task-specific spatial-aware token (SAT) to condition localization in a weakly supervised manner. Specifically, a spatial token is first introduced in the input space to aggregate representations for localization task. Then a spatial aware attention module is constructed, which allows spatial token to generate foreground probabilities of different patches by querying and to extract localization knowledge from the classification task. Besides, for the problem of sparse and unbalanced pixel-level supervision obtained from the image-level label, two spatial constraints, including batch area loss and normalization loss, are designed to compensate and enhance this supervision. Experiments show that the proposed SAT achieves state-of-the-art performance on both CUB-200 and ImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under the extreme setting of using only 1 image per class from ImageNet for training, SAT already exceeds the SOTA method by 2.1% GT-known Loc. Code and models are available at https://github.com/wpy1999/SAT.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Spatial-Aware_Token_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Spatial-Aware_Token_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Pingyu Wu</author><author>Wei Zhai</author><author>Yang Cao</author><author>Jiebo Luo</author><author>Zheng-Jun Zha</author>
            </authors>
        </paper>
        

        <paper>
            <title>Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis</title>
            <abstract>Diffusion-based models have achieved state-of-the-art performance on text-to-image synthesis tasks. However, one critical limitation of these models is the low fidelity of generated images with respect to the text description, such as missing objects, mismatched attributes, and mislocated objects. One key reason for such inconsistencies is the inaccurate cross-attention to text in both the spatial dimension, which controls at what pixel region an object should appear, and the temporal dimension, which controls how different levels of details are added through the denoising steps. In this paper, we propose a new text-to-image algorithm that adds explicit control over spatial-temporal cross-attention in diffusion models. We first utilize a layout predictor to predict the pixel regions for objects mentioned in the text. We then impose spatial attention control by combining the attention over the entire text description and that over the local description of the particular object in the corresponding pixel region of that object. The temporal attention control is further added by allowing the combination weights to change at each denoising step, and the combination weights are optimized to ensure high fidelity between the image and the text. Experiments show that our method generates images with higher fidelity compared to diffusion-model-based baselines without fine-tuning the diffusion model. Our code is publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Qiucheng Wu</author><author>Yujian Liu</author><author>Handong Zhao</author><author>Trung Bui</author><author>Zhe Lin</author><author>Yang Zhang</author><author>Shiyu Chang</author>
            </authors>
        </paper>
        

        <paper>
            <title>GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection</title>
            <abstract>LiDAR and cameras are complementary sensors for 3D object detection in autonomous driving. However, it is challenging to explore the unnatural interaction between point clouds and images, and the critical factor is how to conduct feature alignment of heterogeneous modalities. Currently, many methods achieve feature alignment by projection calibration only, without considering the problem of coordinate conversion accuracy errors between sensors, leading to sub-optimal performance. In this paper, we present GraphAlign, a more accurate feature alignment strategy for 3D object detection by graph matching. Specifically, we fuse image features from a semantic segmentation encoder in the image branch and point cloud features from a 3D Sparse CNN in the LiDAR branch. To save computation, we construct the nearest neighbor relationship by calculating Euclidean distance within the subspaces that are divided into the point cloud features. Through the projection calibration between the image and point cloud, we project the nearest neighbors of point cloud features onto the image features. Then by matching the nearest neighbors with a single point cloud to multiple images, we search for a more appropriate feature alignment. In addition, we provide a self-attention module to enhance the weights of significant relations to fine-tune the feature alignment between heterogeneous modalities. Extensive experiments on nuScenes benchmark demonstrate the effectiveness and efficiency of our GraphAlign.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Song_GraphAlign_Enhancing_Accurate_Feature_Alignment_by_Graph_matching_for_Multi-Modal_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ziying Song</author><author>Haiyue Wei</author><author>Lin Bai</author><author>Lei Yang</author><author>Caiyan Jia</author>
            </authors>
        </paper>
        

        <paper>
            <title>NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects</title>
            <abstract>We propose NEMTO, the first end-to-end neural rendering pipeline to model 3D transparent objects with complex geometry and unknown indices of refraction. Commonly used appearance modeling such as the Disney BSDF model cannot accurately address this challenging problem due to the complex light paths bending through refractions and the strong dependency of surface appearance on illumination. With 2D images of the transparent object as input, our method is capable of high-quality novel view and relighting synthesis. We leverage implicit Signed Distance Functions (SDF) to model the object geometry and propose a refraction-aware ray bending network to model the effects of light refraction within the object. Our ray bending network is more tolerant to geometric inaccuracies than traditional physically-based methods for rendering transparent objects. We provide extensive evaluations on both synthetic and real-world datasets to demonstrate our high-quality synthesis and the applicability of our method.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_NEMTO_Neural_Environment_Matting_for_Novel_View_and_Relighting_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_NEMTO_Neural_Environment_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dongqing Wang</author><author>Tong Zhang</author><author>Sabine S sstrunk</author>
            </authors>
        </paper>
        

        <paper>
            <title>USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation</title>
            <abstract>Seed area generation is usually the starting point of weakly supervised semantic segmentation (WSSS). Computing the Class Activation Map (CAM) from a multi-label classification network is the de facto paradigm for seed area generation, but CAMs generated from Convolutional Neural Networks (CNNs) and Transformers are prone to be under- and over-activated, respectively, which makes the strategies to refine CAMs for CNNs usually inappropriate for Transformers, and vice versa. In this paper, we propose a Unified optimization paradigm for Seed Area GEneration (USAGE) for both types of networks, in which the objective function to be optimized consists of two terms: One is a generation loss, which controls the shape of seed areas by a temperature parameter following a deterministic principle for different types of networks; The other is a regularization loss, which ensures the consistency between the seed areas that are generated by self-adaptive network adjustment from different views, to overturn false activation in seed areas. Experimental results show that USAGE consistently improves seed area generation for both CNNs and Transformers by large margins, e.g., outperforming state-of-the-art methods by an mIoU of 4.1% on PASCAL VOC. Moreover, based on the USAGE generated seed areas on Transformers, we achieve state-of-the-art WSSS results on both PASCAL VOC and MS COCO.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Peng_USAGE_A_Unified_Seed_Area_Generation_Paradigm_for_Weakly_Supervised_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Peng_USAGE_A_Unified_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zelin Peng</author><author>Guanchun Wang</author><author>Lingxi Xie</author><author>Dongsheng Jiang</author><author>Wei Shen</author><author>Qi Tian</author>
            </authors>
        </paper>
        

        <paper>
            <title>NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction</title>
            <abstract>Recent methods for neural surface representation and rendering, for example NeuS, have demonstrated the remarkably high-quality reconstruction of static scenes. However, the training of NeuS takes an extremely long time (8 hours), which makes it almost impossible to apply them to dynamic scenes with thousands of frames. We propose a fast neural surface reconstruction approach, called NeuS2, which achieves two orders of magnitude improvement in terms of acceleration without compromising reconstruction quality. To accelerate the training process, we parameterize a neural surface representation by multi-resolution hash encodings and present a novel lightweight calculation of second-order derivatives tailored to our networks to leverage CUDA parallelism, achieving a factor two speed up. To further stabilize and expedite training, a progressive learning strategy is proposed to optimize multi-resolution hash encodings from coarse to fine. We extend our method for fast training of dynamic scenes, with a proposed incremental training strategy and a novel global transformation prediction component, which allow our method to handle challenging long sequences with large movements and deformations. Our experiments on various datasets demonstrate that NeuS2 significantly outperforms the state-of-the-arts in both surface reconstruction accuracy and training speed for both static and dynamic scenes. The code is available at our website: https://vcai.mpi-inf.mpg.de/projects/NeuS2/.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_NeuS2_Fast_Learning_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Yiming Wang</author><author>Qin Han</author><author>Marc Habermann</author><author>Kostas Daniilidis</author><author>Christian Theobalt</author><author>Lingjie Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Gender Artifacts in Visual Datasets</title>
            <abstract>Gender biases are known to exist within large-scale visual datasets and can be reflected or even amplified in downstream models. Many prior works have proposed methods for mitigating gender biases, often by attempting to remove gender expression information from images. To understand the feasibility and practicality of these approaches, we investigate what &quot;gender artifacts&quot; exist in large-scale visual datasets. We define a &quot;gender artifact&quot; as a visual cue correlated with gender , focusing specifically on cues that are learnable by a modern image classifier and have an interpretable human corollary. Through our analyses, we find that gender artifacts are ubiquitous in the COCO and OpenImages datasets, occurring everywhere from low-level information (e.g., the mean value of the color channels) to higher-level image composition (e.g., pose and location of people). Further, bias mitigation methods that attempt to remove gender actually remove more information from the scene than the person. Given the prevalence of gender artifacts, we claim that attempts to remove these artifacts from such datasets are largely infeasible as certain removed artifacts may be necessary for the downstream task of object recognition. Instead, the responsibility lies with researchers and practitioners to be aware that the distribution of images within datasets is highly gendered and hence develop fairness-aware methods which are robust to these distributional shifts across groups.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Meister_Gender_Artifacts_in_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nicole Meister</author><author>Dora Zhao</author><author>Angelina Wang</author><author>Vikram V. Ramaswamy</author><author>Ruth Fong</author><author>Olga Russakovsky</author>
            </authors>
        </paper>
        

        <paper>
            <title>SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</title>
            <abstract>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval performance on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free &quot;name-only transfer&quot; in which the only knowledge we possess about downstream tasks comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks--&quot;SuS&quot; and &quot;TIP-X&quot;, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art (SoTA) zero-shot classification results on 19 benchmark datasets. We further show the utility of TIP-X in the training-free few-shot setting, where we again achieve SoTA results over strong training-free baselines.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Udandarao_SuS-X_Training-Free_Name-Only_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Vishaal Udandarao</author><author>Ankush Gupta</author><author>Samuel Albanie</author>
            </authors>
        </paper>
        

        <paper>
            <title>Beating Backdoor Attack at Its Own Game</title>
            <abstract>Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network&apos;s performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker&apos;s backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on multiple benchmarks with different architectures and representative attacks. Results demonstrate that our method achieves state-of-the-art defense effectiveness with by far the lowest performance drop on clean data. Considering the surprising defense ability displayed by our framework, we call for more attention to utilizing backdoor for backdoor defense. Code is available at https://github.com/damianliumin/non-adversarial_backdoor.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Beating_Backdoor_Attack_at_Its_Own_Game_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Min Liu</author><author>Alberto Sangiovanni-Vincentelli</author><author>Xiangyu Yue</author>
            </authors>
        </paper>
        

        <paper>
            <title>Do DALL-E and Flamingo Understand Each Other?</title>
            <abstract>The field of multimodal research focusing on the comprehension and creation of both images and text has witnessed significant strides. This progress is exemplified by the emergence of sophisticated models dedicated to image captioning at scale, such as the notable Flamingo model and text-to-image generative models, with DALL-E serving as a prominent example. An interesting question worth exploring in this domain is whether Flamingo and DALL-E understand each other. To study this question, we propose a reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image. We argue that these models understand each other if the generated image is similar to the given image. Specifically, we study the relationship between the quality of the image reconstruction and that of the text generation. We find that an optimal description of an image is one that gives rise to a generated image similar to the original one. The finding motivates us to propose a unified framework to finetune the text-to-image and image-to-text models. Concretely, the reconstruction part forms a regularization loss to guide the tuning of the models. Extensive experiments on multiple datasets with different image captioning and image generation models validate our findings and demonstrate the effectiveness of our proposed unified framework. As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work. Project website: https://dalleflamingo.github.io.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Do_DALL-E_and_Flamingo_Understand_Each_Other_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Do_DALL-E_and_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hang Li</author><author>Jindong Gu</author><author>Rajat Koner</author><author>Sahand Sharifzadeh</author><author>Volker Tresp</author>
            </authors>
        </paper>
        

        <paper>
            <title>Prototype-based Dataset Comparison</title>
            <abstract>Dataset summarisation is a fruitful approach to dataset inspection. However, when applied to a single dataset the discovery of visual concepts is restricted to those most prominent. We argue that a comparative approach can expand upon this paradigm to enable richer forms of dataset inspection that go beyond the most prominent concepts. To enable dataset comparison we present a module that learns concept-level prototypes across datasets. We leverage self-supervised learning to discover these prototypes without supervision, and we demonstrate the benefits of our approach in two case-studies. Our findings show that dataset comparison extends dataset inspection and we hope to encourage more works in this direction. Code and usage instructions available at https://github.com/Nanne/ProtoSim</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/van_Noord_Protoype-based_Dataset_Comparison_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/van_Noord_Protoype-based_Dataset_Comparison_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nanne van Noord</author>
            </authors>
        </paper>
        

        <paper>
            <title>FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation</title>
            <abstract>Curvilinear object segmentation is critical for many applications. However, manually annotating curvilinear objects is very time-consuming and error-prone, yielding insufficiently available annotated datasets for existing supervised methods and domain adaptation methods. This paper proposes a self-supervised curvilinear object segmentation method (FreeCOS) that learns robust and distinctive features from fractals and unlabeled images. The key contributions include a novel Fractal-FDA synthesis (FFS) module and a geometric information alignment (GIA) approach. FFS generates curvilinear structures based on the parametric Fractal L-system and integrates the generated structures into unlabeled images to obtain synthetic training images via Fourier Domain Adaptation. GIA reduces the intensity differences between the synthetic and unlabeled images by comparing the intensity order of a given pixel to the values of its nearby neighbors. Such image alignment can explicitly remove the dependency on absolute intensity values and enhance the inherent geometric characteristics which are common in both synthetic and real images. In addition, GIA aligns features of synthetic and real images via the prediction space adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL). Extensive experimental results on four public datasets, i.e., XCAD, DRIVE, STARE and CrackTree demonstrate that our method outperforms the state-of-the-art unsupervised methods, self-supervised methods and traditional methods by a large margin. The source code of this work is available at https://github.com/TY-Shi/FreeCOS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shi_FreeCOS_Self-Supervised_Learning_from_Fractals_and_Unlabeled_Images_for_Curvilinear_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shi_FreeCOS_Self-Supervised_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Tianyi Shi</author><author>Xiaohuan Ding</author><author>Liang Zhang</author><author>Xin Yang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Generating Dynamic Kernels via Transformers for Lane Detection</title>
            <abstract>State-of-the-art lane detection methods often rely on specific knowledge about lanes -- such as straight lines and parametric curves -- to detect lane lines. While the specific knowledge can ease the modeling process, it poses challenges in handling lane lines with complex topologies (e.g., dense, forked, curved, etc.). Recently, dynamic convolution-based methods have shown promising performance by utilizing the features from some key locations of a lane line, such as the starting point, as convolutional kernels, and convoluting them with the whole feature map to detect lane lines. While such methods reduce the reliance on specific knowledge, the kernels computed from the key locations fail to capture the lane line&apos;s global structure due to its long and thin structure, leading to inaccurate detection of lane lines with complex topologies. In addition, the kernels resulting from the key locations are sensitive to occlusion and lane intersections. To overcome these limitations, we propose a transformer-based dynamic kernel generation architecture for lane detection. It utilizes a transformer to generate dynamic convolutional kernels for each lane line in the input image, and then detect these lane lines with dynamic convolution. Compared to the kernels generated from the key locations of a lane line, the kernels generated with the transformer can capture the lane line&apos;s global structure from the whole feature map, enabling them to effectively handle occlusions and lane lines with complex topologies. We evaluate our method on three lane detection benchmarks, and the results demonstrate its state-of-the-art performance. Specifically, our method achieves an F1 score of 63.40 on OpenLane and 88.47 on CurveLanes, surpassing the state of the art by 4.30 and 2.37 points, respectively.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ziye Chen</author><author>Yu Liu</author><author>Mingming Gong</author><author>Bo Du</author><author>Guoqi Qian</author><author>Kate Smith-Miles</author>
            </authors>
        </paper>
        

        <paper>
            <title>Boosting Long-tailed Object Detection via Step-wise Learning on Smooth-tail Data</title>
            <abstract>Real-world data tends to follow a long-tailed distribution, where the class imbalance results in dominance of the head classes during training. In this paper, we propose a frustratingly simple but effective step-wise learning framework to gradually enhance the capability of the model in detecting all categories of long-tailed datasets. Specifically, we build smooth-tail data where the long-tailed distribution of categories decays smoothly to correct the bias towards head classes. We pre-train a model on the whole long-tailed data to preserve discriminability between all categories. We then fine-tune the class-agnostic modules of the pre-trained model on the head class dominant replay data to get a head class expert model with improved decision boundaries from all categories. Finally, we train a unified model on the tail class dominant replay data while transferring knowledge from the head class expert model to ensure accurate detection of all categories. Extensive experiments on long-tailed datasets LVIS v0.5 and LVIS v1.0 demonstrate the superior performance of our method, where we can improve the AP with ResNet-50 backbone from 27.0% to 30.3% AP, and especially for the rare categories from 15.5% to 24.9% AP. Our best model using ResNet-101 backbone can achieve 30.7% AP, which suppresses all existing detectors using the same backbone.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Dong_Boosting_Long-tailed_Object_Detection_via_Step-wise_Learning_on_Smooth-tail_Data_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Na Dong</author><author>Yongqiang Zhang</author><author>Mingli Ding</author><author>Gim Hee Lee</author>
            </authors>
        </paper>
        

        <paper>
            <title>Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</title>
            <abstract>We introduce a novel framework for one-shot audio-driven talking head generation. Unlike prior works that require additional driving sources for controlled synthesis in a deterministic manner, we instead sample all holistic lip-irrelevant facial motions (i.e. pose, expression, blink, gaze, etc.) to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchronization and overall naturalness. This is achieved by our newly proposed audio-to-visual diffusion prior trained on top of the mapping between audio and non-lip representations. Thanks to the probabilistic nature of the diffusion prior, one big advantage of our framework is it can synthesize diverse facial motion sequences given the same audio clip, which is quite user-friendly for many real applications. Through comprehensive evaluations of public benchmarks, we conclude that (1) our diffusion prior outperforms auto-regressive prior significantly on all the concerned metrics; (2) our overall system is competitive with prior works in terms of audio-lip synchronization but can effectively sample rich and natural-looking lip-irrelevant facial motions while still semantically harmonized with the audio input.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yu_Talking_Head_Generation_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Zhentao Yu</author><author>Zixin Yin</author><author>Deyu Zhou</author><author>Duomin Wang</author><author>Finn Wong</author><author>Baoyuan Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples</title>
            <abstract>Referring video object segmentation (RVOS), as a supervised learning task, relies on sufficient annotated data for a given scene. However, in more realistic scenarios, only minimal annotations are available for a new scene, which poses significant challenges to existing RVOS methods. With this in mind, we propose a simple yet effective model with a newly designed cross-modal affinity (CMA) module based on a Transformer architecture. The CMA module builds multimodal affinity with a few samples, thus quickly learning new semantic information, and enabling the model to adapt to different scenarios. Since the proposed method targets limited samples for new scenes, we generalize the problem as - few-shot referring video object segmentation (FS-RVOS). To foster research in this direction, we build up a new FS-RVOS benchmark based on currently available datasets. The benchmark covers a wide range and includes multiple situations, which can maximally simulate real-world scenarios. Extensive experiments show that our model adapts well to different scenarios with only a few samples, reaching state-of-the-art performance on the benchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performance of 53.1 J and 54.8 F, which are 10% better than the baselines. Furthermore, we show impressive results of 77.7 J and 74.8 F on Mini-Ref-SAIL-VOS, which are significantly better than the baselines. Code is publicly available at https://github.com/hengliusky/Few_shot_RVOS.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Learning_Cross-Modal_Affinity_for_Referring_Video_Object_Segmentation_Targeting_Limited_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Learning_Cross-Modal_Affinity_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Guanghui Li</author><author>Mingqi Gao</author><author>Heng Liu</author><author>Xiantong Zhen</author><author>Feng Zheng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Divide and Conquer: a Two-Step Method for High Quality Face De-identification with Model Explainability</title>
            <abstract>Face de-identification involves concealing the true identity of a face while retaining other facial characteristics. Current target-generic methods typically disentangle identity features in the latent space, using adversarial training to balance privacy and utility. However, this pattern often leads to a trade-off between privacy and utility, and the latent space remains difficult to explain. To address these issues, we propose IDeudemon, which employs a &quot;divide and conquer&quot; strategy to protect identity and preserve utility step by step while maintaining good explainability. In Step I, we obfuscate the 3D disentangled ID code calculated by a parametric NeRF model to protect identity. In Step II, we incorporate visual similarity assistance and train a GAN with adjusted losses to preserve image utility. Thanks to the powerful 3D prior and delicate generative designs, our approach could protect the identity naturally, produce high quality details and is robust to different poses and expressions. Extensive experiments demonstrate that the proposed IDeudemon outperforms previous state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wen_Divide_and_Conquer_a_Two-Step_Method_for_High_Quality_Face_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yunqian Wen</author><author>Bo Liu</author><author>Jingyi Cao</author><author>Rong Xie</author><author>Li Song</author>
            </authors>
        </paper>
        

        <paper>
            <title>Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models</title>
            <abstract>Vision-language pre-training (VLP) models have shown vulnerability to adversarial examples in multimodal tasks. Furthermore, malicious adversaries can be deliberately transferred to attack other black-box models. However, existing work has mainly focused on investigating white-box attacks. In this paper, we present the first study to investigate the adversarial transferability of recent VLP models. We observe that existing methods exhibit much lower transferability, compared to the strong attack performance in white-box settings. The transferability degradation is partly caused by the under-utilization of cross-modal interactions. Particularly, unlike unimodal learning, VLP models rely heavily on cross-modal interactions and the multimodal alignments are many-to-many, e.g., an image can be described in various natural languages. To this end, we propose a highly transferable Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions and incorporates alignment-preserving augmentation with cross-modal guidance. Experimental results demonstrate that SGA could generate adversarial examples that can strongly transfer across different VLP models on multiple downstream vision-language tasks. On image-text retrieval, SGA significantly enhances the attack success rate for transfer attacks from ALBEF to TCL by a large margin (at least 9.78% and up to 30.21%), compared to the state-of-the-art. Our code is available at https://github.com/Zoky-2020/SGA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Set-level_Guidance_Attack_Boosting_Adversarial_Transferability_of_Vision-Language_Pre-training_Models_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lu_Set-level_Guidance_Attack_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dong Lu</author><author>Zhiqiang Wang</author><author>Teng Wang</author><author>Weili Guan</author><author>Hongchang Gao</author><author>Feng Zheng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multimodal Distillation for Egocentric Action Recognition</title>
            <abstract>The focal point of egocentric video understanding is modelling hand-object interactions. Standard models, e.g. CNNs or Vision Transformers, which receive RGB frames as input perform well, however, their performance improves further by employing additional input modalities that provide complementary cues, such as object detections, optical flow, audio, etc. The added complexity of the modality-specific modules, on the other hand, makes these models impractical for deployment. The goal of this work is to retain the performance of such a multimodal approach, while using only the RGB frames as input at inference time. We demonstrate that for egocentric action recognition on the Epic-Kitchens and the Something-Something datasets, students which are taught by multimodal teachers tend to be more accurate and better calibrated than architecturally equivalent models trained on ground truth labels in a unimodal or multimodal fashion. We further present a principled multimodal knowledge distillation framework, allowing us to deal with issues which occur when applying multimodal knowledge distillation in a naive manner. Lastly, we demonstrate the achieved reduction in computational complexity, and show that our approach maintains higher performance with the reduction of the number of input views.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Radevski_Multimodal_Distillation_for_Egocentric_Action_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Radevski_Multimodal_Distillation_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Gorjan Radevski</author><author>Dusan Grujicic</author><author>Matthew Blaschko</author><author>Marie-Francine Moens</author><author>Tinne Tuytelaars</author>
            </authors>
        </paper>
        

        <paper>
            <title>Perceptual Artifacts Localization for Image Synthesis Tasks</title>
            <abstract>Recent advancements in deep generative models have facilitated the creation of photo-realistic images across various tasks. However, these generated images often exhibit perceptual artifacts in specific regions, necessitating manual correction. In this study, we present a comprehensive empirical examination of Perceptual Artifacts Localization (PAL) spanning diverse image synthesis endeavors. We introduce a novel dataset comprising 10,168 generated images, each annotated with per-pixel perceptual artifact labels across ten synthesis tasks. A segmentation model, trained on our proposed dataset, effectively localizes artifacts across a range of tasks. Additionally, we illustrate its proficiency in adapting to previously unseen models using minimal training samples. We further propose an innovative zoom-in inpainting pipeline that seamlessly rectifies perceptual artifacts in the generated images. Through our experimental analyses, we elucidate several invaluable downstream applications, such as automated artifact rectification, non-referential image quality evaluation, and abnormal region detection in images. The dataset and code are released here: https://owenzlz.github.io/PAL4VST</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Perceptual_Artifacts_Localization_for_Image_Synthesis_Tasks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Perceptual_Artifacts_Localization_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Lingzhi Zhang</author><author>Zhengjie Xu</author><author>Connelly Barnes</author><author>Yuqian Zhou</author><author>Qing Liu</author><author>He Zhang</author><author>Sohrab Amirghodsi</author><author>Zhe Lin</author><author>Eli Shechtman</author><author>Jianbo Shi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Better May Not Be Fairer: A Study on Subgroup Discrepancy in Image Classification</title>
            <abstract>In this paper, we provide 20,000 non-trivial human annotations on popular datasets as a first step to bridge gap to studying how natural semantic spurious features affect image classification, as prior works often study datasets mixing low-level features due to limitations in accessing realistic datasets. We investigate how natural background colors play a role as spurious features by annotating the test sets of CIFAR10 and CIFAR100 into subgroups based on the background color of each image. We name our datasets CIFAR10-B and CIFAR100-B and integrate them with CIFAR-Cs. We find that overall human-level accuracy does not guarantee consistent subgroup performances, and the phenomenon remains even on models pre-trained on ImageNet or after data augmentation (DA). To alleviate this issue, we propose FlowAug, a semantic DA that leverages decoupled semantic representations captured by a pre-trained generative flow. Experimental results show that FlowAug achieves more consistent subgroup results than other types of DA methods on CIFAR10/100 and on CIFAR10/100-C. Additionally, it shows better generalization performance. Furthermore, we propose a generic metric, MacroStd, for studying model robustness to spurious correlations, where we take a macro average on the weighted standard deviations across different classes. We show MacroStd being more predictive of better performances; per our metric, FlowAug demonstrates improvements on subgroup discrepancy. Although this metric is proposed to study our curated datasets, it applies to all datasets that have subgroups or subclasses. Lastly, we also show superior out-of-distribution results on CIFAR10.1.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chiu_Better_May_Not_Be_Fairer_A_Study_on_Subgroup_Discrepancy_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chiu_Better_May_Not_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ming-Chang Chiu</author><author>Pin-Yu Chen</author><author>Xuezhe Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>3D Implicit Transporter for Temporally Consistent Keypoint Discovery</title>
            <abstract>Keypoint-based representation has proven advantageous in various visual and robotic tasks. However, the existing 2D and 3D methods for detecting keypoints mainly rely on geometric consistency to achieve spatial alignment, neglecting temporal consistency. To address this issue, the Transporter method was introduced for 2D data, which reconstructs the target frame from the source frame to incorporate both spatial and temporal information. However, the direct application of the Transporter to 3D point clouds is infeasible due to their structural differences from 2D images. Thus, we propose the first 3D version of the Transporter, which leverages hybrid 3D representation, cross attention, and implicit reconstruction. We apply this new learning system on 3D articulated objects/humans and show that learned keypoints are spatiotemporal consistent. Additionally, we propose a control policy that utilizes the learned keypoints for 3D object manipulation and demonstrate its superior performance. Our codes, data, and models will be made publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhong_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhong_3D_Implicit_Transporter_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chengliang Zhong</author><author>Yuhang Zheng</author><author>Yupeng Zheng</author><author>Hao Zhao</author><author>Li Yi</author><author>Xiaodong Mu</author><author>Ling Wang</author><author>Pengfei Li</author><author>Guyue Zhou</author><author>Chao Yang</author><author>Xinliang Zhang</author><author>Jian Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Adaptive Rotated Convolution for Rotated Object Detection</title>
            <abstract>Rotated object detection aims to identify and locate objects in images with arbitrary orientation. In this scenario, the oriented directions of objects vary considerably across different images, while multiple orientations of objects exist within an image. This intrinsic characteristic makes it challenging for standard backbone networks to extract high-quality features of these arbitrarily orientated objects. In this paper, we present Adaptive Rotated Convolution (ARC) module to handle the aforementioned challenges. In our ARC module, the convolution kernels rotate adaptively to extract object features with varying orientations in different images, and an efficient conditional computation mechanism is introduced to accommodate the large orientation variations of objects within an image. The two designs work seamlessly in rotated object detection problem. Moreover, ARC can conveniently serve as a plug-and-play module in various vision backbones to boost their representation ability to detect oriented objects accurately. Experiments on commonly used benchmarks (DOTA and HRSC2016) demonstrate that equipped with our proposed ARC module in the backbone network, the performance of multiple popular oriented object detectors is significantly improved (e.g. +3.03% mAP on Rotated RetinaNet and +4.16% on CFA). Combined with the highly competitive method Oriented R-CNN, the proposed approach achieves state-of-the-art performance on the DOTA dataset with 81.77% mAP. Code is available at https://github.com/LeapLabTHU/ARC.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yifan Pu</author><author>Yiru Wang</author><author>Zhuofan Xia</author><author>Yizeng Han</author><author>Yulin Wang</author><author>Weihao Gan</author><author>Zidong Wang</author><author>Shiji Song</author><author>Gao Huang</author>
            </authors>
        </paper>
        

        <paper>
            <title>UniVTG: Towards Unified Video-Language Temporal Grounding</title>
            <abstract>Video Temporal Grounding (VTG), which aims to ground target clips from videos (such as consecutive intervals or disjoint shots) according to custom language queries (e.g., sentences or words), is key for video browsing on social media. Most methods in this direction develop task-specific models that are trained with type-specific labels, such as moment retrieval (time interval) and highlight detection (worthiness curve), which limits their abilities to generalize to various VTG tasks and labels. In this paper, we propose to Unify the diverse VTG labels and tasks, dubbed UniVTG, along three directions: Firstly, we revisit a wide range of VTG labels and tasks and define a unified formulation. Based on this, we develop data annotation schemes to create scalable pseudo supervision. Secondly, we develop an effective and flexible grounding model capable of addressing each task and making full use of each label. Lastly, thanks to the unified framework, we are able to unlock temporal grounding pretraining from large-scale diverse labels and develop stronger grounding abilities e.g., zero-shot grounding. Extensive experiments on three tasks (moment retrieval, highlight detection and video summarization) across seven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights, TV-Sum, and QFVS) demonstrate the effectiveness and flexibility of our proposed framework. The codes are available at https://github.com/showlab/UniVTG.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lin_UniVTG_Towards_Unified_Video-Language_Temporal_Grounding_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Kevin Qinghong Lin</author><author>Pengchuan Zhang</author><author>Joya Chen</author><author>Shraman Pramanick</author><author>Difei Gao</author><author>Alex Jinpeng Wang</author><author>Rui Yan</author><author>Mike Zheng Shou</author>
            </authors>
        </paper>
        

        <paper>
            <title>Fast Globally Optimal Surface Normal Estimation from an Affine Correspondence</title>
            <abstract>We present a new solver for estimating a surface normal from a single affine correspondence in two calibrated views. The proposed approach provides a new globally optimal solution for this over-determined problem and proves that it reduces to a linear system that can be solved extremely efficiently. This allows for performing significantly faster than other recent methods, solving the same problem and obtaining the same globally optimal solution. We demonstrate on 15k image pairs from standard benchmarks that the proposed approach leads to the same results as other optimal algorithms while being, on average, five times faster than the fastest alternative. Besides its theoretical value, we demonstrate that such an approach has clear benefits, e.g., in image-based visual localization, due to not requiring a dense point cloud to recover the surface normal. We show on the Cambridge Landmarks dataset that leveraging the proposed surface normal estimation further improves localization accuracy. Matlab and C++ implementations are also published in the supplementary material.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Hajder_Fast_Globally_Optimal_Surface_Normal_Estimation_from_an_Affine_Correspondence_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Hajder_Fast_Globally_Optimal_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Levente Hajder</author><author>Lajos L czi</author><author>Daniel Barath</author>
            </authors>
        </paper>
        

        <paper>
            <title>Frequency-aware GAN for Adversarial Manipulation Generation</title>
            <abstract>Image manipulation techniques have drawn growing concerns as manipulated images might cause morality and security problems. Various methods have been proposed to detect manipulations and achieved promising performance. However, these methods might be vulnerable to adversarial attacks. In this work, we design an Adversarial Manipulation Generation (AMG) task to explore the vulnerability of image manipulation detectors. We first propose an optimal loss function and extend existing attacks to generate adversarial examples. We observe that existing spatial attacks cause large degradation in image quality and find the loss of high-frequency detailed components might be its major reason. Inspired by this observation, we propose a novel adversarial attack that incorporates both spatial and frequency features into the GAN architecture to generate adversarial examples. We further design an encoder-decoder architecture with skip connections of high-frequency components to preserve fine details. We evaluated our method on three image manipulation detectors (FCN, ManTra-Net and MVSS-Net) with three benchmark datasets (DEFACTO, CASIAv2 and COVER). Experiments show that our method generates adversarial examples significantly fast (0.01s per image), preserves better image quality (PSNR 30% higher than spatial attacks) and achieves a high attack success rate. We also observe that the examples generated by AMG can fool both classification and segmentation models, which indicates better transferability among different tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Frequency-aware_GAN_for_Adversarial_Manipulation_Generation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Peifei Zhu</author><author>Genki Osada</author><author>Hirokatsu Kataoka</author><author>Tsubasa Takahashi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Template-guided Hierarchical Feature Restoration for Anomaly Detection</title>
            <abstract>Targeting for detecting anomalies of various sizes for complicated normal patterns, we propose a Template-guided Hierarchical Feature Restoration method, which introduces two key techniques, bottleneck compression and template-guided compensation, for anomaly-free feature restoration. Specially, our framework compresses hierarchical features of an image by bottleneck structure to preserve the most crucial features shared among normal samples. We design template-guided compensation to restore the distorted features towards anomaly-free features. Particularly, we choose the most similar normal sample as the template and leverage hierarchical features from the template to compensate the distorted features. The bottleneck could partially filter out anomaly features, while the compensation further converts the reminding anomaly features towards normal with template guidance. Finally, anomalies are detected in terms of the cosine distance between the pre-trained features of an inference image and the corresponding restored anomaly-free features. Experimental results demonstrate the effectiveness of our approach, which achieves the state-of-the-art performance on the MVTec LOCO AD dataset.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Guo_Template-guided_Hierarchical_Feature_Restoration_for_Anomaly_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Guo_Template-guided_Hierarchical_Feature_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Hewei Guo</author><author>Liping Ren</author><author>Jingjing Fu</author><author>Yuwang Wang</author><author>Zhizheng Zhang</author><author>Cuiling Lan</author><author>Haoqian Wang</author><author>Xinwen Hou</author>
            </authors>
        </paper>
        

        <paper>
            <title>PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring</title>
            <abstract>Liquid perception is critical for robotic pouring tasks. It usually requires the robust visual detection of flowing liquid. However, while recent works have shown promising results in liquid perception, they typically require labeled data for model training, a process that is both time-consuming and reliant on human labor. To this end, this paper proposes a simple yet effective framework PourIt!, to serve as a tool for robotic pouring tasks. We design a simple data collection pipeline that only needs image-level labels to reduce the reliance on tedious pixel-wise annotations. Then, a binary classification model is trained to generate Class Activation Map (CAM) that focuses on the visual difference between these two kinds of collected data, i.e., the existence of liquid drop or not. We also devise a feature contrast strategy to improve the quality of the CAM, thus entirely and tightly covering the actual liquid regions. Then, the container pose is further utilized to facilitate the 3D point cloud recovery of the detected liquid region. Finally, the liquid-to-container distance is calculated for visual closed-loop control of the physical robot. To validate the effectiveness of our proposed method, we also contribute a novel dataset for our task and name it PourIt! dataset. Extensive results on this dataset and physical Franka robot have shown the utility and effectiveness of our method in the robotic pouring tasks. Our dataset, code and pre-trained models will be available on the project page.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_PourIt_Weakly-Supervised_Liquid_Perception_from_a_Single_Image_for_Visual_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Lin_PourIt_Weakly-Supervised_Liquid_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Haitao Lin</author><author>Yanwei Fu</author><author>Xiangyang Xue</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance</title>
            <abstract>Diffusion models generate images by iterative denoising. Recent work has shown that by making the denoising process deterministic, one can encode real images into latent codes of the same size, which can be used for image editing. This paper explores the possibility of defining a latent space even when the denoising process remains stochastic. Recall that, in stochastic diffusion models, Gaussian noises are added in each denoising step, and we can concatenate all the noises to form a latent code. This results in a latent space of much higher dimensionality than the original image. We demonstrate that this latent space of stochastic diffusion models can be used in the same way as that of deterministic diffusion models in two applications. First, we propose CycleDiffusion, a method for zero-shot and unpaired image editing using stochastic diffusion models, which improves the performance over its deterministic counterpart. Second, we demonstrate unified, plug-and-play guidance in the latent spaces of deterministic and stochastic diffusion models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_A_Latent_Space_of_Stochastic_Diffusion_Models_for_Zero-Shot_Image_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Chen Henry Wu</author><author>Fernando De la Torre</author>
            </authors>
        </paper>
        

        <paper>
            <title>Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting</title>
            <abstract>Human-Object Interaction (HOI) detection is essential for understanding and modeling real-world events. Existing works on HOI detection mainly focus on static images and a closed setting, where all HOI classes are provided in the training set. In comparison, detecting HOIs in videos in open set scenarios is more challenging. First, under open set circumstances, HOI detectors are expected to hold strong generalizability to recognize unseen HOIs not included in the training data. Second, accurately capturing temporal contextual information from videos is difficult, but it is crucial for detecting temporal-related actions such as open, close, pull, push. To this end, we propose ACoLP, a model of Action-centric Chain-of-Look Prompting for open set video HOI detection. ACoLP regards actions as the carrier of semantics in videos, which captures the essential semantic information across frames. To make the model generalizable on unseen classes, inspired by the chain-of-thought prompting in natural language processing, we introduce the chain-of-look prompting scheme that decomposes prompt generation from large-scale vision-language model into a series of intermediate visual reasoning steps. Consequently, our model captures complex visual reasoning processes underlying the HOI events in videos, providing essential guidance for detecting unseen classes. Extensive experiments on two video HOI datasets, VidHOI and CAD120, demonstrate that ACoLP achieves competitive performance compared with the state-of-the-art methods in the conventional closed setting, and outperforms existing methods by a large margin in the open set setting. Our code is avaliable at https://github. com/southnx/ACoLP.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xi_Open_Set_Video_HOI_detection_from_Action-Centric_Chain-of-Look_Prompting_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xi_Open_Set_Video_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Nan Xi</author><author>Jingjing Meng</author><author>Junsong Yuan</author>
            </authors>
        </paper>
        

        <paper>
            <title>Robust Mixture-of-Expert Training for Convolutional Neural Networks</title>
            <abstract>Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture, has demonstrated a great promise to enable high-accuracy and ultra-efficient model inference. Despite the growing popularity of MoE, little work investigated its potential to advance convolutional neural networks (CNNs), especially in the plane of adversarial robustness. Since the lack of robustness has become one of the main hurdles for CNNs, in this paper we ask: How to adversarially robustify a CNN-based MoE model? Can we robustly train it like an ordinary CNN model? Our pilot study shows that the conventional adversarial training (AT) mechanism (developed for vanilla CNNs) no longer remains effective to robustify an MoE-CNN. To better understand this phenomenon, we dissect the robustness of an MoE-CNN into two dimensions: Robustness of routers (i.e., gating functions to select data-specific experts) and robustness of experts (i.e., the router-guided pathways defined by the subnetworks of the backbone CNN). Our analyses show that routers and experts are hard to adapt to each other in the vanilla AT. Thus, we propose a new router-expert alternating Adversarial training framework for MoE, termed AdvMoE. The effectiveness of our proposal is justified across 4 commonly-used CNN model architectures over 4 benchmark datasets. We find that AdvMoE achieves 1% 4% adversarial robustness improvement over the original dense CNN, and enjoys the efficiency merit of sparsity-gated MoE, leading to more than 50% inference cost reduction.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_Robust_Mixture-of-Expert_Training_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Yihua Zhang</author><author>Ruisi Cai</author><author>Tianlong Chen</author><author>Guanhua Zhang</author><author>Huan Zhang</author><author>Pin-Yu Chen</author><author>Shiyu Chang</author><author>Zhangyang Wang</author><author>Sijia Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>UniTR: A Unified and Efficient Multi-Modal Transformer for Bird&apos;s-Eye-View Representation</title>
            <abstract>Jointly processing information from multiple sensors is crucial to achieving accurate and robust perception for reliable autonomous driving systems. However, current 3D perception research follows a modality-specific paradigm, leading to additional computation overheads and inefficient collaboration between different sensor data. In this paper, we present an efficient multi-modal backbone for outdoor 3D perception named UniTR, which processes a variety of modalities with unified modeling and shared parameters. Unlike previous works, UniTR introduces a modality-agnostic transformer encoder to handle these view-discrepant sensor data for parallel modal-wise representation learning and automatic cross-modal interaction without additional fusion steps. More importantly, to make full use of these complementary sensor types, we present a novel multi-modal integration strategy by both considering semantic-abundant 2D perspective and geometry-aware 3D sparse neighborhood relations. UniTR is also a fundamentally task-agnostic backbone that naturally supports different 3D perception tasks. It sets a new state-of-the-art performance on the nuScenes benchmark, achieving +1.1 NDS higher for 3D object detection and +12.0 higher mIoU for BEV map segmentation with lower inference latency. Code will be available at https://github.com/Haiyang-W/UniTR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_UniTR_A_Unified_and_Efficient_Multi-Modal_Transformer_for_Birds-Eye-View_Representation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_UniTR_A_Unified_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Haiyang Wang</author><author>Hao Tang</author><author>Shaoshuai Shi</author><author>Aoxue Li</author><author>Zhenguo Li</author><author>Bernt Schiele</author><author>Liwei Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras</title>
            <abstract>Dense 3D reconstruction and ego-motion estimation are key challenges in autonomous driving and robotics. Compared to the complex, multi-modal systems deployed today, multi-camera systems provide a simpler, low-cost alternative. However, camera-based 3D reconstruction of complex dynamic scenes has proven extremely difficult, as existing solutions often produce incomplete or incoherent results. We propose R3D3, a multi-camera system for dense 3D reconstruction and ego-motion estimation. Our approach iterates between geometric estimation that exploits spatial-temporal information from multiple cameras, and monocular depth refinement. We integrate multi-camera feature correlation and dense bundle adjustment operators that yield robust geometric depth and pose estimates. To improve reconstruction where geometric depth is unreliable, e.g. for moving objects or low-textured regions, we introduce learnable scene priors via a depth refinement network. We show that this design enables a dense, consistent 3D reconstruction of challenging, dynamic outdoor environments. Consequently, we achieve state-of-the-art dense depth prediction on the DDAD and NuScenes benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Schmied_R3D3_Dense_3D_Reconstruction_of_Dynamic_Scenes_from_Multiple_Cameras_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Schmied_R3D3_Dense_3D_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Aron Schmied</author><author>Tobias Fischer</author><author>Martin Danelljan</author><author>Marc Pollefeys</author><author>Fisher Yu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection</title>
            <abstract>Humans recognize anomalies through two aspects: larger patch-wise representation discrepancies and weaker patch-to-normal-patch correlations. However, the previous AD methods didn&apos;t sufficiently combine the two complementary aspects to design AD models. To this end, we find that Transformer can ideally satisfy the two aspects as its great power in the unified modeling of patchwise representations and patch-to-patch correlations. In this paper, we propose a novel AD framework: FOcus-the- Discrepancy (FOD), which can simultaneously spot the patch-wise, intra- and inter-discrepancies of anomalies. The major characteristic of our method is that we renovate the self attention maps in transformers to Intra-Inter-Correlation (I2Correlation). The I2Correlation contains a two-branch structure to first explicitly establish intraand inter-image correlations, and then fuses the features of two-branch to spotlight the abnormal patterns. To learn the intra- and inter-correlations adaptively, we propose the RBF-kernel-based target-correlations as learning targets for self-supervised learning. Besides, we introduce an entropy constraint strategy to solve the mode collapse issue in optimization and further amplify the normal abnormal distinguishability. Extensive experiments on three unsupervised real-world AD benchmarks show the superior performance of our approach. Code will be available at https://github.com/xcyao00/FOD.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yao_Focus_the_Discrepancy_Intra-_and_Inter-Correlation_Learning_for_Image_Anomaly_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yao_Focus_the_Discrepancy_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xincheng Yao</author><author>Ruoqi Li</author><author>Zefeng Qian</author><author>Yan Luo</author><author>Chongyang Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding</title>
            <abstract>3D GAN inversion aims to achieve high reconstruction fidelity and reasonable 3D geometry simultaneously from a single image input. However, existing 3D GAN inversion methods rely on time-consuming optimization for each individual case. In this work, we introduce a novel encoder-based inversion framework based on EG3D, one of the most widely-used 3D GAN models. We leverage the inherent properties of EG3D&apos;s latent space to design a discriminator and a background depth regularization. This enables us to train a geometry-aware encoder capable of converting the input image into corresponding latent code. Additionally, we explore the feature space of EG3D and develop an adaptive refinement stage that improves the representation ability of features in EG3D to enhance the recovery of fine-grained textural details. Finally, we propose an occlusion-aware fusion operation to prevent distortion in unobserved regions. Our method achieves impressive results comparable to optimization-based methods while operating up to 500 times faster. Our framework is well-suited for applications such as semantic editing.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yuan_Make_Encoder_Great_Again_in_3D_GAN_Inversion_through_Geometry_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yuan_Make_Encoder_Great_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Ziyang Yuan</author><author>Yiming Zhu</author><author>Yu Li</author><author>Hongyu Liu</author><author>Chun Yuan</author>
            </authors>
        </paper>
        

        <paper>
            <title>DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer</title>
            <abstract>Generating visual layouts is an essential ingredient of graphic design. The ability to condition layout generation on a partial subset of component attributes is critical to real-world applications that involve user interaction. Recently, diffusion models have demonstrated high-quality generative performances in various domains. However, it is unclear how to apply diffusion models to the natural representation of layouts which consists of a mix of discrete (class) and continuous (location, size) attributes. To address the conditioning layout generation problem, we introduce DLT, a joint discrete-continuous diffusion model. DLT is a transformer-based model which has a flexible conditioning mechanism that allows for conditioning on any given subset of all layout components classes, locations and sizes. Our method outperforms state-of-the-art generative models on various layout generation datasets with respect to different metrics and conditioning settings. Additionally, we validate the effectiveness of our proposed conditioning mechanism and the joint continuous-diffusion process. This joint process can be incorporated into a wide range of mixed discrete-continuous generative tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Levi_DLT_Conditioned_layout_generation_with_Joint_Discrete-Continuous_Diffusion_Layout_Transformer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Levi_DLT_Conditioned_layout_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Elad Levi</author><author>Eli Brosh</author><author>Mykola Mykhailych</author><author>Meir Perez</author>
            </authors>
        </paper>
        

        <paper>
            <title>Open-Vocabulary Semantic Segmentation with Decoupled One-Pass Network</title>
            <abstract>Recently, the open-vocabulary semantic segmentation problem has attracted increasing attention and the best performing methods are based on two-stream networks: one stream for proposal mask generation and the other for segment classification using a pre-trained visual-language model. However, existing two-stream methods require passing a great number of (up to a hundred) image crops into the visual-language model, which is highly inefficient. To address the problem, we propose a network that only needs a single pass through the visual-language model for each input image. Specifically, we first propose a novel networkadaptation approach, termed patch severance, to restrict the harmful interference between the patch embeddings in the pre-trained visual encoder. We then propose classification anchor learning to encourage the network to spatially focus on more discriminative features for classification. Extensive experiments demonstrate that the proposed method achieves outstanding performance, surpassing state-of-the-art methods while being 4 to 7 times faster at inference. Code: https://github.com/CongHan0808/DeOP.git</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Han_Open-Vocabulary_Semantic_Segmentation_with_Decoupled_One-Pass_Network_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Han_Open-Vocabulary_Semantic_Segmentation_with_Decoupled_One-Pass_Network_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Cong Han</author><author>Yujie Zhong</author><author>Dengjie Li</author><author>Kai Han</author><author>Lin Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation</title>
            <abstract>Facial sketch synthesis (FSS) aims to generate a vivid sketch portrait from a given facial photo. Existing FSS methods merely rely on 2D representations of facial semantic or appearance. However, professional human artists usually use outlines or shadings to covey 3D geometry. Thus facial 3D geometry (e.g. depth map) is extremely important for FSS. Besides, different artists may use diverse drawing techniques and create multiple styles of sketches; but the style is globally consistent in a sketch. Inspired by such observations, in this paper, we propose a novel Human-Inspired Dynamic Adaptation (HIDA) method. Specially, we propose to dynamically modulate neuron activations based on a joint consideration of both facial 3D geometry and 2D appearance, as well as globally consistent style control. Besides, we use deformable convolutions at coarse-scales to align deep features, for generating abstract and distinct outlines. Experiments show that HIDA can generate high-quality sketches in multiple styles, and significantly outperforms previous methods, over a large range of challenging faces. Besides, HIDA allows precise style control of the synthesized sketch, and generalizes well to natural scenes and other artistic styles. Our code and results have been released online at: https://github.com/AiArt-HDU/HIDA.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gao_Human-Inspired_Facial_Sketch_Synthesis_with_Dynamic_Adaptation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Fei Gao</author><author>Yifan Zhu</author><author>Chang Jiang</author><author>Nannan Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion</title>
            <abstract>We introduce a novel method to automatically generate an artistic typography by stylizing one or more letter fonts to visually convey the semantics of an input word, while ensuring that the output remains readable. To address an assortment of challenges with our task at hand including conflicting goals (artistic stylization vs. legibility), lack of ground truth, and immense search space, our approach utilizes large language models to bridge texts and visual images for stylization and build an unsupervised generative model with a diffusion model backbone. Specifically, we employ the denoising generator in Latent Diffusion Model (LDM), with the key addition of a CNN-based discriminator to adapt the input style onto the input text. The discriminator uses rasterized images of a given letter/word font as real samples and the output of the denoising generator as fake samples. Our model is coined DS-Fusion for discriminated and stylized diffusion. We showcase the quality and versatility of our method through numerous examples, qualitative and quantitative evaluation, and ablation studies. User studies comparing to strong baselines including CLIPDraw, DALL-E 2, Stable Diffusion, as well as artist-crafted typographies, demonstrate strong performance of DS-Fusion.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Tanveer_DS-Fusion_Artistic_Typography_via_Discriminated_and_Stylized_Diffusion_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Tanveer_DS-Fusion_Artistic_Typography_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Maham Tanveer</author><author>Yizhi Wang</author><author>Ali Mahdavi-Amiri</author><author>Hao Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection</title>
            <abstract>Current methods for open-vocabulary object detection (OVOD) rely on a pre-trained vision-language model (VLM) to acquire the recognition ability. In this paper, we propose a simple yet effective framework to Distill the Knowledge from the VLM to a DETR-like detector, termed DK-DETR. Specifically, we present two ingenious distillation schemes named semantic knowledge distillation (SKD) and relational knowledge distillation (RKD). To utilize the rich knowledge from the VLM systematically, SKD transfers the semantic knowledge explicitly, while RKD exploits implicit relationship information between objects. Furthermore, a distillation branch including a group of auxiliary queries is added to the detector to mitigate the negative effect on base categories. Equipped with SKD and RKD on the distillation branch, DK-DETR improves the detection performance of novel categories significantly and avoids disturbing the detection of base categories. Extensive experiments on LVIS and COCO datasets show that DK-DETR surpasses existing OVOD methods under the setting that the base-category supervision is solely available. The code and models are available at https://github.com/hikvision-research/opera.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Distilling_DETR_with_Visual-Linguistic_Knowledge_for_Open-Vocabulary_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Liangqi Li</author><author>Jiaxu Miao</author><author>Dahu Shi</author><author>Wenming Tan</author><author>Ye Ren</author><author>Yi Yang</author><author>Shiliang Pu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning</title>
            <abstract>Large, pretrained models are commonly finetuned with imagery that is heavily augmented to mimic different conditions and scales, with the resulting models used for various tasks with imagery from a range of spatial scales. Such models overlook scale-specific information in the data for scale-dependent domains, such as remote sensing. In this paper, we present Scale-MAE, a pretraining method that explicitly learns relationships between data at different, known scales throughout the pretraining process. Scale-MAE pretrains a network by masking an input image at a known input scale, where the area of the Earth covered by the image determines the scale of the ViT positional encoding, not the image resolution. Scale-MAE encodes the masked image with a standard ViT backbone, and then decodes the masked image through a bandpass filter to reconstruct low/high frequency images at lower/higher scales. We find that tasking the network with reconstructing both low/high frequency images leads to robust multiscale representations for remote sensing imagery. Scale-MAE achieves an average of a 2.4 - 5.6% non-parametric kNN classification improvement across eight remote sensing datasets compared to current state-of-the-art and obtains a 0.9 mIoU to 1.7 mIoU improvement on the SpaceNet building segmentation transfer task for a range of evaluation scales.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Reed_Scale-MAE_A_Scale-Aware_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Colorado J Reed</author><author>Ritwik Gupta</author><author>Shufan Li</author><author>Sarah Brockman</author><author>Christopher Funk</author><author>Brian Clipp</author><author>Kurt Keutzer</author><author>Salvatore Candido</author><author>Matt Uyttendaele</author><author>Trevor Darrell</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Unified Framework for Robustness on Diverse Sampling Errors</title>
            <abstract>Recent studies have substantiated that machine learning algorithms including convolutional neural networks often suffer from unreliable generalizations when there is a significant gap between the source and target data distributions. To mitigate this issue, a predetermined distribution shift has been addressed independently (e.g., single domain generalization, de-biasing). However, a distribution mismatch cannot be clearly estimated because the target distribution is unknown at training. Therefore, a conservative approach robust on unexpected diverse distributions is more desirable in practice. Our work starts from a motivation to allow adaptive inference once we know the target, since it is accessible only at testing. Instead of assuming and fixing the target distribution at training, our proposed approach allows adjusting the feature space the model refers to at every prediction, i.e., instance-wise adaptive inference. The extensive evaluation demonstrates our method is effective for generalization on diverse distributions.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Jeon_A_Unified_Framework_for_Robustness_on_Diverse_Sampling_Errors_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Jeon_A_Unified_Framework_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Myeongho Jeon</author><author>Myungjoo Kang</author><author>Joonseok Lee</author>
            </authors>
        </paper>
        

        <paper>
            <title>LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment</title>
            <abstract>3D panoptic segmentation is a challenging perception task that requires both semantic segmentation and instance segmentation. In this task, we notice that images could provide rich texture, color, and discriminative information, which can complement LiDAR data for evident performance improvement, but their fusion remains a challenging problem. To this end, we propose LCPS, the first LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel Alignment (ACPA) module that calibrates the coordinate misalignment caused by asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment (SARA) module that extends the one-to-one point-pixel mapping to one-to-many semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that integrates both geometric and semantic fusion information for the entire point cloud. Our fusion strategy improves about 6.9% PQ performance over the LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative experiments further demonstrate the effectiveness of our novel framework. The code will be released at https://github.com/zhangzw12319/lcps.git.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_LiDAR-Camera_Panoptic_Segmentation_via_Geometry-Consistent_and_Semantic-Aware_Alignment_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_LiDAR-Camera_Panoptic_Segmentation_via_Geometry-Consistent_and_Semantic-Aware_Alignment_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zhiwei Zhang</author><author>Zhizhong Zhang</author><author>Qian Yu</author><author>Ran Yi</author><author>Yuan Xie</author><author>Lizhuang Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>Scene-Aware Label Graph Learning for Multi-Label Image Classification</title>
            <abstract>Multi-label image classification refers to assigning a set of labels for an image. One of the main challenges of this task is how to effectively capture the correlation among labels. Existing studies on this issue mostly rely on the statistical label co-occurrence or semantic similarity of labels. However, an important fact is ignored that the co-occurrence of labels is closely related with image scenes (indoor, outdoor, etc.), which is a vital characteristic in multi-label image classification. In this paper, a novel scene-aware label graph learning framework is proposed, which is capable of learning visual representations for labels while fully perceiving their co-occurrence relationships under variable scenes. Specifically, our framework is able to detect scene categories of images without relying on manual annotations, and keeps track of the co-occurring labels by maintaining a global co-occurrence matrix for each scene category throughout the whole training phase. These scene-independent co-occurrence matrices are further employed to guide the interactions among label representations in a graph propagation manner towards accurate label prediction. Extensive experiments on public benchmarks demonstrate the superiority of our proposed framework compared to the state of the arts. Code will be publicly available soon.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Scene-Aware_Label_Graph_Learning_for_Multi-Label_Image_Classification_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Scene-Aware_Label_Graph_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xuelin Zhu</author><author>Jian Liu</author><author>Weijia Liu</author><author>Jiawei Ge</author><author>Bo Liu</author><author>Jiuxin Cao</author>
            </authors>
        </paper>
        

        <paper>
            <title>Fcaformer: Forward Cross Attention in Hybrid Vision Transformer</title>
            <abstract>Currently, one main research line in designing more efficient vision transformer is reducing computational cost of self attention modules by adopting sparse attention or using local attention windows. In contrast, we propose a different approach that aims to improve the performance of transformer-based architectures by densifying the attention pattern. Specifically, we proposed forward cross attention for hybrid vision transformer (FcaFormer), where tokens from previous blocks in the same stage are secondary used. To achieve this, the FcaFormer leverages two innovative components: learnable scale factors (LSFs) and a token merge and enhancement module (TME). The LSFs enable efficient processing of cross tokens, while the TME generates representative cross tokens. By integrating these components, the proposed FcaFormer enhances the interactions of tokens across blocks with potentially different semantics, and encourages more information flows to the lower levels. Based on the forward cross attention (Fca), we have designed a series of FcaFormer models that achieve the best trade-off between model size, computational cost, memory cost, and accuracy. For example, without the need for knowledge distillation to strengthen training, our FcaFormer achieves 83.1% top-1 accuracy on Imagenet with only 16.3 million parameters and about 3.6 billion MACs. This saves almost half of the parameters and a few computational cost while achieving 0.7% higher accuracy compared with distilled EfficientFormer</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Fcaformer_Forward_Cross_Attention_in_Hybrid_Vision_Transformer_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Haokui Zhang</author><author>Wenze Hu</author><author>Xiaoyu Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval</title>
            <abstract>The performance of text-video retrieval has been significantly improved by vision-language cross-modal learning schemes. The typical solution is to directly align the global video-level and sentence-level features during learning, which would ignore the intrinsic video-text relations, i.e., a text description only corresponds to a spatio-temporal part of videos. Hence, the matching process should consider both fine-grained spatial content and various temporal semantic events. To this end, we propose a text-video learning framework with progressive spatio-temporal prototype matching. Specifically, the vanilla matching process is decomposed into two complementary phases: object-phrase prototype matching and event-sentence prototype matching. In the object-phrase prototype matching phase, a spatial prototype generation mechanism is developed to predict key patches or words, which are sparsely integrated into object or phrase prototypes. Importantly, optimizing the local alignment between object-phrase prototypes helps the model perceive spatial details. In the event-sentence prototype matching phase, we design a temporal prototype generation mechanism to associate intra-frame objects and interact inter-frame temporal relations. Such progressively generated event prototypes can reveal semantic diversity in videos for dynamic matching. Validated by comprehensive experiments, our method consistently outperforms the state-of-the-art methods on four video retrieval benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Li_Progressive_Spatio-Temporal_Prototype_Matching_for_Text-Video_Retrieval_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Pandeng Li</author><author>Chen-Wei Xie</author><author>Liming Zhao</author><author>Hongtao Xie</author><author>Jiannan Ge</author><author>Yun Zheng</author><author>Deli Zhao</author><author>Yongdong Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Data Augmented Flatness-aware Gradient Projection for Continual Learning</title>
            <abstract>The goal of continual learning (CL) is to continuously learn new tasks without forgetting previously learned old tasks. To alleviate catastrophic forgetting, gradient projection based CL methods require that the gradient updates of new tasks are orthogonal to the subspace spanned by old tasks. This limits the learning process and leads to poor performance on the new task due to the projection constraint being too strong. In this paper, we first revisit the gradient projection method from the perspective of flatness of loss surface, and find that unflatness of the loss surface leads to catastrophic forgetting of the old tasks when the projection constraint is reduced to improve the performance of new tasks. Based on our findings, we propose a Data Augmented Flatness-aware Gradient Projection (DFGP) method to solve the problem, which consists of three modules: data and weight perturbation, flatness-aware optimization, and gradient projection. Specifically, we first perform a flatness-aware perturbation on the task data and current weights to find the case that makes the task loss worst. Next, flatness-aware optimization optimizes both the loss and the flatness of the loss surface on raw and worst-case perturbed data to obtain a flatness-aware gradient. Finally, gradient projection updates the network with the flatness-aware gradient along directions orthogonal to the subspace of the old tasks. Extensive experiments on four datasets show that our method improves the flatness of loss surface and the performance of new tasks, and achieves state-of-the-art (SOTA) performance in the average accuracy of all tasks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yang_Data_Augmented_Flatness-aware_Gradient_Projection_for_Continual_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yang_Data_Augmented_Flatness-aware_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Enneng Yang</author><author>Li Shen</author><author>Zhenyi Wang</author><author>Shiwei Liu</author><author>Guibing Guo</author><author>Xingwei Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Sample-wise Label Confidence Incorporation for Learning with Noisy Labels</title>
            <abstract>Deep learning algorithms require large amounts of labeled data for effective performance, but the presence of noisy labels often significantly degrade their performance. Although recent studies on designing a robust objective function to label noise, known as the robust loss method, have shown promising results for learning with noisy labels, they suffer from the issue of underfitting not only noisy samples but also clean ones, leading to suboptimal model performance. To address this issue, we propose a novel learning framework that selectively suppresses noisy samples while avoiding underfitting clean data. Our framework incorporates label confidence as a measure of label noise, enabling the network model to prioritize the training of samples deemed to be noise-free. The label confidence is based on the robust loss methods, and we provide theoretical evidence that our method can reach the optimal point of the robust loss, subject to certain conditions. Furthermore, the proposed method is generalizable and can be combined with existing robust loss methods, making it suitable for a wide range of applications of learning with noisy labels. We evaluate our approach on both synthetic and real-world datasets, and the experimental results demonstrate its effectiveness in achieving outstanding classification performance compared to state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ahn_Sample-wise_Label_Confidence_Incorporation_for_Learning_with_Noisy_Labels_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ahn_Sample-wise_Label_Confidence_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Chanho Ahn</author><author>Kikyung Kim</author><author>Ji-won Baek</author><author>Jongin Lim</author><author>Seungju Han</author>
            </authors>
        </paper>
        

        <paper>
            <title>CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation</title>
            <abstract>There has been a growing interest in developing multimodal machine translation (MMT) systems that enhance neural machine translation (NMT) with visual knowledge. This problem setup involves using images as auxiliary information during training, and more recently, eliminating their use during inference. Towards this end, previous works face a challenge in training powerful MMT models from scratch due to the scarcity of annotated multilingual vision-language data, especially for low-resource languages. Simultaneously, there has been an influx of multilingual pre-trained models for NMT and multimodal pre-trained models for vision-language tasks, primarily in English, which have shown exceptional generalisation ability. However, these are not directly applicable to MMT since they do not provide aligned multimodal multilingual features for generative tasks. To alleviate this issue, instead of designing complex modules for MMT, we propose CLIPTrans, which simply adapts the independently pre-trained multimodal M-CLIP and the multilingual mBART. In order to align their embedding spaces, mBART is conditioned on the M-CLIP features by a prefix sequence generated through a lightweight mapping network. We train this in a two-stage pipeline which warms up the model with image captioning before the actual translation task. Through experiments, we demonstrate the merits of this framework and consequently push forward the state-of-the-art across standard benchmarks by an average of +2.67 BLEU. The code can be found at www.github.com/devaansh100/CLIPTrans.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Gupta_CLIPTrans_Transferring_Visual_Knowledge_with_Pre-trained_Models_for_Multimodal_Machine_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Gupta_CLIPTrans_Transferring_Visual_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Devaansh Gupta</author><author>Siddhant Kharbanda</author><author>Jiawei Zhou</author><author>Wanhua Li</author><author>Hanspeter Pfister</author><author>Donglai Wei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Ego-Only: Egocentric Action Detection without Exocentric Transferring</title>
            <abstract>We present Ego-Only, the first approach that enables state-of-the-art action detection on egocentric (first-person) videos without any form of exocentric (third-person) transferring. Despite the content and appearance gap separating the two domains, large-scale exocentric transferring has been the default choice for egocentric action detection. This is because prior works found that egocentric models are difficult to train from scratch and that transferring from exocentric representations leads to improved accuracy. However, in this paper, we revisit this common belief. Motivated by the large gap separating the two domains, we propose a strategy that enables effective training of egocentric models without exocentric transferring. Our Ego-Only approach is simple. It trains the video representation with a masked autoencoder finetuned for temporal segmentation. The learned features are then fed to an off-the-shelf temporal action localization method to detect actions. We find that this renders exocentric transferring unnecessary by showing remarkably strong results achieved by this simple Ego-Only approach on three established egocentric video datasets: Ego4D, EPIC-Kitchens-100, and Charades-Ego. On both action detection and action recognition, Ego-Only outperforms previous best exocentric transferring methods that use orders of magnitude more labels. Ego-Only sets new state-of-the-art results on these datasets and benchmarks without exocentric data.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Ego-Only_Egocentric_Action_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Huiyu Wang</author><author>Mitesh Kumar Singh</author><author>Lorenzo Torresani</author>
            </authors>
        </paper>
        

        <paper>
            <title>CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation</title>
            <abstract>Class incremental semantic segmentation aims to strike a balance between the model&apos;s stability and plasticity by maintaining old knowledge while adapting to new concepts. However, most state-of-the-art methods use the freeze strategy for stability, which compromises the model&apos;s plasticity. In contrast, releasing parameter training for plasticity could lead to the best performance for all categories, but this requires discriminative feature representation. Therefore, we prioritize the model&apos;s plasticity and propose the Contrast inter- and intra-class representations for Incremental Segmentation (CoinSeg), which pursue discriminative representations for flexible parameter tuning. Inspired by the Gaussian mixture model that samples from a mixture of Gaussian distributions, CoinSeg emphasizes intra-class diversity with multiple contrastive representation centroids. Specifically, we use mask proposals to identify regions with strong objectness that are likely to be diverse instances/centroids of a category. These mask proposals are then used for contrastive representations to reinforce intra-class diversity. Meanwhile, to avoid bias from intra-class diversity, we also apply category-level pseudo-labels to enhance category-level consistency and inter-category diversity. Additionally, CoinSeg ensures the model&apos;s stability and alleviates forgetting through a specific flexible tuning strategy. We validate CoinSeg on Pascal VOC 2012 and ADE20K datasets with multiple incremental scenarios and achieve superior results compared to previous state-of-the-art methods, especially in more challenging and realistic long-term scenarios.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_CoinSeg_Contrast_Inter-_and_Intra-_Class_Representations_for_Incremental_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhang_CoinSeg_Contrast_Inter-_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Zekang Zhang</author><author>Guangyu Gao</author><author>Jianbo Jiao</author><author>Chi Harold Liu</author><author>Yunchao Wei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multi-View Active Fine-Grained Visual Recognition</title>
            <abstract>Despite the remarkable progress of Fine-grained visual classification (FGVC) with years of history, it is still limited to recognizing 2 images. Recognizing objects in the physical world (i.e., 3D environment) poses a unique challenge -- discriminative information is not only present in visible local regions but also in other unseen views. Therefore, in addition to finding the distinguishable part from the current view, efficient and accurate recognition requires inferring the critical perspective with minimal glances. E.g., a person might recognize a &quot;Ford sedan&quot; with a glance at its side and then know that looking at the front can help tell which model it is. In this paper, towards FGVC in the real physical world, we put forward the problem of multi-view active fine-grained visual recognition (MAFR) and complete this study in three steps: (i) a multi-view, fine-grained vehicle dataset is collected as the testbed, (ii) a pilot experiment is designed to validate the need and research value of MAFR, (iii) a policy-gradient-based framework along with a dynamic exiting strategy is proposed to achieve efficient recognition with active view selection. Our comprehensive experiments demonstrate that the proposed method outperforms previous multi-view recognition works and can extend existing state-of-the-art FGVC methods and advanced neural networks to become FGVC experts in the 3D environment.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Du_Multi-View_Active_Fine-Grained_Visual_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Ruoyi Du</author><author>Wenqing Yu</author><author>Heqing Wang</author><author>Ting-En Lin</author><author>Dongliang Chang</author><author>Zhanyu Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>Variational Causal Inference Network for Explanatory Visual Question Answering</title>
            <abstract>Explanatory Visual Question Answering (EVQA) is a recently proposed multimodal reasoning task that requires answering visual questions and generating multimodal explanations for the reasoning processes. Unlike traditional Visual Question Answering (VQA) which focuses solely on answering, EVQA aims to provide user-friendly explanations to enhance the explainability and credibility of reasoning models. However, existing EVQA methods typically predict the answer and explanation separately, which ignores the causal correlation between them. Moreover, they neglect the complex relationships among question words, visual regions, and explanation tokens. To address these issues, we propose a Variational Causal Inference Network (VCIN) that establishes the causal correlation between predicted answers and explanations, and captures cross-modal relationships to generate rational explanations. First, we utilize a vision-and-language pretrained model to extract visual features and question features. Secondly, we propose a multimodal explanation gating transformer that constructs cross-modal relationships and generates rational explanations. Finally, we propose a variational causal inference to establish the target causal structure and predict the answers. Comprehensive experiments demonstrate the superiority of VCIN over state-of-the-art EVQA methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Xue_Variational_Causal_Inference_Network_for_Explanatory_Visual_Question_Answering_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Xue_Variational_Causal_Inference_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Dizhan Xue</author><author>Shengsheng Qian</author><author>Changsheng Xu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation</title>
            <abstract>Deep neural networks are vulnerable to universal adversarial perturbation (UAP), an instance-agnostic perturbation capable of fooling the target model for most samples. Compared to instance-specific adversarial examples, UAP is more challenging as it needs to generalize across various samples and models. In this paper, we examine the serious dilemma of UAP generation methods from a generalization perspective -- the gradient vanishing problem using small-batch stochastic gradient optimization and the local optima problem using large-batch optimization. To address these problems, we propose a simple and effective method called Stochastic Gradient Aggregation (SGA), which alleviates the gradient vanishing and escapes from poor local optima at the same time. Specifically, SGA employs the small-batch training to perform multiple iterations of inner pre-search. Then, all the inner gradients are aggregated as a one-step gradient estimation to enhance the gradient stability and reduce quantization errors. Extensive experiments on the standard ImageNet dataset demonstrate that our method significantly enhances the generalization ability of UAP and outperforms other state-of-the-art methods. The code is available at https://github.com/liuxuannan/Stochastic-Gradient-Aggregation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Enhancing_Generalization_of_Universal_Adversarial_Perturbation_through_Gradient_Aggregation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Liu_Enhancing_Generalization_of_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xuannan Liu</author><author>Yaoyao Zhong</author><author>Yuhang Zhang</author><author>Lixiong Qin</author><author>Weihong Deng</author>
            </authors>
        </paper>
        

        <paper>
            <title>Parallel Attention Interaction Network for Few-Shot Skeleton-Based Action Recognition</title>
            <abstract>Learning discriminative features from very few labeled samples to identify novel classes has received increasing attention in skeleton-based action recognition. Existing works aim to learn action-specific embeddings by exploiting either intra-skeleton or inter-skeleton spatial associations, which may lead to less discriminative representations. To address these issues, we propose a novel Parallel Attention Interaction Network (PAINet) that incorporates two complementary branches to strengthen the match by inter-skeleton and intra-skeleton correlation. Specifically, a topology encoding module utilizing topology and physical information is proposed to enhance the modeling of interactive parts and joint pairs in both branches. In the Cross Spatial Alignment branch, we employ a spatial cross-attention module to establish joint associations across sequences, and a directional Average Symmetric Surface Metric is introduced to locate the closest temporal similarity. In parallel, the Cross Temporal Alignment branch incorporates a spatial self-attention module to aggregate spatial context within sequences as well as applies the temporal cross-attention network to correct misalignment temporally and calculate similarity. Extensive experiments on three skeleton benchmarks, namely NTU-T, NTU-S, and Kinetics, demonstrate the superiority of our framework and consistently outperform state-of-the-art methods.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Liu_Parallel_Attention_Interaction_Network_for_Few-Shot_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Xingyu Liu</author><author>Sanping Zhou</author><author>Le Wang</author><author>Gang Hua</author>
            </authors>
        </paper>
        

        <paper>
            <title>Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement</title>
            <abstract>The popularity of Contrastive Language-Image Pre-training (CLIP) has propelled its application to diverse downstream vision tasks. To improve its capacity on downstream tasks, few-shot learning has become a widely-adopted technique. However, existing methods either exhibit limited performance or suffer from excessive learnable parameters. In this paper, we propose APE, an Adaptive Prior rEfinement method for CLIP&apos;s pre-trained knowledge, which achieves superior accuracy with high computational efficiency. Via a prior refinement module, we analyze the inter-class disparity in the downstream data and decouple the domain-specific knowledge from the CLIP-extracted cache model. On top of that, we introduce two model variants, a training-free APE and a training-required APE-T. We explore the trilateral affinities between the test image, prior cache model, and textual representations, and only enable a lightweight category-residual module to be trained. For the average accuracy over 11 benchmarks, both APE and APE-T attain state-of-the-art and respectively outperform the second-best by +1.59% and +1.99% under 16 shots with x30 less learnable parameters.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhu_Not_All_Features_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Xiangyang Zhu</author><author>Renrui Zhang</author><author>Bowei He</author><author>Aojun Zhou</author><author>Dong Wang</author><author>Bin Zhao</author><author>Peng Gao</author>
            </authors>
        </paper>
        

        <paper>
            <title>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</title>
            <abstract>Video-language pre-training (VLP) has become increasingly important due to its ability to generalize to various vision and language tasks. However, existing egocentric VLP frameworks utilize separate video and language encoders and learn task-specific cross-modal information only during fine-tuning, limiting the development of a unified system. In this work, we introduce the second generation of egocentric video-language pre-training (EgoVLPv2), a significant improvement from the previous generation, by incorporating cross-modal fusion directly into the video and language backbones. EgoVLPv2 learns strong video-text representation during pre-training and reuses the cross-modal attention modules to support different downstream tasks in a flexible and efficient manner, reducing fine-tuning costs. Moreover, our proposed fusion in the backbone strategy is more lightweight and compute-efficient than stacking additional fusion-specific layers. Extensive experiments on a wide range of VL tasks demonstrate the effectiveness of EgoVLPv2 by achieving consistent state-of-the-art performance over strong baselines across all downstream.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pramanick_EgoVLPv2_Egocentric_Video-Language_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shraman Pramanick</author><author>Yale Song</author><author>Sayan Nag</author><author>Kevin Qinghong Lin</author><author>Hardik Shah</author><author>Mike Zheng Shou</author><author>Rama Chellappa</author><author>Pengchuan Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>Deep Equilibrium Object Detection</title>
            <abstract>Query-based object detectors directly decode image features into object instances with a set of learnable queries. These query vectors are progressively refined to stable meaningful representations through a sequence of decoder layers, and then used to directly predict object locations and categories with simple FFN heads. In this paper, we present a new query-based object detector (DEQDet) by designing a deep equilibrium decoder. Our DEQ decoder models the query vector refinement as the fixed point solving of an implicit layer and is equivalent to applying infinite steps of refinement. To be more specific to object decoding, we use a two-step unrolled equilibrium equation to explicitly capture the query vector refinement. Accordingly, we are able to incorporate refinement awareness into the DEQ training with the inexact gradient back-propagation (RAG). In addition, to stabilize the training of our DEQDet and improve its generalization ability, we devise the deep supervision scheme on the optimization path of DEQ with refinement-aware perturbation (RAP). Our experiments demonstrate DEQDet converges faster, consumes less memory, and achieves better results than the baseline counterpart (AdaMixer). In particular, our DEQDet with ResNet50 backbone and 300 queries achieves the 49.5 mAP and 33.0 APs on the MS COCO benchmark under 2x training scheme (24 epochs).</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wang_Deep_Equilibrium_Object_Detection_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Wang_Deep_Equilibrium_Object_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shuai Wang</author><author>Yao Teng</author><author>Limin Wang</author>
            </authors>
        </paper>
        

        <paper>
            <title>SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training</title>
            <abstract>Video-language pre-training is crucial for learning powerful multi-modal representation. However, it typically requires a massive amount of computation. In this paper, we develop SMAUG, an efficient pre-training framework for video-language models. The foundation component in SMAUG is masked autoencoders. Different from prior works which only mask textual inputs, our masking strategy considers both visual and textual modalities, providing a better cross-modal alignment and saving more pre-training costs. On top of that, we introduce a space-time token sparsification module, which leverages context information to further select only &quot;important&quot; spatial regions and temporal frames for pre-training. Coupling all these designs allows our method to enjoy both competitive performances on text-to-video retrieval and video question answering tasks, and much less pre-training costs by 1.9x or more. For example, our SMAUG only needs 50 NVIDIA A6000 GPU hours for pre-training to attain competitive performances on these two video-language tasks across six popular benchmarks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Lin_SMAUG_Sparse_Masked_Autoencoder_for_Efficient_Video-Language_Pre-Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yuanze Lin</author><author>Chen Wei</author><author>Huiyu Wang</author><author>Alan Yuille</author><author>Cihang Xie</author>
            </authors>
        </paper>
        

        <paper>
            <title>Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples</title>
            <abstract>Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called one-shot VFL that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose few-shot VFL to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular datasets. Our methods can improve the accuracy by more than 46.5% and reduce the communication cost by more than 330xcompared with state-of-the-art VFL methods when evaluated on CIFAR-10. Our code is publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Sun_Communication-Efficient_Vertical_Federated_Learning_with_Limited_Overlapping_Samples_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Jingwei Sun</author><author>Ziyue Xu</author><author>Dong Yang</author><author>Vishwesh Nath</author><author>Wenqi Li</author><author>Can Zhao</author><author>Daguang Xu</author><author>Yiran Chen</author><author>Holger R. Roth</author>
            </authors>
        </paper>
        

        <paper>
            <title>On the Audio-visual Synchronization for Lip-to-Speech Synthesis</title>
            <abstract>Most lip-to-speech (LTS) synthesis models are trained and evaluated with the assumption that the audio-video pairs in the dataset are well synchronized. In this work, we demonstrate that commonly used audiovisual datasets such as GRID, TCD-TIMIT, and Lip2Wav can, however, have the data asynchrony issue, which will lead to inaccurate evaluation with conventional time alignment-sensitive metrics such as STOI, ESTOI, and MCD. Moreover, training an LTS model with such datasets can result in model asynchrony, meaning that the generated speech and input video are out of sync. To address these problems, we first provide a time-alignment frontend for the commonly used metrics to ensure accurate evaluation. Then, we propose a synchronized lip-to-speech (SLTS) model with an automatic synchronization mechanism (ASM) that corrects data asynchrony and penalizes model asynchrony during training. We evaluated the effectiveness of our approach on both artificial and popular audiovisual datasets. Our proposed method outperforms existing SOTA models in a variety of evaluation metrics.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Niu_On_the_Audio-visual_Synchronization_for_Lip-to-Speech_Synthesis_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Zhe Niu</author><author>Brian Mak</author>
            </authors>
        </paper>
        

        <paper>
            <title>BallGAN: 3D-aware Image Synthesis with a Spherical Background</title>
            <abstract>3D-aware GANs aim to synthesize realistic 3D scenes that can be rendered in arbitrary camera viewpoints, generating high-quality images with well-defined geometry. As 3D content creation becomes more popular, the ability to generate foreground objects separately from the background has become a crucial property. Existing methods have been developed regarding overall image quality, but they can not generate foreground objects only and often show degraded 3D geometry. In this work, we propose to represent the background as a spherical surface for multiple reasons inspired by computer graphics. Our method naturally provides foreground-only 3D synthesis facilitating easier 3D content creation. Furthermore, it improves the foreground geometry of 3D-aware GANs and the training stability on datasets with complex backgrounds.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Shin_BallGAN_3D-aware_Image_Synthesis_with_a_Spherical_Background_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Shin_BallGAN_3D-aware_Image_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Minjung Shin</author><author>Yunji Seo</author><author>Jeongmin Bae</author><author>Young Sun Choi</author><author>Hyunsu Kim</author><author>Hyeran Byun</author><author>Youngjung Uh</author>
            </authors>
        </paper>
        

        <paper>
            <title>AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism</title>
            <abstract>Generating 3D human motion based on textual descriptions has been a research focus in recent years. It requires the generated motion to be diverse, natural, and conform to the textual description. Due to the complex spatio-temporal nature of human motion and the difficulty in learning the cross-modal relationship between text and motion, text-driven motion generation is still a challenging problem. To address these issues, we propose AttT2M, a two-stage method with multi-perspective attention mechanism: body-part attention and global-local motion-text attention. The former focuses on the motion embedding perspective, which means introducing a body-part spatio-temporal encoder into VQ-VAE to learn a more expressive discrete latent space. The latter is from the cross-modal perspective, which is used to learn the sentence-level and word-level motion-text cross-modal relationship. The text-driven motion is finally generated with a generative transformer. Extensive experiments conducted on HumanML3D and KIT-ML demonstrate that our method outperforms the current state-of-the-art works in terms of qualitative and quantitative evaluation, and achieve fine-grained synthesis and action2motion. Our code will be publicly available.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhong_AttT2M_Text-Driven_Human_Motion_Generation_with_Multi-Perspective_Attention_Mechanism_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhong_AttT2M_Text-Driven_Human_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Chongyang Zhong</author><author>Lei Hu</author><author>Zihao Zhang</author><author>Shihong Xia</author>
            </authors>
        </paper>
        

        <paper>
            <title>A Theory of Topological Derivatives for Inverse Rendering of Geometry</title>
            <abstract>We introduce a theoretical framework for differentiable surface evolution that allows discrete topology changes through the use of topological derivatives for variational optimization of image functionals. While prior methods for inverse rendering of geometry rely on silhouette gradients for topology changes, such signals are sparse. In contrast, our theory derives topological derivatives that relate the introduction of vanishing holes and phases to changes in image intensity. As a result, we enable differentiable shape perturbations in the form of hole or phase nucleation. We validate the proposed theory with optimization of closed curves in 2D and surfaces in 3D to lend insights into limitations of current methods and enable improved applications such as image vectorization, vector-graphics generation from text prompts, single-image reconstruction of shape ambigrams and multiview 3D reconstruction.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Mehta_A_Theory_of_Topological_Derivatives_for_Inverse_Rendering_of_Geometry_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Mehta_A_Theory_of_Topological_Derivatives_for_Inverse_Rendering_of_Geometry_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Ishit Mehta</author><author>Manmohan Chandraker</author><author>Ravi Ramamoorthi</author>
            </authors>
        </paper>
        

        <paper>
            <title>Canonical Factors for Hybrid Neural Fields</title>
            <abstract>Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a simple two-dimensional model problem that a hybrid architecture that simultaneously learns these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using 2D image, signed distance field, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of standard procedures for evaluating neural field representations.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Yi_Canonical_Factors_for_Hybrid_Neural_Fields_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Yi_Canonical_Factors_for_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Brent Yi</author><author>Weijia Zeng</author><author>Sam Buchanan</author><author>Yi Ma</author>
            </authors>
        </paper>
        

        <paper>
            <title>GET: Group Event Transformer for Event-Based Vision</title>
            <abstract>Event cameras are a type of novel neuromorphic sen-sor that has been gaining increasing attention. Existing event-based backbones mainly rely on image-based designs to extract spatial information within the image transformed from events, overlooking important event properties like time and polarity. To address this issue, we propose a novel Group-based vision Transformer backbone for Event-based vision, called Group Event Transformer (GET), which de-couples temporal-polarity information from spatial infor-mation throughout the feature extraction process. Specifi-cally, we first propose a new event representation for GET, named Group Token, which groups asynchronous events based on their timestamps and polarities. Then, GET ap-plies the Event Dual Self-Attention block, and Group Token Aggregation module to facilitate effective feature commu-nication and integration in both the spatial and temporal-polarity domains. After that, GET can be integrated with different downstream tasks by connecting it with vari-ous heads. We evaluate our method on four event-based classification datasets (Cifar10-DVS, N-MNIST, N-CARS, and DVS128Gesture) and two event-based object detection datasets (1Mpx and Gen1), and the results demonstrate that GET outperforms other state-of-the-art methods. The code is available at https://github.com/Peterande/GET-Group-Event-Transformer.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Peng_GET_Group_Event_ICCV_2023_supplemental.zip</supp_url>
            <authors>
                <author>Yansong Peng</author><author>Yueyi Zhang</author><author>Zhiwei Xiong</author><author>Xiaoyan Sun</author><author>Feng Wu</author>
            </authors>
        </paper>
        

        <paper>
            <title>When Do Curricula Work in Federated Learning?</title>
            <abstract>An oft-cited open problem of federated learning is the existence of data heterogeneity among clients. One path- way to understanding the drastic accuracy drop in feder- ated learning is by scrutinizing the behavior of the clients&apos; deep models on data with different levels of &quot;difficulty&quot;, which has been left unaddressed. In this paper, we investi- gate a different and rarely studied dimension of FL: ordered learning. Specifically, we aim to investigate how ordered learning principles can contribute to alleviating the hetero- geneity effects in FL. We present theoretical analysis and conduct extensive empirical studies on the efficacy of or- derings spanning three kinds of learning: curriculum, anti- curriculum, and random curriculum. We find that curricu- lum learning largely alleviates non-IIDness. Interestingly, the more disparate the data distributions across clients the more they benefit from ordered learning. We provide analysis explaining this phenomenon, specifically indicating how curriculum training appears to make the objective land- scape progressively less convex, suggesting fast converging iterations at the beginning of the training procedure. We derive quantitative results of convergence for both convex and nonconvex objectives by modeling the curriculum train- ing on federated devices as local SGD with locally biased stochastic gradients. Also, inspired by ordered learning, we propose a novel client selection technique that benefits from the real-world disparity in the clients. Our proposed approach to client selection has a synergic effect when applied together with ordered learning in FL.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Vahidian_When_Do_Curricula_Work_in_Federated_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Vahidian_When_Do_Curricula_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Saeed Vahidian</author><author>Sreevatsank Kadaveru</author><author>Woonjoon Baek</author><author>Weijia Wang</author><author>Vyacheslav Kungurtsev</author><author>Chen Chen</author><author>Mubarak Shah</author><author>Bill Lin</author>
            </authors>
        </paper>
        

        <paper>
            <title>Audio-Visual Class-Incremental Learning</title>
            <abstract>In this paper, we introduce audio-visual class-incremental learning, a class-incremental learning scenario for audio-visual video recognition. We demonstrate that joint audio-visual modeling can improve class-incremental learning, but current methods fail to preserve semantic similarity between audio and visual features as incremental step grows. Furthermore, we observe that audio-visual correlations learned in previous tasks can be forgotten as incremental steps progress, leading to poor performance. To overcome these challenges, we propose AV-CIL, which incorporates Dual-Audio-Visual Similarity Constraint (D-AVSC) to maintain both instance-aware and class-aware semantic similarity between audio-visual modalities and Visual Attention Distillation (VAD) to retain previously learned audio-guided visual attentive ability. We create three audio-visual class-incremental datasets, AVE-Class-Incremental (AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and VGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and VGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and VS100-CI demonstrate that AV-CIL significantly outperforms existing class-incremental learning methods in audio-visual class-incremental learning. Code and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Pian_Audio-Visual_Class-Incremental_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Pian_Audio-Visual_Class-Incremental_Learning_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Weiguo Pian</author><author>Shentong Mo</author><author>Yunhui Guo</author><author>Yapeng Tian</author>
            </authors>
        </paper>
        

        <paper>
            <title>Towards Viewpoint-Invariant Visual Recognition via Adversarial Training</title>
            <abstract>Visual recognition models are not invariant to viewpoint changes in the 3D world, as different viewing directions can dramatically affect the predictions given the same object. Although many efforts have been devoted to making neural networks invariant to 2D image translations and rotations, viewpoint invariance is rarely investigated. As most models process images in the perspective view, it is challenging to impose invariance to 3D viewpoint changes based only on 2D inputs. Motivated by the success of adversarial training in promoting model robustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) to improve viewpoint robustness of common image classifiers. By regarding viewpoint transformation as an attack, VIAT is formulated as a minimax optimization problem, where the inner maximization characterizes diverse adversarial viewpoints by learning a Gaussian mixture distribution based on a new attack GMVFool, while the outer minimization trains a viewpoint-invariant classifier by minimizing the expected loss over the worst-case adversarial viewpoint distributions. To further improve the generalization performance, a distribution sharing strategy is introduced leveraging the transferability of adversarial viewpoints across objects. Experiments validate the effectiveness of VIAT in improving the viewpoint robustness of various image classifiers based on the diversity of adversarial viewpoints generated by GMVFool.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Ruan_Towards_Viewpoint-Invariant_Visual_Recognition_via_Adversarial_Training_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Ruan_Towards_Viewpoint-Invariant_Visual_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Shouwei Ruan</author><author>Yinpeng Dong</author><author>Hang Su</author><author>Jianteng Peng</author><author>Ning Chen</author><author>Xingxing Wei</author>
            </authors>
        </paper>
        

        <paper>
            <title>Multi-Metrics Adaptively Identifies Backdoors in Federated Learning</title>
            <abstract>The decentralized and privacy-preserving nature of federated learning (FL) makes it vulnerable to backdoor attacks aiming to manipulate the behavior of the resulting model on specific adversary-chosen inputs. However, most existing defenses based on statistical differences take effect only against specific attacks, especially when the malicious gradients are similar to benign ones or the data are highly non-independent and identically distributed (non-IID). In this paper, we revisit the distance-based defense methods and discover that i) Euclidean distance becomes meaningless in high dimensions and ii) malicious gradients with diverse characteristics cannot be identified by a single metric. To this end, we present a simple yet effective defense strategy with multi-metrics and dynamic weighting to identify backdoors adaptively. Furthermore, our novel defense has no reliance on predefined assumptions over attack settings or data distributions and little impact on benign performance. To evaluate the effectiveness of our approach, we conduct comprehensive experiments on different datasets under various attack settings, where our method achieves the best defensive performance. For instance, we achieve the lowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showing significant superiority over previous defenses. The results also demonstrate that our method can be well-adapted to a wide range of non-IID degrees without sacrificing the benign performance.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_Multi-Metrics_Adaptively_Identifies_Backdoors_in_Federated_Learning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Huang_Multi-Metrics_Adaptively_Identifies_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Siquan Huang</author><author>Yijiang Li</author><author>Chong Chen</author><author>Leyu Shi</author><author>Ying Gao</author>
            </authors>
        </paper>
        

        <paper>
            <title>FPR: False Positive Rectification for Weakly Supervised Semantic Segmentation</title>
            <abstract>Many weakly supervised semantic segmentation (WSSS) methods employ the class activation map (CAM) to generate the initial segmentation results. However, CAM often fails to distinguish the foreground from its co-occurred background (e.g., train and railroad), resulting in inaccurate activation from the background. Previous endeavors address this co-occurrence issue by introducing external supervision and human priors. In this paper, we present a False Positive Rectification (FPR) approach to tackle the co-occurrence problem by leveraging the false positives of CAM. Based on the observation that the CAM-activated regions of absent classes contain class-specific co-occurred background cues, we collect these false positives and utilize them to guide the training of CAM network by proposing a region-level contrast loss and a pixel-level rectification loss. Without introducing any external supervision and human priors, the proposed FPR effectively suppresses wrong activations from the background objects. Extensive experiments on the PASCAL VOC 2012 and MS COCO 2014 demonstrate that FPR brings significant improvements for off-the-shelf methods and achieves state-of-the-art performance. Code is available at https://github.com/mt-cly/FPR.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chen_FPR_False_Positive_Rectification_for_Weakly_Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chen_FPR_False_Positive_Rectification_for_Weakly_Supervised_Semantic_Segmentation_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Liyi Chen</author><author>Chenyang Lei</author><author>Ruihuang Li</author><author>Shuai Li</author><author>Zhaoxiang Zhang</author><author>Lei Zhang</author>
            </authors>
        </paper>
        

        <paper>
            <title>DETRDistill: A Universal Knowledge Distillation Framework for DETR-families</title>
            <abstract>Transformer-based detectors (DETRs) are becoming popular for their simple framework, but the large model size and heavy time consumption hinder their deployment in the real world. While knowledge distillation (KD) can be an appealing technique to compress giant detectors into small ones for comparable detection performance and low inference cost. Since DETRs formulate object detection as a set prediction problem, existing KD methods designed for classic convolution-based detectors may not be directly applicable. In this paper, we propose DETRDistill, a novel knowledge distillation method dedicated to DETR-families. Specifically, we first design a Hungarian-matching logits distillation to encourage the student model to have the exact predictions as those of the teacher DETRs. Then, we propose a target-aware feature distillation to help the student model learn from the object-centric features of the teacher model. Finally, in order to improve the convergence rate of the student DETR, we introduce a query-prior assignment distillation to speed up the student model learning from well-trained queries and stable assignment of the teacher model. Extensive experimental results on the COCO dataset validate the effectiveness of our approach. Notably, DETRDistill consistently improves various DETRs by more than 2.0 mAP, even surpassing their teacher models.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Chang_DETRDistill_A_Universal_Knowledge_Distillation_Framework_for_DETR-families_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Chang_DETRDistill_A_Universal_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Jiahao Chang</author><author>Shuo Wang</author><author>Hai-Ming Xu</author><author>Zehui Chen</author><author>Chenhongyi Yang</author><author>Feng Zhao</author>
            </authors>
        </paper>
        

        <paper>
            <title>F&amp;F Attack: Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives</title>
            <abstract>Multi-object tracking (MOT) aims to build moving trajectories for number-agnostic objects. Modern multi-object trackers commonly follow the tracking-by-detection strategy. Therefore, fooling detectors can be an effective solution but it usually requires attacks in multiple successive frames, resulting in low efficiency. Attacking association processes improves efficiency but may require model-specific design, leading to poor generalization. In this paper, we propose a novel False negative and False positive attack (F&amp;F attack) mechanism: it perturbs the input image to erase original detections and to inject deceptive false alarms around original ones while integrating the association attack implicitly. The mechanism can produce effective identity switches against multi-object trackers by only fooling detectors in a few frames. To demonstrate the flexibility of the mechanism, we deploy it to three multi-object trackers (ByteTrack, SORT, and CenterTrack) which are enabled by two representative detectors (YOLOX and CenterNet). Comprehensive experiments on MOT17 and MOT20 datasets show that our method significantly outperforms existing attackers, revealing the vulnerability of the tracking-by-detection paradigm to detection attacks.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_FF_Attack_Adversarial_Attack_against_Multiple_Object_Trackers_by_Inducing_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Zhou_FF_Attack_Adversarial_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Tao Zhou</author><author>Qi Ye</author><author>Wenhan Luo</author><author>Kaihao Zhang</author><author>Zhiguo Shi</author><author>Jiming Chen</author>
            </authors>
        </paper>
        

        <paper>
            <title>Transferable Decoding with Visual Entities for Zero-Shot Image Captioning</title>
            <abstract>Image-to-text generation aims to describe images using natural language. Recently, zero-shot image captioning based on pre-trained vision-language models (VLMs) and large language models (LLMs) has made significant progress. However, we have observed and empirically demonstrated that these methods are susceptible to modality bias induced by LLMs and tend to generate descriptions containing objects (entities) that do not actually exist in the image but frequently appear during training (i.e., object hallucination). In this paper, we propose ViECap, a transferable decoding model that leverages entity-aware decoding to generate descriptions in both seen and unseen scenarios. ViECap incorporates entity-aware hard prompts to guide LLMs&apos; attention toward the visual entities present in the image, enabling coherent caption generation across diverse scenes. With entity-aware hard prompts, ViECap is capable of maintaining performance when transferring from in-domain to out-of-domain scenarios. Extensive experiments demonstrate that ViECap sets a new state-of-theart cross-domain (transferable) captioning and performs competitively in-domain captioning compared to previous VLMs-based zero-shot methods. Our code is available at: https://github.com/FeiElysia/ViECap</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>http://openaccess.thecvf.com//content/ICCV2023/supplemental/Fei_Transferable_Decoding_with_ICCV_2023_supplemental.pdf</supp_url>
            <authors>
                <author>Junjie Fei</author><author>Teng Wang</author><author>Jinrui Zhang</author><author>Zhenyu He</author><author>Chengjie Wang</author><author>Feng Zheng</author>
            </authors>
        </paper>
        

        <paper>
            <title>ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model</title>
            <abstract>3D human motion generation is crucial for creative industry. Recent advances rely on generative models with domain knowledge for text-driven motion generation, leading to substantial progress in capturing common motions. However, the performance on more diverse motions remains unsatisfactory. In this work, we propose ReMoDiffuse, a diffusion-model-based motion generation framework that integrates a retrieval mechanism to refine the denoising process. ReMoDiffuse enhances the generalizability and diversity of text-driven motion generation with three key designs:1) Hybrid Retrieval finds appropriate references from the database in terms of both semantic and kinematic similarities. 2) Semantic-Modulated Transformer selectively absorbs retrieval knowledge, adapting to the difference between retrieved samples and the target motion sequence. 3) Condition Mixture better utilizes the retrieval database during inference, overcoming the scale sensitivity in classifier-free guidance. Extensive experiments demonstrate that \name outperforms state-of-the-art methods by balancing both text-motion consistency and motion quality, especially for more diverse motion generation.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Mingyuan Zhang</author><author>Xinying Guo</author><author>Liang Pan</author><author>Zhongang Cai</author><author>Fangzhou Hong</author><author>Huirong Li</author><author>Lei Yang</author><author>Ziwei Liu</author>
            </authors>
        </paper>
        

        <paper>
            <title>Advancing Referring Expression Segmentation Beyond Single Image</title>
            <abstract>Referring Expression Segmentation (RES) is a widely explored multi-modal task, which endeavors to segment the pre-existing object within a single image with a given linguistic expression. However, in broader real-world scenarios, it is not always possible to determine if the described object exists in a specific image. Generally, a collection of images is available, some of which potentially contain the target objects. To this end, we propose a more realistic setting, named Group-wise Referring Expression Segmentation (GRES), which expands RES to a group of related images, allowing the described objects to exist in a subset of the input image group. To support this new setting, we introduce an elaborately compiled dataset named Grouped Referring Dataset (GRD), containing complete group-wise annotations of the target objects described by given expressions. Moreover, we also present a baseline method named Grouped Referring Segmenter (GRSer), which explicitly captures the language-vision and intra-group vision-vision interactions to achieve state-of-the-art results on the proposed GRES setting and related tasks, such as Co-Salient Object Detection and traditional RES. Our dataset and codes are publicly released in https://github.com/shikras/d-cube.</abstract>
            <pdf_url>http://openaccess.thecvf.com//content/ICCV2023/papers/Wu_Advancing_Referring_Expression_Segmentation_Beyond_Single_Image_ICCV_2023_paper.pdf</pdf_url>
            <supp_url>None</supp_url>
            <authors>
                <author>Yixuan Wu</author><author>Zhao Zhang</author><author>Chi Xie</author><author>Feng Zhu</author><author>Rui Zhao</author>
            </authors>
        </paper>
        
