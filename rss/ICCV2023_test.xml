<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
    <title>ICCV 2023 Papers</title>
<item>
    <title><![CDATA[Towards Attack-tolerant Federated Learning via Critical Parameter Analysis]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Han_Towards_Attack-tolerant_Federated_Learning_via_Critical_Parameter_Analysis_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Federated learning is used to train a shared model in a decentralized way without clients sharing private data with each other. Federated learning systems are susceptible to poisoning attacks when malicious clients send false updates to the central server. Existing defense strategies are ineffective under non-IID data settings. This paper proposes a new defense strategy, FedCPA (Federated learning with Critical Parameter Analysis). Our attack-tolerant aggregation method is based on the observation that benign local models have similar sets of top-k and bottom-k critical parameters, whereas poisoned local models do not. Experiments with different attack scenarios on multiple datasets demonstrate that our model outperforms existing defense strategies in defending against poisoning attacks.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4999</startPage>
    <endPage>5008</endPage>
    <authors><![CDATA[Sungwon Han;Sungwon Park;Fangzhao Wu;Sundong Kim;Bin Zhu;Xing Xie;Meeyoung Cha]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Stochastic Segmentation with Conditional Categorical Diffusion Models]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zbinden_Stochastic_Segmentation_with_Conditional_Categorical_Diffusion_Models_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Semantic segmentation has made significant progress in recent years thanks to deep neural networks, but the common objective of generating a single segmentation output that accurately matches the image&apos;s content may not be suitable for safety-critical domains such as medical diagnostics and autonomous driving. Instead, multiple possible correct segmentation maps may be required to reflect the true distribution of annotation maps. In this context, stochastic semantic segmentation methods must learn to predict conditional distributions of labels given the image, but this is challenging due to the typically multimodal distributions, high-dimensional output spaces, and limited annotation data. To address these challenges, we propose a conditional categorical diffusion model (CCDM) for semantic segmentation based on Denoising Diffusion Probabilistic Models. Our model is conditioned to the input image, enabling it to generate multiple segmentation label maps that account for the aleatoric uncertainty arising from divergent ground truth annotations. Our experimental results show that CCDM achieves state-of-the-art performance on LIDC, a stochastic semantic segmentation dataset, and outperforms established baselines on the classical segmentation dataset Cityscapes.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>1119</startPage>
    <endPage>1129</endPage>
    <authors><![CDATA[Lukas Zbinden;Lars Doorenbos;Theodoros Pissas;Adrian Thomas Huber;Raphael Sznitman;Pablo M rquez-Neila]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain&apos;s Recognition Mechanism]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_A_Dynamic_Dual-Processing_Object_Detection_Framework_Inspired_by_the_Brains_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[There are two main approaches to object detection: CNN-based and Transformer-based. The former views object detection as a dense local matching problem, while the latter sees it as a sparse global retrieval problem. Research in neuroscience has shown that the recognition decision in the brain is based on two processes, namely familiarity and recollection. Based on this biological support, we propose an efficient and effective dual-processing object detection framework. It integrates CNN- and Transformer-based detectors into a comprehensive object detection system consisting of a shared backbone, an efficient dual-stream encoder, and a dynamic dual-decoder. To better integrate local and global features, we design a search space for the CNN-Transformer dual-stream encoder to find the optimal fusion solution. To enable better coordination between the CNN- and Transformer-based decoders, we provide the dual-decoder with a selective mask. This mask dynamically chooses the more advantageous decoder for each position in the image based on high-level representation. As demonstrated by extensive experiments, our approach shows flexibility and effectiveness in prompting the mAP of the various source detectors by 3.0 3.7 without increasing FLOPs.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>6264</startPage>
    <endPage>6274</endPage>
    <authors><![CDATA[Minying Zhang;Tianpeng Bu;Lulu Hu]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Lu_Hard_No-Box_Adversarial_Attack_on_Skeleton-Based_Human_Action_Recognition_with_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Recently, methods for skeleton-based human activity recognition have been shown to be vulnerable to adversarial attacks. However, these attack methods require either the full knowledge of the victim (i.e. white-box attacks), access to training data (i.e. transfer-based attacks) or frequent model queries (i.e. black-box attacks). All their requirements are highly restrictive, raising the question of how detrimental the vulnerability is. In this paper, we show that the vulnerability indeed exists. To this end, we consider a new attack task: the attacker has no access to the victim model or the training data or labels, where we coin the term hard no-box attack. Specifically, we first learn a motion manifold where we define an adversarial loss to compute a new gradient for the attack, named skeleton-motion-informed (SMI) gradient. Our gradient contains information of the motion dynamics, which is different from existing gradient-based attack methods that compute the loss gradient assuming each dimension in the data is independent. The SMI gradient can augment many gradient-based attack methods, leading to a new family of no-box attack methods. Extensive evaluation and comparison show that our method imposes a real threat to existing classifiers. They also show that the SMI gradient improves the transferability and imperceptibility of adversarial samples in both no-box and transfer-based black-box settings.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4597</startPage>
    <endPage>4606</endPage>
    <authors><![CDATA[Zhengzhi Lu;He Wang;Ziyi Chang;Guoan Yang;Hubert P. H. Shum]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Autonomous vehicles operating in complex real-world environments require accurate predictions of interactive behaviors between traffic participants. This paper tackles the interaction prediction problem by formulating it with hierarchical game theory and proposing the GameFormer model for its implementation. The model incorporates a Transformer encoder, which effectively models the relationships between scene elements, alongside a novel hierarchical Transformer decoder structure. At each decoding level, the decoder utilizes the prediction outcomes from the previous level, in addition to the shared environmental context, to iteratively refine the interaction process. Moreover, we propose a learning process that regulates an agent&apos;s behavior at the current level to respond to other agents&apos; behaviors from the preceding level. Through comprehensive experiments on large-scale real-world driving datasets, we demonstrate the state-of-the-art accuracy of our model on the Waymo interaction prediction task. Additionally, we validate the model&apos;s capacity to jointly reason about the motion plan of the ego agent and the behaviors of multiple agents in both open-loop and closed-loop planning tests, outperforming various baseline methods. Furthermore, we evaluate the efficacy of our model on the nuPlan planning benchmark, where it achieves leading performance.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>3903</startPage>
    <endPage>3913</endPage>
    <authors><![CDATA[Zhiyu Huang;Haochen Liu;Chen Lv]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Learning_in_Imperfect_Environment_Multi-Label_Classification_with_Long-Tailed_Distribution_and_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Conventional multi-label classification (MLC) methods assume that all samples are fully labeled and identically distributed. Unfortunately, this assumption is unrealistic in large-scale MLC data that has long-tailed (LT) distribution and partial labels (PL). To address the problem, we introduce a novel task, Partial labeling and Long-Tailed Multi-Label Classification (PLT-MLC), to jointly consider the above two imperfect learning environments. Not surprisingly, we find that most LT-MLC and PL-MLC approaches fail to solve the PLT-MLC, resulting in significant performance degradation on the two proposed PLT-MLC benchmarks. Therefore, we propose an end-to-end learning framework: COrrection -&gt; ModificatIon -&gt; balanCe, abbreviated as COMC. Our bootstrapping philosophy is to simultaneously correct the missing labels (Correction) with convinced prediction confidence over a class-aware threshold and to learn from these recall labels during training. We next propose a novel multi-focal modifier loss that simultaneously addresses head-tail imbalance and positive-negative imbalance to adaptively modify the attention to different samples (Modification) under the LT class distribution. We also develop a balanced training strategy by distilling the model&apos;s learning effect from head and tail samples, and thus design the balanced classifier (Balance) conditioned on the head and tail learning effect to maintain a stable performance. Our experimental study shows that the proposed method significantly outperforms the general MLC, LT-MLC and ML-MLC methods in terms of effectiveness and robustness on our newly created PLT-MLC datasets.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>1423</startPage>
    <endPage>1432</endPage>
    <authors><![CDATA[Wenqiao Zhang;Changshuo Liu;Lingze Zeng;Bengchin Ooi;Siliang Tang;Yueting Zhuang]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[In real-world scenarios, typical visual recognition systems could fail under two major causes, i.e., the misclassification between known classes and the excusable misbehavior on unknown-class images. To tackle these deficiencies, flexible visual recognition should dynamically predict multiple classes when they are unconfident between choices and reject making predictions when the input is entirely out of the training distribution. Two challenges emerge along with this novel task. First, prediction uncertainty should be separately quantified as confusion depicting inter-class uncertainties and ignorance identifying out-of-distribution samples. Second, both confusion and ignorance should be comparable between samples to enable effective decision-making. In this paper, we propose to model these two sources of uncertainty explicitly with the theory of Subjective Logic. Regarding recognition as an evidence-collecting process, confusion is then defined as conflicting evidence, while ignorance is the absence of evidence. By predicting Dirichlet concentration parameters for singletons, comprehensive subjective opinions, including confusion and ignorance, could be achieved via further evidence combinations. Through a series of experiments on synthetic data analysis, visual recognition, and open-set detection, we demonstrate the effectiveness of our methods in quantifying two sources of uncertainties and dealing with flexible recognition.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>1338</startPage>
    <endPage>1347</endPage>
    <authors><![CDATA[Lei Fan;Bo Liu;Haoxiang Li;Ying Wu;Gang Hua]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Texture Generation on 3D Meshes with Point-UV Diffusion]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Yu_Texture_Generation_on_3D_Meshes_with_Point-UV_Diffusion_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[In this work, we focus on synthesizing high-quality textures on 3D meshes. We present Point-UV diffusion, a coarse-to-fine pipeline that marries the denoising diffusion model with UV mapping to generate 3D consistent and high-quality texture images in UV space. We start with introducing a point diffusion model to synthesize low-frequency texture components with our tailored style guidance to tackle the biased color distribution. The derived coarse texture offers global consistency and serves as a condition for the subsequent UV diffusion stage, aiding in regularizing the model to generate a 3D consistent UV texture image. Then, a UV diffusion model with hybrid conditions is developed to enhance the texture fidelity in the 2D UV space. Our method can process meshes of any genus, generating diversified, geometry-compatible, and high-fidelity textures.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4206</startPage>
    <endPage>4216</endPage>
    <authors><![CDATA[Xin Yu;Peng Dai;Wenbo Li;Lan Ma;Zhengzhe Liu;Xiaojuan Qi]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Enhanced Soft Label for Semi-Supervised Semantic Segmentation]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Ma_Enhanced_Soft_Label_for_Semi-Supervised_Semantic_Segmentation_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[As a mainstream framework in the field of semi-supervised learning (SSL), self-training via pseudo labeling and its variants have witnessed impressive progress in semi-supervised semantic segmentation with the recent advance of deep neural networks. However, modern self-training based SSL algorithms use a pre-defined constant threshold to select unlabeled pixel samples that contribute to the training, thus failing to be compatible with different learning difficulties of variant categories and different learning status of the model. To address these issues, we propose Enhanced Soft Label (ESL), a curriculum learning approach to fully leverage the high-value supervisory signals implicit in the untrustworthy pseudo label. ESL believes that pixels with unconfident predictions can be pretty sure about their belonging to a subset of dominant classes though being arduous to determine the exact one. It thus contains a Dynamic Soft Label (DSL) module to dynamically maintain the high probability classes, keeping the label &quot;soft&quot; so as to make full use of the high entropy prediction. However, the DSL itself will inevitably introduce ambiguity between dominant classes, thus blurring the classification boundary. Therefore, we further propose a pixel-to-part contrastive learning method cooperated with an unsupervised object part grouping mechanism to improve its ability to distinguish between different classes. Extensive experimental results on Pascal VOC 2012 and Cityscapes show that our approach achieves remarkable improvements over existing state-of-the-art approaches.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>1185</startPage>
    <endPage>1195</endPage>
    <authors><![CDATA[Jie Ma;Chuan Wang;Yang Liu;Liang Lin;Guanbin Li]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Vehicle-to-Vehicle technologies have enabled autonomous vehicles to share information to see through occlusions, greatly enhancing perception performance. Nevertheless, existing works all focused on homogeneous traffic where vehicles are equipped with the same type of sensors, which significantly hampers the scale of collaboration and benefit of cross-modality interactions. In this paper, we investigate the multi-agent hetero-modal cooperative perception problem where agents may have distinct sensor modalities. We present HM-ViT, the first unified multi-agent hetero-modal cooperative perception framework that can collaboratively predict 3D objects for highly dynamic Vehicle-to-Vehicle (V2V) collaborations with varying numbers and types of agents. To effectively fuse features from multi-view images and LiDAR point clouds, we design a novel heterogeneous 3D graph transformer to jointly reason inter-agent and intra-agent interactions. The extensive experiments on the V2V perception dataset OPV2V demonstrate that the HM-ViT outperforms SOTA cooperative perception methods for V2V hetero-modal cooperative perception. Our code will be released at https://github.com/XHwind/HM-ViT.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>284</startPage>
    <endPage>295</endPage>
    <authors><![CDATA[Hao Xiang;Runsheng Xu;Jiaqi Ma]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[In this paper, we present our method for neural face reenactment, called HyperReenact, that aims to generate realistic talking head images of a source identity, driven by a target facial pose. Existing state-of-the-art face reenactment methods train controllable generative models that learn to synthesize realistic facial images, yet producing reenacted faces that are prone to significant visual artifacts, especially under the challenging condition of extreme head pose changes, or requiring expensive few-shot fine-tuning to better preserve the source identity characteristics. We propose to address these limitations by leveraging the photorealistic generation ability and the disentangled properties of a pretrained StyleGAN2 generator, by first inverting the real images into its latent space and then using a hypernetwork to perform: (i) refinement of the source identity characteristics and (ii) facial pose re-targeting, eliminating this way the dependence on external editing methods that typically produce artifacts. Our method operates under the one-shot setting (i.e., using a single source frame) and allows for cross-subject reenactment, without requiring any subject-specific fine-tuning. We compare our method both quantitatively and qualitatively against several state-of-the-art techniques on the standard benchmarks of VoxCeleb1 and VoxCeleb2, demonstrating the superiority of our approach in producing artifact-free images, exhibiting remarkable robustness even under extreme head pose changes. We make the code and the pretrained models publicly available at: https://github.com/StelaBou/HyperReenact]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>7149</startPage>
    <endPage>7159</endPage>
    <authors><![CDATA[Stella Bounareli;Christos Tzelepis;Vasileios Argyriou;Ioannis Patras;Georgios Tzimiropoulos]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Unified Visual Relationship Detection with Vision and Language Models]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zhao_Unified_Visual_Relationship_Detection_with_Vision_and_Language_Models_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[This work focuses on training a single visual relationship detector predicting over the union of label spaces from multiple datasets. Merging labels spanning different datasets could be challenging due to inconsistent taxonomies. The issue is exacerbated in visual relationship detection when second-order visual semantics are introduced between pairs of objects. To address this challenge, we propose UniVRD, a novel bottom-up method for Unified Visual Relationship Detection by leveraging vision and language models (VLMs). VLMs provide well-aligned image and text embeddings, where similar relationships are optimized to be close to each other for semantic unification. Our bottom-up design enables the model to enjoy the benefit of training with both object detection and visual relationship datasets. Empirical results on both human-object interaction detection and scene-graph generation demonstrate the competitive performance of our model. UniVRD achieves 38.07 mAP on HICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP. More importantly, we show that our unified detector performs as well as dataset-specific models in mAP, and achieves further improvements when we scale up the model. Our code will be made publicly available on GitHub.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>6962</startPage>
    <endPage>6973</endPage>
    <authors><![CDATA[Long Zhao;Liangzhe Yuan;Boqing Gong;Yin Cui;Florian Schroff;Ming-Hsuan Yang;Hartwig Adam;Ting Liu]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Struppek_Rickrolling_the_Artist_Injecting_Backdoors_into_Text_Encoders_for_Text-to-Image_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nudity or violence, and help to make image generation safer.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4584</startPage>
    <endPage>4596</endPage>
    <authors><![CDATA[Lukas Struppek;Dominik Hintersdorf;Kristian Kersting]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/PNVR_LD-ZNet_A_Latent_Diffusion_Approach_for_Text-Based_Image_Segmentation_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Large-scale pre-training tasks like image classification, captioning, or self-supervised techniques do not incentivize learning the semantic boundaries of objects. However, recent generative foundation models built using text-based latent diffusion techniques may learn semantic boundaries. This is because they have to synthesize intricate details about all objects in an image based on a text description. Therefore, we present a technique for segmenting real and AI-generated images using latent diffusion models (LDMs) trained on internet-scale datasets. First, we show that the latent space of LDMs (z-space) is a better input representation compared to other feature representations like RGB images or CLIP encodings for text-based image segmentation. By training the segmentation models on the latent z-space, which creates a compressed representation across several domains like different forms of art, cartoons, illustrations, and photographs, we are also able to bridge the domain gap between real and AI-generated images. We show that the internal features of LDMs contain rich semantic information and present a technique in the form of LD-ZNet to further boost the performance of text-based segmentation. Overall, we show up to 6% improvement over standard baselines for text-to-image segmentation on natural images. For AI-generated imagery, we show close to 20% improvement compared to state-of-the-art techniques. The project is available at https://koutilya-pnvr.github.io/LD-ZNet/.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4157</startPage>
    <endPage>4168</endPage>
    <authors><![CDATA[Koutilya PNVR;Bharat Singh;Pallabi Ghosh;Behjat Siddiquie;David Jacobs]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Downstream-agnostic Adversarial Examples]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zhou_Downstream-agnostic_Adversarial_Examples_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of &quot;big model&quot;. Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use. In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>4345</startPage>
    <endPage>4355</endPage>
    <authors><![CDATA[Ziqi Zhou;Shengshan Hu;Ruizhi Zhao;Qian Wang;Leo Yu Zhang;Junhui Hou;Hai Jin]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Studying How to Efficiently and Effectively Guide Models with Explanations]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Rao_Studying_How_to_Efficiently_and_Effectively_Guide_Models_with_Explanations_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Despite being highly performant, deep neural networks might base their decisions on features that spuriously correlate with the provided labels, thus hurting generalization. To mitigate this, &apos;model guidance&apos; has recently gained popularity, i.e. the idea of regularizing the models&apos; explanations to ensure that they are &quot;right for the right reasons&quot;. While various techniques to achieve such model guidance have been proposed, experimental validation of these approaches has thus far been limited to relatively simple and / or synthetic datasets. To better understand the effectiveness of the various design choices that have been explored in the context of model guidance, in this work we conduct an in-depth evaluation across various loss functions, attribution methods, models, and &apos;guidance depths&apos; on the PASCAL VOC 2007 and MS COCO 2014 datasets. As annotation costs for model guidance can limit its applicability, we also place a particular focus on efficiency. Specifically, we guide the models via bounding box annotations, which are much cheaper to obtain than the commonly used segmentation masks, and evaluate the robustness of model guidance under limited (e.g. with only 1% of annotated images) or overly coarse annotations. Further, we propose using the EPG score as an additional evaluation metric and loss function (&apos;Energy loss&apos;). We show that optimizing for the Energy loss leads to models that exhibit a distinct focus on object-specific features, despite only using bounding box annotations that also include background regions. Lastly, we show that such model guidance can improve generalization under distribution shifts. Code available at: https://github.com/sukrutrao/Model-Guidance]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>1922</startPage>
    <endPage>1933</endPage>
    <authors><![CDATA[Sukrut Rao;Moritz B hle;Amin Parchami-Araghi;Bernt Schiele]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Skeleton sequence representation learning has shown great advantages for action recognition due to its promising ability to model human joints and topology. However, the current methods usually require sufficient labeled data for training computationally expensive models. Moreover, these methods ignore how to utilize the fine-grained dependencies among different skeleton joints to pre-train an efficient skeleton sequence learning model that can generalize well across different datasets. In this paper, we propose an efficient skeleton sequence learning framework, named Skeleton Sequence Learning (SSL). To comprehensively capture the human pose and obtain discriminative skeleton sequence representation, we build an asymmetric graph-based encoder-decoder pre-training architecture named SkeletonMAE, which embeds skeleton joint sequence into graph convolutional network and reconstructs the masked skeleton joints and edges based on the prior human topology knowledge. Then, the pre-trained SkeletonMAE encoder is integrated with the Spatial-Temporal Representation Learning (STRL) module to build the SSL framework. Extensive experimental results show that our SSL generalizes well across different datasets and outperforms the state-of-the-art self-supervised skeleton-based methods on FineGym, Diving48, NTU 60 and NTU 120 datasets. Moreover, we obtain comparable performance to some fully supervised methods. The code is avaliable at https://github.com/HongYan1123/SkeletonMAE.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>5606</startPage>
    <endPage>5618</endPage>
    <authors><![CDATA[Hong Yan;Yang Liu;Yushen Wei;Zhen Li;Guanbin Li;Liang Lin]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Pose-Free Neural Radiance Fields via Implicit Pose Regularization]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Zhang_Pose-Free_Neural_Radiance_Fields_via_Implicit_Pose_Regularization_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[Pose-free neural radiance fields (NeRF) aim to train NeRF with unposed multi-view images and it has achieved very impressive success in recent years. Most existing works share the pipeline of training a coarse pose estimator with rendered images at first, followed by a joint optimization of estimated poses and neural radiance field. However, as the pose estimator is trained with only rendered images, the pose estimation is usually biased or inaccurate for real images due to the domain gap between real images and rendered images, leading to poor robustness for the pose estimation of real images and further local min- ima in joint optimization. We design IR-NeRF, an innovative pose-free NeRF that introduces implicit pose regularization to refine pose estimator with unposed real images and improve the robustness of the pose estimation for real images. With a collection of 2D images of a specific scene, IR-NeRF constructs a scene codebook that stores scene features and captures the scene-specific pose distribution implicitly as priors. Thus, the robustness of pose estimation can be promoted with the scene priors according to the rationale that a 2D real image can be well reconstructed from the scene codebook only when its estimated pose lies within the pose distribution. Extensive experiments show that IR-NeRF achieves superior novel view synthesis and outperforms the state-of-the-art consistently across multiple synthetic and real datasets.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>3534</startPage>
    <endPage>3543</endPage>
    <authors><![CDATA[Jiahui Zhang;Fangneng Zhan;Yingchen Yu;Kunhao Liu;Rongliang Wu;Xiaoqin Zhang;Ling Shao;Shijian Lu]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Mensink_Encyclopedic_VQA_Visual_Questions_About_Detailed_Properties_of_Fine-Grained_Categories_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[We propose Encyclopedic-VQA, a large scale visual question answering (VQA) dataset featuring visual questions about detailed properties of fine-grained categories and instances. It contains 221k unique question+answer pairs each matched with (up to) 5 images, resulting in a total of 1M VQA samples. Moreover, our dataset comes with a controlled knowledge base derived from Wikipedia, marking the evidence to support each answer. Empirically, we show that our dataset poses a hard challenge for large vision+language models as they perform poorly on our dataset: PaLI [9] is state-of-the-art on OK-VQA [29], yet it only achieves 13.0% accuracy on our dataset. Moreover, we experimentally show that progress on answering our encyclopedic questions can be achieved by augmenting large models with a mechanism that retrieves relevant information for the knowledge base. An oracle experiment with perfect retrieval achieves 87.0% accuracy on the single-hop portion of our dataset, and an automatic retrieval- augmented prototype yields 48.8%. We believe that our dataset enables future research on retrieval-augmented vision+language models.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>3113</startPage>
    <endPage>3124</endPage>
    <authors><![CDATA[Thomas Mensink;Jasper Uijlings;Lluis Castrejon;Arushi Goel;Felipe Cadar;Howard Zhou;Fei Sha;Andr Araujo;Vittorio Ferrari]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
<item>
    <title><![CDATA[Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View]]></title>
    <link><![CDATA[http://openaccess.thecvf.com//content/ICCV2023/papers/Yao_Towards_Understanding_the_Generalization_of_Deepfake_Detectors_from_a_Game-Theoretical_ICCV_2023_paper.pdf]]></link>
    <description><![CDATA[This paper aims to explain the generalization of deepfake detectors from the novel perspective of multi-order interactions among visual concepts. Specifically, we propose three hypotheses: 1. Deepfake detectors encode multi-order interactions among visual concepts, in which the low-order interactions usually have substantially negative contributions to deepfake detection. 2. Deepfake detectors with better generalization abilities tend to encode low-order interactions with fewer negative contributions. 3. Generalized deepfake detectors usually weaken the negative contributions of low-order interactions by suppressing their strength. Accordingly, we design several mathematical metrics to evaluate the effect of low-order interaction for deepfake detectors. Extensive comparative experiments are conducted, which verify the soundness of our hypotheses. Based on the analyses, we further propose a generic method, which directly reduces the toxic effects of low-order interactions to improve the generalization of deepfake detectors to some extent. The code will be released when the paper is accepted.]]></description>
    <pubDate><![CDATA[October 2023]]></pubDate>
    <pubYear><![CDATA[2023]]></pubYear>
    <startPage>2031</startPage>
    <endPage>2041</endPage>
    <authors><![CDATA[Kelu Yao;Jin Wang;Boyu Diao;Chao Li]]></authors>
    <booktitle><![CDATA[Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)]]></booktitle>
</item>
</channel>
</rss>
